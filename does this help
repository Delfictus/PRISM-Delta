     	selp.f64 	%fd157, %fd99, %fd157, %p36;
     	selp.b32 	%r184, %r41, %r184, %p36;

     $L__BB7_38:
     	add.s32 	%r44, %r162, 3;
     	setp.eq.s32 	%p37, %r44, %r3;
     	@%p37 bra 	$L__BB7_40;
     	bra.uni 	$L__BB7_39;

     $L__BB7_40:
     	mov.u64 	%rd111, 0;
     	st.global.u64 	[%rd22], %rd111;
     	bra.uni 	$L__BB7_41;

     $L__BB7_39:
     	setp.gt.f64 	%p38, %fd1, 0d3FE999999999999A;
     	ld.local.f64 	%fd100, [%rd23];
     	ld.local.f64 	%fd101, [%rd25+24];
     	sub.f64 	%fd102, %fd101, %fd100;
     	ld.global.nc.f64 	%fd103, [%rd26+24];
     	ld.global.nc.f64 	%fd104, [%rd24];
     	sub.f64 	%fd105, %fd103, %fd104;
     	mul.f64 	%fd106, %fd105, %fd6;
     	sub.f64 	%fd107, %fd102, %fd106;
     	setp.lt.f64 	%p39, %fd107, 0d0000000000000000;
     	and.pred  	%p40, %p38, %p39;
     	mul.f64 	%fd108, %fd107, %fd4;
     	selp.f64 	%fd109, %fd108, %fd107, %p40;
     	add.f64 	%fd110, %fd109, %fd5;
     	st.global.f64 	[%rd27+24], %fd110;
     	setp.lt.f64 	%p41, %fd110, %fd157;
     	selp.f64 	%fd157, %fd110, %fd157, %p41;
     	selp.b32 	%r184, %r44, %r184, %p41;

     $L__BB7_41:
     	add.s32 	%r162, %r162, 4;
     	add.s32 	%r156, %r156, -4;
     	setp.ne.s32 	%p42, %r156, 0;
     	@%p42 bra 	$L__BB7_29;

     $L__BB7_42:
     	ld.param.u32 	%r140, [evaluate_moves_f64_param_5];
     	and.b32  	%r139, %r140, 3;
     	setp.eq.s32 	%p43, %r139, 0;
     	@%p43 bra 	$L__BB7_69;

     	ld.param.u32 	%r142, [evaluate_moves_f64_param_5];
     	and.b32  	%r167, %r142, 3;
     	add.s32 	%r121, %r162, %r28;
     	mul.wide.s32 	%rd112, %r121, 8;
     	add.s64 	%rd137, %rd3, %rd112;
     	add.s32 	%r122, %r162, %r29;
     	mul.wide.s32 	%rd113, %r122, 8;
     	add.s64 	%rd136, %rd2, %rd113;
     	mul.wide.s32 	%rd114, %r162, 8;
     	add.s64 	%rd135, %rd6, %rd114;
     	sub.s32 	%r164, %r162, %r3;

     $L__BB7_44:
     	.pragma "nounroll";
     	setp.eq.s32 	%p44, %r164, 0;
     	@%p44 bra 	$L__BB7_46;

     	ld.local.f64 	%fd111, [%rd23];
     	ld.local.f64 	%fd112, [%rd135];
     	sub.f64 	%fd113, %fd112, %fd111;
     	ld.global.nc.f64 	%fd114, [%rd136];
     	ld.global.nc.f64 	%fd115, [%rd24];
     	sub.f64 	%fd116, %fd114, %fd115;
     	mul.f64 	%fd117, %fd116, %fd6;
     	sub.f64 	%fd118, %fd113, %fd117;
     	setp.lt.f64 	%p45, %fd118, 0d0000000000000000;
     	setp.gt.f64 	%p46, %fd1, 0d3FE999999999999A;
     	and.pred  	%p47, %p46, %p45;
     	mul.f64 	%fd119, %fd118, %fd4;
     	selp.f64 	%fd120, %fd119, %fd118, %p47;
     	add.f64 	%fd121, %fd120, %fd5;
     	st.global.f64 	[%rd137], %fd121;
     	setp.lt.f64 	%p48, %fd121, %fd157;
     	selp.f64 	%fd157, %fd121, %fd157, %p48;
     	selp.b32 	%r184, %r162, %r184, %p48;
     	bra.uni 	$L__BB7_47;

     $L__BB7_46:
     	mov.u64 	%rd115, 0;
     	st.global.u64 	[%rd22], %rd115;

     $L__BB7_47:
     	add.s32 	%r162, %r162, 1;
     	add.s64 	%rd137, %rd137, 8;
     	add.s64 	%rd136, %rd136, 8;
     	add.s64 	%rd135, %rd135, 8;
     	add.s32 	%r164, %r164, 1;
     	add.s32 	%r167, %r167, -1;
     	setp.eq.s32 	%p49, %r167, 0;
     	@%p49 bra 	$L__BB7_69;
     	bra.uni 	$L__BB7_44;

     $L__BB7_48:
     	setp.lt.u32 	%p50, %r30, 3;
     	mov.f64 	%fd157, 0d0000000000000000;
     	mov.u32 	%r177, 0;
     	mov.u32 	%r184, %r3;
     	@%p50 bra 	$L__BB7_63;

     	sub.s32 	%r171, %r98, %r182;
     	mov.f64 	%fd157, 0d0000000000000000;
     	mov.u32 	%r177, 0;
     	mov.u32 	%r184, %r3;

     $L__BB7_50:
     	mul.wide.s32 	%rd116, %r177, 8;
     	add.s64 	%rd37, %rd6, %rd116;
     	add.s32 	%r126, %r177, %r28;
     	mul.wide.s32 	%rd117, %r126, 8;
     	add.s64 	%rd38, %rd3, %rd117;
     	setp.eq.s32 	%p51, %r177, %r3;
     	@%p51 bra 	$L__BB7_52;
     	bra.uni 	$L__BB7_51;

     $L__BB7_52:
     	mov.u64 	%rd118, 0;
     	st.global.u64 	[%rd22], %rd118;
     	bra.uni 	$L__BB7_53;

     $L__BB7_51:
     	setp.gt.f64 	%p52, %fd1, 0d3FE999999999999A;
     	ld.local.f64 	%fd125, [%rd23];
     	ld.local.f64 	%fd126, [%rd37];
     	sub.f64 	%fd127, %fd126, %fd125;
     	setp.lt.f64 	%p53, %fd127, 0d0000000000000000;
     	and.pred  	%p54, %p52, %p53;
     	mul.f64 	%fd128, %fd127, %fd4;
     	selp.f64 	%fd129, %fd128, %fd127, %p54;
     	add.f64 	%fd130, %fd129, %fd5;
     	st.global.f64 	[%rd38], %fd130;
     	setp.lt.f64 	%p55, %fd130, %fd157;
     	selp.f64 	%fd157, %fd130, %fd157, %p55;
     	selp.b32 	%r184, %r177, %r184, %p55;

     $L__BB7_53:
     	add.s32 	%r68, %r177, 1;
     	setp.eq.s32 	%p56, %r68, %r3;
     	@%p56 bra 	$L__BB7_55;
     	bra.uni 	$L__BB7_54;

     $L__BB7_55:
     	mov.u64 	%rd119, 0;
     	st.global.u64 	[%rd22], %rd119;
     	bra.uni 	$L__BB7_56;

     $L__BB7_54:
     	setp.gt.f64 	%p57, %fd1, 0d3FE999999999999A;
     	ld.local.f64 	%fd131, [%rd23];
     	ld.local.f64 	%fd132, [%rd37+8];
     	sub.f64 	%fd133, %fd132, %fd131;
     	setp.lt.f64 	%p58, %fd133, 0d0000000000000000;
     	and.pred  	%p59, %p57, %p58;
     	mul.f64 	%fd134, %fd133, %fd4;
     	selp.f64 	%fd135, %fd134, %fd133, %p59;
     	add.f64 	%fd136, %fd135, %fd5;
     	st.global.f64 	[%rd38+8], %fd136;
     	setp.lt.f64 	%p60, %fd136, %fd157;
     	selp.f64 	%fd157, %fd136, %fd157, %p60;
     	selp.b32 	%r184, %r68, %r184, %p60;

     $L__BB7_56:
     	add.s32 	%r71, %r177, 2;
     	setp.eq.s32 	%p61, %r71, %r3;
     	@%p61 bra 	$L__BB7_58;
     	bra.uni 	$L__BB7_57;

     $L__BB7_58:
     	mov.u64 	%rd120, 0;
     	st.global.u64 	[%rd22], %rd120;
     	bra.uni 	$L__BB7_59;

     $L__BB7_57:
     	setp.gt.f64 	%p62, %fd1, 0d3FE999999999999A;
     	ld.local.f64 	%fd137, [%rd23];
     	ld.local.f64 	%fd138, [%rd37+16];
     	sub.f64 	%fd139, %fd138, %fd137;
     	setp.lt.f64 	%p63, %fd139, 0d0000000000000000;
     	and.pred  	%p64, %p62, %p63;
     	mul.f64 	%fd140, %fd139, %fd4;
     	selp.f64 	%fd141, %fd140, %fd139, %p64;
     	add.f64 	%fd142, %fd141, %fd5;
     	st.global.f64 	[%rd38+16], %fd142;
     	setp.lt.f64 	%p65, %fd142, %fd157;
     	selp.f64 	%fd157, %fd142, %fd157, %p65;
     	selp.b32 	%r184, %r71, %r184, %p65;

     $L__BB7_59:
     	add.s32 	%r74, %r177, 3;
     	setp.eq.s32 	%p66, %r74, %r3;
     	@%p66 bra 	$L__BB7_61;
     	bra.uni 	$L__BB7_60;

     $L__BB7_61:
     	mov.u64 	%rd121, 0;
     	st.global.u64 	[%rd22], %rd121;
     	bra.uni 	$L__BB7_62;

     $L__BB7_60:
     	setp.gt.f64 	%p67, %fd1, 0d3FE999999999999A;
     	ld.local.f64 	%fd143, [%rd23];
     	ld.local.f64 	%fd144, [%rd37+24];
     	sub.f64 	%fd145, %fd144, %fd143;
     	setp.lt.f64 	%p68, %fd145, 0d0000000000000000;
     	and.pred  	%p69, %p67, %p68;
     	mul.f64 	%fd146, %fd145, %fd4;
     	selp.f64 	%fd147, %fd146, %fd145, %p69;
     	add.f64 	%fd148, %fd147, %fd5;
     	st.global.f64 	[%rd38+24], %fd148;
     	setp.lt.f64 	%p70, %fd148, %fd157;
     	selp.f64 	%fd157, %fd148, %fd157, %p70;
     	selp.b32 	%r184, %r74, %r184, %p70;

     $L__BB7_62:
     	add.s32 	%r177, %r177, 4;
     	add.s32 	%r171, %r171, -4;
     	setp.ne.s32 	%p71, %r171, 0;
     	@%p71 bra 	$L__BB7_50;

     $L__BB7_63:
     	setp.eq.s32 	%p72, %r182, 0;
     	@%p72 bra 	$L__BB7_69;

     	add.s32 	%r127, %r177, %r28;
     	mul.wide.s32 	%rd122, %r127, 8;
     	add.s64 	%rd139, %rd3, %rd122;
     	mul.wide.s32 	%rd123, %r177, 8;
     	add.s64 	%rd138, %rd6, %rd123;
     	sub.s32 	%r179, %r177, %r3;

     $L__BB7_65:
     	.pragma "nounroll";
     	setp.eq.s32 	%p73, %r179, 0;
     	@%p73 bra 	$L__BB7_67;

     	ld.local.f64 	%fd149, [%rd23];
     	ld.local.f64 	%fd150, [%rd138];
     	sub.f64 	%fd151, %fd150, %fd149;
     	setp.lt.f64 	%p74, %fd151, 0d0000000000000000;
     	setp.gt.f64 	%p75, %fd1, 0d3FE999999999999A;
     	and.pred  	%p76, %p75, %p74;
     	mul.f64 	%fd152, %fd151, %fd4;
     	selp.f64 	%fd153, %fd152, %fd151, %p76;
     	add.f64 	%fd154, %fd153, %fd5;
     	st.global.f64 	[%rd139], %fd154;
     	setp.lt.f64 	%p77, %fd154, %fd157;
     	selp.f64 	%fd157, %fd154, %fd157, %p77;
     	selp.b32 	%r184, %r177, %r184, %p77;
     	bra.uni 	$L__BB7_68;

     $L__BB7_67:
     	mov.u64 	%rd124, 0;
     	st.global.u64 	[%rd22], %rd124;

     $L__BB7_68:
     	add.s32 	%r177, %r177, 1;
     	add.s64 	%rd139, %rd139, 8;
     	add.s64 	%rd138, %rd138, 8;
     	add.s32 	%r179, %r179, 1;
     	add.s32 	%r182, %r182, -1;
     	setp.ne.s32 	%p78, %r182, 0;
     	@%p78 bra 	$L__BB7_65;

     $L__BB7_69:
     	setp.ltu.f64 	%p79, %fd157, 0d0000000000000000;
     	@%p79 bra 	$L__BB7_73;

     	ld.param.u32 	%r143, [evaluate_moves_f64_param_5];
     	mad.lo.s32 	%r93, %r2, 1664525, 1013904223;
     	rem.s32 	%r184, %r93, %r143;
     	setp.ne.s32 	%p80, %r184, %r3;
     	@%p80 bra 	$L__BB7_72;

     	ld.param.u32 	%r145, [evaluate_moves_f64_param_5];
     	add.s32 	%r128, %r3, 1;
     	rem.s32 	%r184, %r128, %r145;

     $L__BB7_72:
     	ld.param.u32 	%r144, [evaluate_moves_f64_param_5];
     	mov.u32 	%r134, %tid.x;
     	mov.u32 	%r133, %ntid.x;
     	mov.u32 	%r132, %ctaid.x;
     	mad.lo.s32 	%r131, %r132, %r133, %r134;
     	and.b32  	%r129, %r93, 15;
     	cvt.rn.f64.s32 	%fd155, %r129;
     	fma.rn.f64 	%fd156, %fd155, 0dBF50624DD2F1A9FC, 0dBFA999999999999A;
     	mad.lo.s32 	%r130, %r131, %r144, %r184;
     	mul.wide.s32 	%rd125, %r130, 8;
     	add.s64 	%rd126, %rd3, %rd125;
     	st.global.f64 	[%rd126], %fd156;

     $L__BB7_73:
     	mov.u32 	%r138, %tid.x;
     	mov.u32 	%r137, %ntid.x;
     	mov.u32 	%r136, %ctaid.x;
     	mad.lo.s32 	%r135, %r136, %r137, %r138;
     	cvt.s64.s32 	%rd131, %r135;
     	ld.param.u64 	%rd130, [evaluate_moves_f64_param_11];
     	cvta.to.global.u64 	%rd127, %rd130;
     	shl.b64 	%rd128, %rd131, 2;
     	add.s64 	%rd129, %rd127, %rd128;
     	st.global.u32 	[%rd129], %r184;

     $L__BB7_74:
     	ret;

     }
     	// .globl	evaluate_moves_f64_geometry
     .visible .entry evaluate_moves_f64_geometry(
     	.param .u64 evaluate_moves_f64_geometry_param_0,
     	.param .u64 evaluate_moves_f64_geometry_param_1,
     	.param .u64 evaluate_moves_f64_geometry_param_2,
     	.param .u64 evaluate_moves_f64_geometry_param_3,
     	.param .u32 evaluate_moves_f64_geometry_param_4,
     	.param .u32 evaluate_moves_f64_geometry_param_5,
     	.param .u64 evaluate_moves_f64_geometry_param_6,
     	.param .u64 evaluate_moves_f64_geometry_param_7,
     	.param .u64 evaluate_moves_f64_geometry_param_8,
     	.param .u64 evaluate_moves_f64_geometry_param_9,
     	.param .u32 evaluate_moves_f64_geometry_param_10,
     	.param .u64 evaluate_moves_f64_geometry_param_11,
     	.param .u64 evaluate_moves_f64_geometry_param_12
     )
     {
     	.local .align 16 .b8 	__local_depot8[2048];
     	.reg .b64 	%SP;
     	.reg .b64 	%SPL;
     	.reg .pred 	%p<80>;
     	.reg .f32 	%f<5>;
     	.reg .b32 	%r<178>;
     	.reg .f64 	%fd<165>;
     	.reg .b64 	%rd<141>;


     	mov.u64 	%SPL, __local_depot8;
     	ld.param.u64 	%rd50, [evaluate_moves_f64_geometry_param_0];
     	ld.param.u32 	%r95, [evaluate_moves_f64_geometry_param_4];
     	ld.param.u32 	%r94, [evaluate_moves_f64_geometry_param_5];
     	ld.param.u64 	%rd52, [evaluate_moves_f64_geometry_param_6];
     	ld.param.u64 	%rd46, [evaluate_moves_f64_geometry_param_7];
     	ld.param.u64 	%rd47, [evaluate_moves_f64_geometry_param_8];
     	ld.param.u64 	%rd48, [evaluate_moves_f64_geometry_param_9];
     	ld.param.u64 	%rd53, [evaluate_moves_f64_geometry_param_11];
     	cvta.to.global.u64 	%rd2, %rd53;
     	cvta.to.global.u64 	%rd3, %rd48;
     	cvta.to.global.u64 	%rd4, %rd52;
     	cvta.to.global.u64 	%rd5, %rd50;
     	add.u64 	%rd6, %SPL, 0;
     	mov.u32 	%r96, %ntid.x;
     	mov.u32 	%r97, %ctaid.x;
     	mov.u32 	%r98, %tid.x;
     	mad.lo.s32 	%r1, %r97, %r96, %r98;
     	setp.ge.s32 	%p1, %r1, %r95;
     	setp.gt.s32 	%p2, %r94, 256;
     	or.pred  	%p3, %p1, %p2;
     	@%p3 bra 	$L__BB8_60;

     	ld.param.u64 	%rd132, [evaluate_moves_f64_geometry_param_1];
     	ld.param.u64 	%rd131, [evaluate_moves_f64_geometry_param_3];
     	cvta.to.global.u64 	%rd55, %rd131;
     	mul.wide.s32 	%rd56, %r1, 4;
     	add.s64 	%rd57, %rd55, %rd56;
     	ld.global.nc.u32 	%r2, [%rd57];
     	mul.wide.s32 	%rd58, %r2, 4;
     	add.s64 	%rd59, %rd5, %rd58;
     	ld.global.nc.u32 	%r3, [%rd59];
     	cvta.to.global.u64 	%rd60, %rd132;
     	add.s64 	%rd61, %rd60, %rd58;
     	ld.global.nc.u32 	%r4, [%rd61];
     	ld.global.nc.u32 	%r5, [%rd61+4];
     	mul.wide.s32 	%rd62, %r2, 8;
     	add.s64 	%rd63, %rd4, %rd62;
     	ld.global.nc.f64 	%fd1, [%rd63];
     	cvta.to.global.u64 	%rd64, %rd46;
     	add.s64 	%rd65, %rd64, %rd62;
     	ld.global.nc.f64 	%fd2, [%rd65];
     	cvta.to.global.u64 	%rd66, %rd47;
     	add.s64 	%rd67, %rd66, %rd58;
     	ld.global.nc.u32 	%r6, [%rd67];
     	setp.lt.s32 	%p4, %r94, 1;
     	@%p4 bra 	$L__BB8_8;

     	add.s32 	%r100, %r94, -1;
     	and.b32  	%r7, %r94, 3;
     	setp.lt.u32 	%p5, %r100, 3;
     	mov.u32 	%r141, 0;
     	@%p5 bra 	$L__BB8_5;

     	sub.s32 	%r140, %r94, %r7;
     	mov.u32 	%r141, 0;

     $L__BB8_4:
     	mul.wide.s32 	%rd68, %r141, 8;
     	add.s64 	%rd69, %rd6, %rd68;
     	mov.f64 	%fd33, 0d0000000000000000;
     	st.local.v2.f64 	[%rd69], {%fd33, %fd33};
     	st.local.v2.f64 	[%rd69+16], {%fd33, %fd33};
     	add.s32 	%r141, %r141, 4;
     	add.s32 	%r140, %r140, -4;
     	setp.ne.s32 	%p6, %r140, 0;
     	@%p6 bra 	$L__BB8_4;

     $L__BB8_5:
     	and.b32  	%r131, %r94, 3;
     	setp.eq.s32 	%p7, %r131, 0;
     	@%p7 bra 	$L__BB8_8;

     	and.b32  	%r142, %r94, 3;
     	mul.wide.s32 	%rd70, %r141, 8;
     	add.s64 	%rd133, %rd6, %rd70;

     $L__BB8_7:
     	.pragma "nounroll";
     	mov.u64 	%rd71, 0;
     	st.local.u64 	[%rd133], %rd71;
     	add.s64 	%rd133, %rd133, 8;
     	add.s32 	%r142, %r142, -1;
     	setp.ne.s32 	%p8, %r142, 0;
     	@%p8 bra 	$L__BB8_7;

     $L__BB8_8:
     	setp.le.s32 	%p9, %r5, %r4;
     	@%p9 bra 	$L__BB8_25;

     	ld.const.f32 	%f1, [c_stress_weight];
     	cvt.ftz.f64.f32 	%fd3, %f1;
     	sub.s32 	%r102, %r5, %r4;
     	and.b32  	%r144, %r102, 3;
     	setp.eq.s32 	%p10, %r144, 0;
     	mov.u32 	%r145, %r4;
     	@%p10 bra 	$L__BB8_14;

     	ld.param.u64 	%rd125, [evaluate_moves_f64_geometry_param_2];
     	cvta.to.global.u64 	%rd124, %rd125;
     	mul.wide.s32 	%rd72, %r4, 4;
     	add.s64 	%rd134, %rd124, %rd72;
     	mov.u32 	%r145, %r4;

     $L__BB8_11:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r103, [%rd134];
     	mul.wide.s32 	%rd73, %r103, 4;
     	add.s64 	%rd74, %rd5, %rd73;
     	ld.global.nc.u32 	%r19, [%rd74];
     	setp.ge.s32 	%p11, %r19, %r94;
     	@%p11 bra 	$L__BB8_13;

     	cvt.s64.s32 	%rd130, %r103;
     	shl.b64 	%rd75, %rd130, 3;
     	add.s64 	%rd76, %rd4, %rd75;
     	ld.global.nc.f64 	%fd34, [%rd76];
     	add.f64 	%fd35, %fd1, %fd34;
     	fma.rn.f64 	%fd36, %fd35, %fd3, 0d3FF0000000000000;
     	mul.wide.s32 	%rd77, %r19, 8;
     	add.s64 	%rd78, %rd6, %rd77;
     	ld.local.f64 	%fd37, [%rd78];
     	add.f64 	%fd38, %fd37, %fd36;
     	st.local.f64 	[%rd78], %fd38;

     $L__BB8_13:
     	add.s32 	%r145, %r145, 1;
     	add.s64 	%rd134, %rd134, 4;
     	add.s32 	%r144, %r144, -1;
     	setp.ne.s32 	%p12, %r144, 0;
     	@%p12 bra 	$L__BB8_11;

     $L__BB8_14:
     	not.b32 	%r104, %r4;
     	add.s32 	%r105, %r5, %r104;
     	setp.lt.u32 	%p13, %r105, 3;
     	@%p13 bra 	$L__BB8_25;

     	ld.param.u64 	%rd120, [evaluate_moves_f64_geometry_param_2];
     	cvta.to.global.u64 	%rd119, %rd120;
     	mul.wide.s32 	%rd79, %r145, 4;
     	add.s64 	%rd135, %rd119, %rd79;

     $L__BB8_16:
     	ld.global.nc.u32 	%r106, [%rd135];
     	mul.wide.s32 	%rd80, %r106, 4;
     	add.s64 	%rd81, %rd5, %rd80;
     	ld.global.nc.u32 	%r24, [%rd81];
     	setp.ge.s32 	%p14, %r24, %r94;
     	@%p14 bra 	$L__BB8_18;

     	cvt.s64.s32 	%rd126, %r106;
     	shl.b64 	%rd82, %rd126, 3;
     	add.s64 	%rd83, %rd4, %rd82;
     	ld.global.nc.f64 	%fd39, [%rd83];
     	add.f64 	%fd40, %fd1, %fd39;
     	fma.rn.f64 	%fd41, %fd40, %fd3, 0d3FF0000000000000;
     	mul.wide.s32 	%rd84, %r24, 8;
     	add.s64 	%rd85, %rd6, %rd84;
     	ld.local.f64 	%fd42, [%rd85];
     	add.f64 	%fd43, %fd42, %fd41;
     	st.local.f64 	[%rd85], %fd43;

     $L__BB8_18:
     	ld.global.nc.u32 	%r107, [%rd135+4];
     	mul.wide.s32 	%rd86, %r107, 4;
     	add.s64 	%rd87, %rd5, %rd86;
     	ld.global.nc.u32 	%r25, [%rd87];
     	setp.ge.s32 	%p15, %r25, %r94;
     	@%p15 bra 	$L__BB8_20;

     	cvt.s64.s32 	%rd127, %r107;
     	shl.b64 	%rd88, %rd127, 3;
     	add.s64 	%rd89, %rd4, %rd88;
     	ld.global.nc.f64 	%fd44, [%rd89];
     	add.f64 	%fd45, %fd1, %fd44;
     	fma.rn.f64 	%fd46, %fd45, %fd3, 0d3FF0000000000000;
     	mul.wide.s32 	%rd90, %r25, 8;
     	add.s64 	%rd91, %rd6, %rd90;
     	ld.local.f64 	%fd47, [%rd91];
     	add.f64 	%fd48, %fd47, %fd46;
     	st.local.f64 	[%rd91], %fd48;

     $L__BB8_20:
     	ld.global.nc.u32 	%r108, [%rd135+8];
     	mul.wide.s32 	%rd92, %r108, 4;
     	add.s64 	%rd93, %rd5, %rd92;
     	ld.global.nc.u32 	%r26, [%rd93];
     	setp.ge.s32 	%p16, %r26, %r94;
     	@%p16 bra 	$L__BB8_22;

     	cvt.s64.s32 	%rd128, %r108;
     	shl.b64 	%rd94, %rd128, 3;
     	add.s64 	%rd95, %rd4, %rd94;
     	ld.global.nc.f64 	%fd49, [%rd95];
     	add.f64 	%fd50, %fd1, %fd49;
     	fma.rn.f64 	%fd51, %fd50, %fd3, 0d3FF0000000000000;
     	mul.wide.s32 	%rd96, %r26, 8;
     	add.s64 	%rd97, %rd6, %rd96;
     	ld.local.f64 	%fd52, [%rd97];
     	add.f64 	%fd53, %fd52, %fd51;
     	st.local.f64 	[%rd97], %fd53;

     $L__BB8_22:
     	ld.global.nc.u32 	%r109, [%rd135+12];
     	mul.wide.s32 	%rd98, %r109, 4;
     	add.s64 	%rd99, %rd5, %rd98;
     	ld.global.nc.u32 	%r27, [%rd99];
     	setp.ge.s32 	%p17, %r27, %r94;
     	@%p17 bra 	$L__BB8_24;

     	cvt.s64.s32 	%rd129, %r109;
     	shl.b64 	%rd100, %rd129, 3;
     	add.s64 	%rd101, %rd4, %rd100;
     	ld.global.nc.f64 	%fd54, [%rd101];
     	add.f64 	%fd55, %fd1, %fd54;
     	fma.rn.f64 	%fd56, %fd55, %fd3, 0d3FF0000000000000;
     	mul.wide.s32 	%rd102, %r27, 8;
     	add.s64 	%rd103, %rd6, %rd102;
     	ld.local.f64 	%fd57, [%rd103];
     	add.f64 	%fd58, %fd57, %fd56;
     	st.local.f64 	[%rd103], %fd58;

     $L__BB8_24:
     	add.s64 	%rd135, %rd135, 16;
     	add.s32 	%r145, %r145, 4;
     	setp.lt.s32 	%p18, %r145, %r5;
     	@%p18 bra 	$L__BB8_16;

     $L__BB8_25:
     	setp.lt.s32 	%p79, %r94, 1;
     	mov.u32 	%r177, %r3;
     	@%p79 bra 	$L__BB8_59;

     	mov.u32 	%r130, %tid.x;
     	mov.u32 	%r129, %ntid.x;
     	mov.u32 	%r128, %ctaid.x;
     	mad.lo.s32 	%r127, %r128, %r129, %r130;
     	ld.param.u64 	%rd123, [evaluate_moves_f64_geometry_param_9];
     	mul.wide.s32 	%rd104, %r3, 8;
     	add.s64 	%rd22, %rd6, %rd104;
     	ld.const.f32 	%f2, [c_hotspot_multiplier];
     	cvt.ftz.f64.f32 	%fd4, %f2;
     	ld.const.f32 	%f3, [c_persistence_weight];
     	cvt.ftz.f64.f32 	%fd59, %f3;
     	mul.f64 	%fd5, %fd2, %fd59;
     	mul.lo.s32 	%r29, %r127, %r94;
     	mul.lo.s32 	%r30, %r2, %r94;
     	ld.const.f32 	%f4, [c_belief_weight];
     	cvt.ftz.f64.f32 	%fd6, %f4;
     	add.s32 	%r31, %r94, -1;
     	and.b32  	%r175, %r94, 3;
     	setp.eq.s64 	%p20, %rd123, 0;
     	@%p20 bra 	$L__BB8_43;

     	add.s32 	%r112, %r30, %r3;
     	mul.wide.s32 	%rd105, %r112, 8;
     	add.s64 	%rd23, %rd3, %rd105;
     	setp.lt.u32 	%p21, %r31, 3;
     	mov.f64 	%fd150, 0d0000000000000000;
     	mov.u32 	%r155, 0;
     	mov.u32 	%r177, %r3;
     	@%p21 bra 	$L__BB8_38;

     	ld.param.u32 	%r137, [evaluate_moves_f64_geometry_param_5];
     	sub.s32 	%r149, %r137, %r175;
     	mov.f64 	%fd150, 0d0000000000000000;
     	mov.u32 	%r155, 0;
     	mov.u32 	%r177, %r3;

     $L__BB8_29:
     	mul.wide.s32 	%rd106, %r155, 8;
     	add.s64 	%rd24, %rd6, %rd106;
     	add.s32 	%r114, %r155, %r30;
     	mul.wide.s32 	%rd107, %r114, 8;
     	add.s64 	%rd25, %rd3, %rd107;
     	add.s32 	%r115, %r155, %r29;
     	mul.wide.s32 	%rd108, %r115, 8;
     	add.s64 	%rd26, %rd2, %rd108;
     	setp.eq.s32 	%p22, %r155, %r3;
     	@%p22 bra 	$L__BB8_31;

     	setp.ne.s32 	%p23, %r6, 0;
     	ld.local.f64 	%fd62, [%rd22];
     	ld.local.f64 	%fd63, [%rd24];
     	sub.f64 	%fd64, %fd63, %fd62;
     	ld.global.nc.f64 	%fd65, [%rd25];
     	ld.global.nc.f64 	%fd66, [%rd23];
     	sub.f64 	%fd67, %fd65, %fd66;
     	mul.f64 	%fd68, %fd67, %fd6;
     	sub.f64 	%fd69, %fd64, %fd68;
     	setp.lt.f64 	%p24, %fd69, 0d0000000000000000;
     	and.pred  	%p25, %p23, %p24;
     	mul.f64 	%fd70, %fd69, %fd4;
     	selp.f64 	%fd71, %fd70, %fd69, %p25;
     	add.f64 	%fd72, %fd71, %fd5;
     	st.global.f64 	[%rd26], %fd72;
     	setp.lt.f64 	%p26, %fd72, %fd150;
     	selp.f64 	%fd150, %fd72, %fd150, %p26;
     	selp.b32 	%r177, %r155, %r177, %p26;

     $L__BB8_31:
     	add.s32 	%r39, %r155, 1;
     	setp.eq.s32 	%p27, %r39, %r3;
     	@%p27 bra 	$L__BB8_33;

     	setp.ne.s32 	%p28, %r6, 0;
     	ld.local.f64 	%fd73, [%rd22];
     	ld.local.f64 	%fd74, [%rd24+8];
     	sub.f64 	%fd75, %fd74, %fd73;
     	ld.global.nc.f64 	%fd76, [%rd25+8];
     	ld.global.nc.f64 	%fd77, [%rd23];
     	sub.f64 	%fd78, %fd76, %fd77;
     	mul.f64 	%fd79, %fd78, %fd6;
     	sub.f64 	%fd80, %fd75, %fd79;
     	setp.lt.f64 	%p29, %fd80, 0d0000000000000000;
     	and.pred  	%p30, %p28, %p29;
     	mul.f64 	%fd81, %fd80, %fd4;
     	selp.f64 	%fd82, %fd81, %fd80, %p30;
     	add.f64 	%fd83, %fd82, %fd5;
     	st.global.f64 	[%rd26+8], %fd83;
     	setp.lt.f64 	%p31, %fd83, %fd150;
     	selp.f64 	%fd150, %fd83, %fd150, %p31;
     	selp.b32 	%r177, %r39, %r177, %p31;

     $L__BB8_33:
     	add.s32 	%r42, %r155, 2;
     	setp.eq.s32 	%p32, %r42, %r3;
     	@%p32 bra 	$L__BB8_35;

     	setp.ne.s32 	%p33, %r6, 0;
     	ld.local.f64 	%fd84, [%rd22];
     	ld.local.f64 	%fd85, [%rd24+16];
     	sub.f64 	%fd86, %fd85, %fd84;
     	ld.global.nc.f64 	%fd87, [%rd25+16];
     	ld.global.nc.f64 	%fd88, [%rd23];
     	sub.f64 	%fd89, %fd87, %fd88;
     	mul.f64 	%fd90, %fd89, %fd6;
     	sub.f64 	%fd91, %fd86, %fd90;
     	setp.lt.f64 	%p34, %fd91, 0d0000000000000000;
     	and.pred  	%p35, %p33, %p34;
     	mul.f64 	%fd92, %fd91, %fd4;
     	selp.f64 	%fd93, %fd92, %fd91, %p35;
     	add.f64 	%fd94, %fd93, %fd5;
     	st.global.f64 	[%rd26+16], %fd94;
     	setp.lt.f64 	%p36, %fd94, %fd150;
     	selp.f64 	%fd150, %fd94, %fd150, %p36;
     	selp.b32 	%r177, %r42, %r177, %p36;

     $L__BB8_35:
     	add.s32 	%r45, %r155, 3;
     	setp.eq.s32 	%p37, %r45, %r3;
     	@%p37 bra 	$L__BB8_37;

     	setp.ne.s32 	%p38, %r6, 0;
     	ld.local.f64 	%fd95, [%rd22];
     	ld.local.f64 	%fd96, [%rd24+24];
     	sub.f64 	%fd97, %fd96, %fd95;
     	ld.global.nc.f64 	%fd98, [%rd25+24];
     	ld.global.nc.f64 	%fd99, [%rd23];
     	sub.f64 	%fd100, %fd98, %fd99;
     	mul.f64 	%fd101, %fd100, %fd6;
     	sub.f64 	%fd102, %fd97, %fd101;
     	setp.lt.f64 	%p39, %fd102, 0d0000000000000000;
     	and.pred  	%p40, %p38, %p39;
     	mul.f64 	%fd103, %fd102, %fd4;
     	selp.f64 	%fd104, %fd103, %fd102, %p40;
     	add.f64 	%fd105, %fd104, %fd5;
     	st.global.f64 	[%rd26+24], %fd105;
     	setp.lt.f64 	%p41, %fd105, %fd150;
     	selp.f64 	%fd150, %fd105, %fd150, %p41;
     	selp.b32 	%r177, %r45, %r177, %p41;

     $L__BB8_37:
     	add.s32 	%r155, %r155, 4;
     	add.s32 	%r149, %r149, -4;
     	setp.ne.s32 	%p42, %r149, 0;
     	@%p42 bra 	$L__BB8_29;

     $L__BB8_38:
     	ld.param.u32 	%r134, [evaluate_moves_f64_geometry_param_5];
     	and.b32  	%r133, %r134, 3;
     	setp.eq.s32 	%p43, %r133, 0;
     	@%p43 bra 	$L__BB8_59;

     	ld.param.u32 	%r136, [evaluate_moves_f64_geometry_param_5];
     	and.b32  	%r160, %r136, 3;
     	add.s32 	%r116, %r155, %r29;
     	mul.wide.s32 	%rd109, %r116, 8;
     	add.s64 	%rd138, %rd2, %rd109;
     	add.s32 	%r117, %r155, %r30;
     	mul.wide.s32 	%rd110, %r117, 8;
     	add.s64 	%rd137, %rd3, %rd110;
     	mul.wide.s32 	%rd111, %r155, 8;
     	add.s64 	%rd136, %rd6, %rd111;
     	sub.s32 	%r157, %r155, %r3;

     $L__BB8_40:
     	.pragma "nounroll";
     	setp.eq.s32 	%p44, %r157, 0;
     	@%p44 bra 	$L__BB8_42;

     	ld.local.f64 	%fd106, [%rd22];
     	ld.local.f64 	%fd107, [%rd136];
     	sub.f64 	%fd108, %fd107, %fd106;
     	ld.global.nc.f64 	%fd109, [%rd137];
     	ld.global.nc.f64 	%fd110, [%rd23];
     	sub.f64 	%fd111, %fd109, %fd110;
     	mul.f64 	%fd112, %fd111, %fd6;
     	sub.f64 	%fd113, %fd108, %fd112;
     	setp.lt.f64 	%p45, %fd113, 0d0000000000000000;
     	setp.ne.s32 	%p46, %r6, 0;
     	and.pred  	%p47, %p46, %p45;
     	mul.f64 	%fd114, %fd113, %fd4;
     	selp.f64 	%fd115, %fd114, %fd113, %p47;
     	add.f64 	%fd116, %fd115, %fd5;
     	st.global.f64 	[%rd138], %fd116;
     	setp.lt.f64 	%p48, %fd116, %fd150;
     	selp.f64 	%fd150, %fd116, %fd150, %p48;
     	selp.b32 	%r177, %r155, %r177, %p48;

     $L__BB8_42:
     	add.s32 	%r155, %r155, 1;
     	add.s64 	%rd138, %rd138, 8;
     	add.s64 	%rd137, %rd137, 8;
     	add.s64 	%rd136, %rd136, 8;
     	add.s32 	%r157, %r157, 1;
     	add.s32 	%r160, %r160, -1;
     	setp.eq.s32 	%p49, %r160, 0;
     	@%p49 bra 	$L__BB8_59;
     	bra.uni 	$L__BB8_40;

     $L__BB8_43:
     	setp.lt.u32 	%p50, %r31, 3;
     	mov.f64 	%fd158, 0d0000000000000000;
     	mov.u32 	%r170, 0;
     	mov.u32 	%r177, %r3;
     	@%p50 bra 	$L__BB8_54;

     	ld.param.u32 	%r138, [evaluate_moves_f64_geometry_param_5];
     	sub.s32 	%r164, %r138, %r175;
     	mov.f64 	%fd158, 0d0000000000000000;
     	mov.u32 	%r170, 0;
     	mov.u32 	%r177, %r3;

     $L__BB8_45:
     	mul.wide.s32 	%rd112, %r170, 8;
     	add.s64 	%rd36, %rd6, %rd112;
     	add.s32 	%r121, %r170, %r29;
     	mul.wide.s32 	%rd113, %r121, 8;
     	add.s64 	%rd37, %rd2, %rd113;
     	setp.eq.s32 	%p51, %r170, %r3;
     	@%p51 bra 	$L__BB8_47;

     	setp.ne.s32 	%p52, %r6, 0;
     	ld.local.f64 	%fd119, [%rd22];
     	ld.local.f64 	%fd120, [%rd36];
     	sub.f64 	%fd121, %fd120, %fd119;
     	setp.lt.f64 	%p53, %fd121, 0d0000000000000000;
     	and.pred  	%p54, %p52, %p53;
     	mul.f64 	%fd122, %fd121, %fd4;
     	selp.f64 	%fd123, %fd122, %fd121, %p54;
     	add.f64 	%fd124, %fd123, %fd5;
     	st.global.f64 	[%rd37], %fd124;
     	setp.lt.f64 	%p55, %fd124, %fd158;
     	selp.f64 	%fd158, %fd124, %fd158, %p55;
     	selp.b32 	%r177, %r170, %r177, %p55;

     $L__BB8_47:
     	add.s32 	%r69, %r170, 1;
     	setp.eq.s32 	%p56, %r69, %r3;
     	@%p56 bra 	$L__BB8_49;

     	setp.ne.s32 	%p57, %r6, 0;
     	ld.local.f64 	%fd125, [%rd22];
     	ld.local.f64 	%fd126, [%rd36+8];
     	sub.f64 	%fd127, %fd126, %fd125;
     	setp.lt.f64 	%p58, %fd127, 0d0000000000000000;
     	and.pred  	%p59, %p57, %p58;
     	mul.f64 	%fd128, %fd127, %fd4;
     	selp.f64 	%fd129, %fd128, %fd127, %p59;
     	add.f64 	%fd130, %fd129, %fd5;
     	st.global.f64 	[%rd37+8], %fd130;
     	setp.lt.f64 	%p60, %fd130, %fd158;
     	selp.f64 	%fd158, %fd130, %fd158, %p60;
     	selp.b32 	%r177, %r69, %r177, %p60;

     $L__BB8_49:
     	add.s32 	%r72, %r170, 2;
     	setp.eq.s32 	%p61, %r72, %r3;
     	@%p61 bra 	$L__BB8_51;

     	setp.ne.s32 	%p62, %r6, 0;
     	ld.local.f64 	%fd131, [%rd22];
     	ld.local.f64 	%fd132, [%rd36+16];
     	sub.f64 	%fd133, %fd132, %fd131;
     	setp.lt.f64 	%p63, %fd133, 0d0000000000000000;
     	and.pred  	%p64, %p62, %p63;
     	mul.f64 	%fd134, %fd133, %fd4;
     	selp.f64 	%fd135, %fd134, %fd133, %p64;
     	add.f64 	%fd136, %fd135, %fd5;
     	st.global.f64 	[%rd37+16], %fd136;
     	setp.lt.f64 	%p65, %fd136, %fd158;
     	selp.f64 	%fd158, %fd136, %fd158, %p65;
     	selp.b32 	%r177, %r72, %r177, %p65;

     $L__BB8_51:
     	add.s32 	%r75, %r170, 3;
     	setp.eq.s32 	%p66, %r75, %r3;
     	@%p66 bra 	$L__BB8_53;

     	setp.ne.s32 	%p67, %r6, 0;
     	ld.local.f64 	%fd137, [%rd22];
     	ld.local.f64 	%fd138, [%rd36+24];
     	sub.f64 	%fd139, %fd138, %fd137;
     	setp.lt.f64 	%p68, %fd139, 0d0000000000000000;
     	and.pred  	%p69, %p67, %p68;
     	mul.f64 	%fd140, %fd139, %fd4;
     	selp.f64 	%fd141, %fd140, %fd139, %p69;
     	add.f64 	%fd142, %fd141, %fd5;
     	st.global.f64 	[%rd37+24], %fd142;
     	setp.lt.f64 	%p70, %fd142, %fd158;
     	selp.f64 	%fd158, %fd142, %fd158, %p70;
     	selp.b32 	%r177, %r75, %r177, %p70;

     $L__BB8_53:
     	add.s32 	%r170, %r170, 4;
     	add.s32 	%r164, %r164, -4;
     	setp.ne.s32 	%p71, %r164, 0;
     	@%p71 bra 	$L__BB8_45;

     $L__BB8_54:
     	setp.eq.s32 	%p72, %r175, 0;
     	@%p72 bra 	$L__BB8_59;

     	add.s32 	%r122, %r170, %r29;
     	mul.wide.s32 	%rd114, %r122, 8;
     	add.s64 	%rd140, %rd2, %rd114;
     	mul.wide.s32 	%rd115, %r170, 8;
     	add.s64 	%rd139, %rd6, %rd115;
     	sub.s32 	%r172, %r170, %r3;

     $L__BB8_56:
     	.pragma "nounroll";
     	setp.eq.s32 	%p73, %r172, 0;
     	@%p73 bra 	$L__BB8_58;

     	ld.local.f64 	%fd143, [%rd22];
     	ld.local.f64 	%fd144, [%rd139];
     	sub.f64 	%fd145, %fd144, %fd143;
     	setp.lt.f64 	%p74, %fd145, 0d0000000000000000;
     	setp.ne.s32 	%p75, %r6, 0;
     	and.pred  	%p76, %p75, %p74;
     	mul.f64 	%fd146, %fd145, %fd4;
     	selp.f64 	%fd147, %fd146, %fd145, %p76;
     	add.f64 	%fd148, %fd147, %fd5;
     	st.global.f64 	[%rd140], %fd148;
     	setp.lt.f64 	%p77, %fd148, %fd158;
     	selp.f64 	%fd158, %fd148, %fd158, %p77;
     	selp.b32 	%r177, %r170, %r177, %p77;

     $L__BB8_58:
     	add.s32 	%r170, %r170, 1;
     	add.s64 	%rd140, %rd140, 8;
     	add.s64 	%rd139, %rd139, 8;
     	add.s32 	%r172, %r172, 1;
     	add.s32 	%r175, %r175, -1;
     	setp.ne.s32 	%p78, %r175, 0;
     	@%p78 bra 	$L__BB8_56;

     $L__BB8_59:
     	mov.u32 	%r126, %tid.x;
     	mov.u32 	%r125, %ntid.x;
     	mov.u32 	%r124, %ctaid.x;
     	mad.lo.s32 	%r123, %r124, %r125, %r126;
     	cvt.s64.s32 	%rd122, %r123;
     	ld.param.u64 	%rd121, [evaluate_moves_f64_geometry_param_12];
     	cvta.to.global.u64 	%rd116, %rd121;
     	shl.b64 	%rd117, %rd122, 2;
     	add.s64 	%rd118, %rd116, %rd117;
     	st.global.u32 	[%rd118], %r177;

     $L__BB8_60:
     	ret;

     }
     	// .globl	apply_moves_with_locking
     .visible .entry apply_moves_with_locking(
     	.param .u64 apply_moves_with_locking_param_0,
     	.param .u64 apply_moves_with_locking_param_1,
     	.param .u64 apply_moves_with_locking_param_2,
     	.param .u64 apply_moves_with_locking_param_3,
     	.param .u32 apply_moves_with_locking_param_4,
     	.param .u32 apply_moves_with_locking_param_5,
     	.param .u64 apply_moves_with_locking_param_6,
     	.param .u64 apply_moves_with_locking_param_7,
     	.param .u64 apply_moves_with_locking_param_8,
     	.param .u64 apply_moves_with_locking_param_9
     )
     {
     	.reg .pred 	%p<38>;
     	.reg .f32 	%f<11>;
     	.reg .b32 	%r<147>;
     	.reg .b64 	%rd<101>;


     	ld.param.u64 	%rd25, [apply_moves_with_locking_param_0];
     	ld.param.u64 	%rd26, [apply_moves_with_locking_param_1];
     	ld.param.u64 	%rd27, [apply_moves_with_locking_param_2];
     	ld.param.u64 	%rd28, [apply_moves_with_locking_param_3];
     	ld.param.u32 	%r49, [apply_moves_with_locking_param_4];
     	ld.param.u32 	%r48, [apply_moves_with_locking_param_5];
     	ld.param.u64 	%rd31, [apply_moves_with_locking_param_6];
     	ld.param.u64 	%rd29, [apply_moves_with_locking_param_7];
     	ld.param.u64 	%rd30, [apply_moves_with_locking_param_8];
     	ld.param.u64 	%rd32, [apply_moves_with_locking_param_9];
     	cvta.to.global.u64 	%rd1, %rd32;
     	cvta.to.global.u64 	%rd2, %rd31;
     	mov.u32 	%r50, %ntid.x;
     	mov.u32 	%r51, %ctaid.x;
     	mov.u32 	%r52, %tid.x;
     	mad.lo.s32 	%r1, %r51, %r50, %r52;
     	setp.ge.s32 	%p1, %r1, %r49;
     	@%p1 bra 	$L__BB9_32;

     	cvta.to.global.u64 	%rd33, %rd26;
     	mul.wide.s32 	%rd34, %r1, 4;
     	add.s64 	%rd35, %rd33, %rd34;
     	cvta.to.global.u64 	%rd36, %rd27;
     	add.s64 	%rd37, %rd36, %rd34;
     	ld.global.nc.u32 	%r2, [%rd37];
     	mad.lo.s32 	%r53, %r1, %r48, %r2;
     	cvta.to.global.u64 	%rd38, %rd28;
     	mul.wide.s32 	%rd39, %r53, 4;
     	add.s64 	%rd40, %rd38, %rd39;
     	ld.global.nc.f32 	%f1, [%rd40];
     	ld.global.nc.u32 	%r3, [%rd35];
     	cvt.s64.s32 	%rd3, %r3;
     	cvta.to.global.u64 	%rd41, %rd25;
     	mul.wide.s32 	%rd42, %r3, 4;
     	add.s64 	%rd43, %rd41, %rd42;
     	ld.global.u32 	%r4, [%rd43];
     	setp.eq.s32 	%p2, %r2, %r4;
     	@%p2 bra 	$L__BB9_32;

     	cvta.to.global.u64 	%rd44, %rd30;
     	shl.b64 	%rd45, %rd3, 2;
     	add.s64 	%rd46, %rd44, %rd45;
     	ld.global.nc.u32 	%r5, [%rd46+4];
     	ld.global.nc.u32 	%r6, [%rd46];
     	setp.le.s32 	%p3, %r5, %r6;
     	mov.f32 	%f9, 0f00000000;
     	mov.f32 	%f10, %f9;
     	@%p3 bra 	$L__BB9_10;

     	sub.s32 	%r57, %r5, %r6;
     	not.b32 	%r58, %r6;
     	add.s32 	%r59, %r5, %r58;
     	and.b32  	%r135, %r57, 3;
     	setp.lt.u32 	%p4, %r59, 3;
     	mov.u32 	%r137, 0;
     	mov.u32 	%r130, %r6;
     	mov.u32 	%r136, %r137;
     	@%p4 bra 	$L__BB9_6;

     	sub.s32 	%r127, %r57, %r135;
     	mul.wide.s32 	%rd47, %r6, 4;
     	add.s64 	%rd48, %rd1, %rd47;
     	add.s64 	%rd95, %rd48, 8;
     	mov.u32 	%r137, 0;
     	mov.u32 	%r130, %r6;

     $L__BB9_5:
     	ld.global.nc.u32 	%r63, [%rd95+-8];
     	mul.wide.s32 	%rd49, %r63, 4;
     	add.s64 	%rd50, %rd41, %rd49;
     	ld.global.u32 	%r64, [%rd50];
     	setp.eq.s32 	%p5, %r64, %r4;
     	selp.u32 	%r65, 1, 0, %p5;
     	add.s32 	%r66, %r136, %r65;
     	setp.eq.s32 	%p6, %r64, %r2;
     	selp.u32 	%r67, 1, 0, %p6;
     	add.s32 	%r68, %r137, %r67;
     	ld.global.nc.u32 	%r69, [%rd95+-4];
     	mul.wide.s32 	%rd51, %r69, 4;
     	add.s64 	%rd52, %rd41, %rd51;
     	ld.global.u32 	%r70, [%rd52];
     	setp.eq.s32 	%p7, %r70, %r4;
     	selp.u32 	%r71, 1, 0, %p7;
     	add.s32 	%r72, %r66, %r71;
     	setp.eq.s32 	%p8, %r70, %r2;
     	selp.u32 	%r73, 1, 0, %p8;
     	add.s32 	%r74, %r68, %r73;
     	ld.global.nc.u32 	%r75, [%rd95];
     	mul.wide.s32 	%rd53, %r75, 4;
     	add.s64 	%rd54, %rd41, %rd53;
     	ld.global.u32 	%r76, [%rd54];
     	setp.eq.s32 	%p9, %r76, %r4;
     	selp.u32 	%r77, 1, 0, %p9;
     	add.s32 	%r78, %r72, %r77;
     	setp.eq.s32 	%p10, %r76, %r2;
     	selp.u32 	%r79, 1, 0, %p10;
     	add.s32 	%r80, %r74, %r79;
     	ld.global.nc.u32 	%r81, [%rd95+4];
     	mul.wide.s32 	%rd55, %r81, 4;
     	add.s64 	%rd56, %rd41, %rd55;
     	ld.global.u32 	%r82, [%rd56];
     	setp.eq.s32 	%p11, %r82, %r4;
     	selp.u32 	%r83, 1, 0, %p11;
     	add.s32 	%r136, %r78, %r83;
     	setp.eq.s32 	%p12, %r82, %r2;
     	selp.u32 	%r84, 1, 0, %p12;
     	add.s32 	%r137, %r80, %r84;
     	add.s32 	%r130, %r130, 4;
     	add.s64 	%rd95, %rd95, 16;
     	add.s32 	%r127, %r127, -4;
     	setp.ne.s32 	%p13, %r127, 0;
     	@%p13 bra 	$L__BB9_5;

     $L__BB9_6:
     	setp.eq.s32 	%p14, %r135, 0;
     	@%p14 bra 	$L__BB9_9;

     	mul.wide.s32 	%rd57, %r130, 4;
     	add.s64 	%rd96, %rd1, %rd57;

     $L__BB9_8:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r85, [%rd96];
     	mul.wide.s32 	%rd58, %r85, 4;
     	add.s64 	%rd59, %rd41, %rd58;
     	ld.global.u32 	%r86, [%rd59];
     	setp.eq.s32 	%p15, %r86, %r4;
     	selp.u32 	%r87, 1, 0, %p15;
     	add.s32 	%r136, %r136, %r87;
     	setp.eq.s32 	%p16, %r86, %r2;
     	selp.u32 	%r88, 1, 0, %p16;
     	add.s32 	%r137, %r137, %r88;
     	add.s64 	%rd96, %rd96, 4;
     	add.s32 	%r135, %r135, -1;
     	setp.ne.s32 	%p17, %r135, 0;
     	@%p17 bra 	$L__BB9_8;

     $L__BB9_9:
     	cvt.rn.f32.s32 	%f10, %r137;
     	cvt.rn.f32.s32 	%f9, %r136;

     $L__BB9_10:
     	sub.ftz.f32 	%f6, %f10, %f9;
     	setp.le.ftz.f32 	%p18, %f6, 0f3F000000;
     	setp.lt.ftz.f32 	%p19, %f1, 0fBA83126F;
     	and.pred  	%p20, %p19, %p18;
     	@%p20 bra 	$L__BB9_12;

     	setp.gtu.ftz.f32 	%p21, %f6, 0f40400000;
     	setp.geu.ftz.f32 	%p22, %f1, 0fBC23D70A;
     	or.pred  	%p23, %p22, %p21;
     	@%p23 bra 	$L__BB9_32;

     $L__BB9_12:
     	add.s64 	%rd12, %rd2, %rd42;
     	mov.u32 	%r89, 1;
     	mov.u32 	%r90, 0;
     	atom.global.cas.b32 	%r91, [%rd12], %r90, %r89;
     	setp.ne.s32 	%p24, %r91, 0;
     	@%p24 bra 	$L__BB9_32;

     	@%p3 bra 	$L__BB9_24;

     	mov.u32 	%r138, %r6;

     $L__BB9_15:
     	mul.wide.s32 	%rd61, %r138, 4;
     	add.s64 	%rd62, %rd1, %rd61;
     	ld.global.nc.u32 	%r92, [%rd62];
     	mul.wide.s32 	%rd63, %r92, 4;
     	add.s64 	%rd64, %rd2, %rd63;
     	mov.u32 	%r93, 1;
     	mov.u32 	%r94, 0;
     	atom.global.cas.b32 	%r95, [%rd64], %r94, %r93;
     	setp.eq.s32 	%p26, %r95, 0;
     	@%p26 bra 	$L__BB9_23;
     	bra.uni 	$L__BB9_16;

     $L__BB9_23:
     	add.s32 	%r138, %r138, 1;
     	setp.lt.s32 	%p32, %r138, %r5;
     	@%p32 bra 	$L__BB9_15;

     $L__BB9_24:
     	st.global.u32 	[%rd43], %r2;
     	cvta.to.global.u64 	%rd81, %rd29;
     	atom.global.add.u32 	%r109, [%rd81], 1;
     	@%p3 bra 	$L__BB9_31;

     	sub.s32 	%r110, %r5, %r6;
     	and.b32  	%r144, %r110, 3;
     	setp.eq.s32 	%p34, %r144, 0;
     	mov.u32 	%r145, %r6;
     	@%p34 bra 	$L__BB9_28;

     	mul.wide.s32 	%rd82, %r6, 4;
     	add.s64 	%rd99, %rd1, %rd82;
     	mov.u32 	%r145, %r6;

     $L__BB9_27:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r111, [%rd99];
     	mul.wide.s32 	%rd83, %r111, 4;
     	add.s64 	%rd84, %rd2, %rd83;
     	atom.global.exch.b32 	%r112, [%rd84], 0;
     	add.s32 	%r145, %r145, 1;
     	add.s64 	%rd99, %rd99, 4;
     	add.s32 	%r144, %r144, -1;
     	setp.ne.s32 	%p35, %r144, 0;
     	@%p35 bra 	$L__BB9_27;

     $L__BB9_28:
     	not.b32 	%r113, %r6;
     	add.s32 	%r114, %r5, %r113;
     	setp.lt.u32 	%p36, %r114, 3;
     	@%p36 bra 	$L__BB9_31;

     	mul.wide.s32 	%rd85, %r145, 4;
     	add.s64 	%rd86, %rd1, %rd85;
     	add.s64 	%rd100, %rd86, 8;

     $L__BB9_30:
     	ld.global.nc.u32 	%r115, [%rd100+-8];
     	mul.wide.s32 	%rd87, %r115, 4;
     	add.s64 	%rd88, %rd2, %rd87;
     	atom.global.exch.b32 	%r116, [%rd88], 0;
     	ld.global.nc.u32 	%r117, [%rd100+-4];
     	mul.wide.s32 	%rd89, %r117, 4;
     	add.s64 	%rd90, %rd2, %rd89;
     	atom.global.exch.b32 	%r118, [%rd90], 0;
     	ld.global.nc.u32 	%r119, [%rd100];
     	mul.wide.s32 	%rd91, %r119, 4;
     	add.s64 	%rd92, %rd2, %rd91;
     	atom.global.exch.b32 	%r120, [%rd92], 0;
     	ld.global.nc.u32 	%r121, [%rd100+4];
     	mul.wide.s32 	%rd93, %r121, 4;
     	add.s64 	%rd94, %rd2, %rd93;
     	atom.global.exch.b32 	%r122, [%rd94], 0;
     	add.s64 	%rd100, %rd100, 16;
     	add.s32 	%r145, %r145, 4;
     	setp.lt.s32 	%p37, %r145, %r5;
     	@%p37 bra 	$L__BB9_30;
     	bra.uni 	$L__BB9_31;

     $L__BB9_16:
     	setp.le.s32 	%p27, %r138, %r6;
     	@%p27 bra 	$L__BB9_31;

     	sub.s32 	%r96, %r138, %r6;
     	and.b32  	%r140, %r96, 3;
     	setp.eq.s32 	%p28, %r140, 0;
     	mov.u32 	%r141, %r6;
     	@%p28 bra 	$L__BB9_20;

     	mul.wide.s32 	%rd65, %r6, 4;
     	add.s64 	%rd97, %rd1, %rd65;
     	mov.u32 	%r141, %r6;

     $L__BB9_19:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r97, [%rd97];
     	mul.wide.s32 	%rd66, %r97, 4;
     	add.s64 	%rd67, %rd2, %rd66;
     	atom.global.exch.b32 	%r98, [%rd67], 0;
     	add.s32 	%r141, %r141, 1;
     	add.s64 	%rd97, %rd97, 4;
     	add.s32 	%r140, %r140, -1;
     	setp.ne.s32 	%p29, %r140, 0;
     	@%p29 bra 	$L__BB9_19;

     $L__BB9_20:
     	not.b32 	%r99, %r6;
     	add.s32 	%r100, %r138, %r99;
     	setp.lt.u32 	%p30, %r100, 3;
     	@%p30 bra 	$L__BB9_31;

     	mul.wide.s32 	%rd68, %r141, 4;
     	add.s64 	%rd69, %rd1, %rd68;
     	add.s64 	%rd98, %rd69, 8;

     $L__BB9_22:
     	ld.global.nc.u32 	%r101, [%rd98+-8];
     	mul.wide.s32 	%rd70, %r101, 4;
     	add.s64 	%rd71, %rd2, %rd70;
     	atom.global.exch.b32 	%r102, [%rd71], 0;
     	ld.global.nc.u32 	%r103, [%rd98+-4];
     	mul.wide.s32 	%rd72, %r103, 4;
     	add.s64 	%rd73, %rd2, %rd72;
     	atom.global.exch.b32 	%r104, [%rd73], 0;
     	ld.global.nc.u32 	%r105, [%rd98];
     	mul.wide.s32 	%rd74, %r105, 4;
     	add.s64 	%rd75, %rd2, %rd74;
     	atom.global.exch.b32 	%r106, [%rd75], 0;
     	ld.global.nc.u32 	%r107, [%rd98+4];
     	mul.wide.s32 	%rd76, %r107, 4;
     	add.s64 	%rd77, %rd2, %rd76;
     	atom.global.exch.b32 	%r108, [%rd77], 0;
     	add.s64 	%rd98, %rd98, 16;
     	add.s32 	%r141, %r141, 4;
     	setp.lt.s32 	%p31, %r141, %r138;
     	@%p31 bra 	$L__BB9_22;

     $L__BB9_31:
     	atom.global.exch.b32 	%r123, [%rd12], 0;

     $L__BB9_32:
     	ret;

     }
     	// .globl	apply_moves_with_locking_f64
     .visible .entry apply_moves_with_locking_f64(
     	.param .u64 apply_moves_with_locking_f64_param_0,
     	.param .u64 apply_moves_with_locking_f64_param_1,
     	.param .u64 apply_moves_with_locking_f64_param_2,
     	.param .u64 apply_moves_with_locking_f64_param_3,
     	.param .u32 apply_moves_with_locking_f64_param_4,
     	.param .u32 apply_moves_with_locking_f64_param_5,
     	.param .u64 apply_moves_with_locking_f64_param_6,
     	.param .u64 apply_moves_with_locking_f64_param_7,
     	.param .u64 apply_moves_with_locking_f64_param_8,
     	.param .u64 apply_moves_with_locking_f64_param_9,
     	.param .u64 apply_moves_with_locking_f64_param_10,
     	.param .u64 apply_moves_with_locking_f64_param_11
     )
     {
     	.reg .pred 	%p<37>;
     	.reg .b32 	%r<101>;
     	.reg .f64 	%fd<70>;
     	.reg .b64 	%rd<113>;


     	ld.param.u64 	%rd33, [apply_moves_with_locking_f64_param_0];
     	ld.param.u64 	%rd27, [apply_moves_with_locking_f64_param_1];
     	ld.param.u64 	%rd28, [apply_moves_with_locking_f64_param_2];
     	ld.param.u64 	%rd29, [apply_moves_with_locking_f64_param_3];
     	ld.param.u32 	%r34, [apply_moves_with_locking_f64_param_4];
     	ld.param.u32 	%r33, [apply_moves_with_locking_f64_param_5];
     	ld.param.u64 	%rd34, [apply_moves_with_locking_f64_param_6];
     	ld.param.u64 	%rd30, [apply_moves_with_locking_f64_param_7];
     	ld.param.u64 	%rd31, [apply_moves_with_locking_f64_param_8];
     	ld.param.u64 	%rd35, [apply_moves_with_locking_f64_param_9];
     	ld.param.u64 	%rd36, [apply_moves_with_locking_f64_param_10];
     	ld.param.u64 	%rd32, [apply_moves_with_locking_f64_param_11];
     	cvta.to.global.u64 	%rd1, %rd36;
     	cvta.to.global.u64 	%rd2, %rd35;
     	cvta.to.global.u64 	%rd3, %rd34;
     	cvta.to.global.u64 	%rd4, %rd33;
     	mov.u32 	%r35, %ntid.x;
     	mov.u32 	%r36, %ctaid.x;
     	mov.u32 	%r37, %tid.x;
     	mad.lo.s32 	%r1, %r36, %r35, %r37;
     	setp.ge.s32 	%p1, %r1, %r34;
     	@%p1 bra 	$L__BB10_30;

     	cvta.to.global.u64 	%rd37, %rd27;
     	mul.wide.s32 	%rd38, %r1, 4;
     	add.s64 	%rd39, %rd37, %rd38;
     	cvta.to.global.u64 	%rd40, %rd28;
     	add.s64 	%rd41, %rd40, %rd38;
     	ld.global.nc.u32 	%r2, [%rd41];
     	ld.global.nc.u32 	%r38, [%rd39];
     	cvt.s64.s32 	%rd5, %r38;
     	mul.wide.s32 	%rd42, %r38, 4;
     	add.s64 	%rd6, %rd4, %rd42;
     	ld.global.u32 	%r3, [%rd6];
     	setp.eq.s32 	%p2, %r2, %r3;
     	@%p2 bra 	$L__BB10_30;

     	cvta.to.global.u64 	%rd43, %rd29;
     	mad.lo.s32 	%r39, %r1, %r33, %r2;
     	mul.wide.s32 	%rd44, %r39, 8;
     	add.s64 	%rd45, %rd43, %rd44;
     	ld.global.nc.f64 	%fd1, [%rd45];
     	cvta.to.global.u64 	%rd46, %rd31;
     	shl.b64 	%rd47, %rd5, 2;
     	add.s64 	%rd48, %rd46, %rd47;
     	ld.global.nc.u32 	%r4, [%rd48];
     	ld.global.nc.u32 	%r5, [%rd48+4];
     	setp.le.s32 	%p3, %r5, %r4;
     	mov.f64 	%fd68, 0d0000000000000000;
     	mov.f64 	%fd69, %fd68;
     	@%p3 bra 	$L__BB10_9;

     	cvta.to.global.u64 	%rd49, %rd32;
     	add.s64 	%rd51, %rd49, %rd47;
     	ld.global.nc.u32 	%r6, [%rd51];
     	sub.s32 	%r40, %r5, %r4;
     	and.b32  	%r89, %r40, 3;
     	setp.eq.s32 	%p4, %r89, 0;
     	mov.f64 	%fd69, 0d0000000000000000;
     	mov.u32 	%r90, %r4;
     	mov.f64 	%fd68, %fd69;
     	@%p4 bra 	$L__BB10_6;

     	mul.wide.s32 	%rd52, %r4, 4;
     	add.s64 	%rd107, %rd2, %rd52;
     	mov.f64 	%fd69, 0d0000000000000000;
     	mov.u32 	%r90, %r4;

     $L__BB10_5:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r41, [%rd107];
     	mul.wide.s32 	%rd53, %r41, 4;
     	add.s64 	%rd54, %rd4, %rd53;
     	mul.wide.s32 	%rd55, %r41, 8;
     	add.s64 	%rd56, %rd1, %rd55;
     	ld.global.nc.f64 	%fd23, [%rd56];
     	fma.rn.f64 	%fd24, %fd23, 0d3FE0000000000000, 0d3FF0000000000000;
     	mul.f64 	%fd25, %fd24, 0d3FF3333333333333;
     	setp.eq.s32 	%p5, %r6, 0;
     	selp.f64 	%fd26, %fd24, %fd25, %p5;
     	ld.global.u32 	%r42, [%rd54];
     	setp.eq.s32 	%p6, %r42, %r3;
     	add.f64 	%fd27, %fd68, %fd26;
     	selp.f64 	%fd68, %fd27, %fd68, %p6;
     	setp.eq.s32 	%p7, %r42, %r2;
     	add.f64 	%fd28, %fd69, %fd26;
     	selp.f64 	%fd69, %fd28, %fd69, %p7;
     	add.s32 	%r90, %r90, 1;
     	add.s64 	%rd107, %rd107, 4;
     	add.s32 	%r89, %r89, -1;
     	setp.ne.s32 	%p8, %r89, 0;
     	@%p8 bra 	$L__BB10_5;

     $L__BB10_6:
     	not.b32 	%r43, %r4;
     	add.s32 	%r44, %r5, %r43;
     	setp.lt.u32 	%p9, %r44, 3;
     	@%p9 bra 	$L__BB10_9;

     	mul.wide.s32 	%rd57, %r90, 4;
     	add.s64 	%rd58, %rd2, %rd57;
     	add.s64 	%rd108, %rd58, 8;

     $L__BB10_8:
     	ld.global.nc.u32 	%r45, [%rd108+-8];
     	mul.wide.s32 	%rd59, %r45, 4;
     	add.s64 	%rd60, %rd4, %rd59;
     	mul.wide.s32 	%rd61, %r45, 8;
     	add.s64 	%rd62, %rd1, %rd61;
     	ld.global.nc.f64 	%fd29, [%rd62];
     	fma.rn.f64 	%fd30, %fd29, 0d3FE0000000000000, 0d3FF0000000000000;
     	mul.f64 	%fd31, %fd30, 0d3FF3333333333333;
     	setp.eq.s32 	%p10, %r6, 0;
     	selp.f64 	%fd32, %fd30, %fd31, %p10;
     	ld.global.u32 	%r46, [%rd60];
     	setp.eq.s32 	%p11, %r46, %r3;
     	add.f64 	%fd33, %fd68, %fd32;
     	selp.f64 	%fd34, %fd33, %fd68, %p11;
     	setp.eq.s32 	%p12, %r46, %r2;
     	add.f64 	%fd35, %fd69, %fd32;
     	selp.f64 	%fd36, %fd35, %fd69, %p12;
     	ld.global.nc.u32 	%r47, [%rd108+-4];
     	mul.wide.s32 	%rd63, %r47, 4;
     	add.s64 	%rd64, %rd4, %rd63;
     	mul.wide.s32 	%rd65, %r47, 8;
     	add.s64 	%rd66, %rd1, %rd65;
     	ld.global.nc.f64 	%fd37, [%rd66];
     	fma.rn.f64 	%fd38, %fd37, 0d3FE0000000000000, 0d3FF0000000000000;
     	mul.f64 	%fd39, %fd38, 0d3FF3333333333333;
     	selp.f64 	%fd40, %fd38, %fd39, %p10;
     	ld.global.u32 	%r48, [%rd64];
     	setp.eq.s32 	%p13, %r48, %r3;
     	add.f64 	%fd41, %fd34, %fd40;
     	selp.f64 	%fd42, %fd41, %fd34, %p13;
     	setp.eq.s32 	%p14, %r48, %r2;
     	add.f64 	%fd43, %fd36, %fd40;
     	selp.f64 	%fd44, %fd43, %fd36, %p14;
     	ld.global.nc.u32 	%r49, [%rd108];
     	mul.wide.s32 	%rd67, %r49, 4;
     	add.s64 	%rd68, %rd4, %rd67;
     	mul.wide.s32 	%rd69, %r49, 8;
     	add.s64 	%rd70, %rd1, %rd69;
     	ld.global.nc.f64 	%fd45, [%rd70];
     	fma.rn.f64 	%fd46, %fd45, 0d3FE0000000000000, 0d3FF0000000000000;
     	mul.f64 	%fd47, %fd46, 0d3FF3333333333333;
     	selp.f64 	%fd48, %fd46, %fd47, %p10;
     	ld.global.u32 	%r50, [%rd68];
     	setp.eq.s32 	%p15, %r50, %r3;
     	add.f64 	%fd49, %fd42, %fd48;
     	selp.f64 	%fd50, %fd49, %fd42, %p15;
     	setp.eq.s32 	%p16, %r50, %r2;
     	add.f64 	%fd51, %fd44, %fd48;
     	selp.f64 	%fd52, %fd51, %fd44, %p16;
     	ld.global.nc.u32 	%r51, [%rd108+4];
     	mul.wide.s32 	%rd71, %r51, 4;
     	add.s64 	%rd72, %rd4, %rd71;
     	mul.wide.s32 	%rd73, %r51, 8;
     	add.s64 	%rd74, %rd1, %rd73;
     	ld.global.nc.f64 	%fd53, [%rd74];
     	fma.rn.f64 	%fd54, %fd53, 0d3FE0000000000000, 0d3FF0000000000000;
     	mul.f64 	%fd55, %fd54, 0d3FF3333333333333;
     	selp.f64 	%fd56, %fd54, %fd55, %p10;
     	ld.global.u32 	%r52, [%rd72];
     	setp.eq.s32 	%p17, %r52, %r3;
     	add.f64 	%fd57, %fd50, %fd56;
     	selp.f64 	%fd68, %fd57, %fd50, %p17;
     	setp.eq.s32 	%p18, %r52, %r2;
     	add.f64 	%fd58, %fd52, %fd56;
     	selp.f64 	%fd69, %fd58, %fd52, %p18;
     	add.s64 	%rd108, %rd108, 16;
     	add.s32 	%r90, %r90, 4;
     	setp.lt.s32 	%p19, %r90, %r5;
     	@%p19 bra 	$L__BB10_8;

     $L__BB10_9:
     	sub.f64 	%fd59, %fd69, %fd68;
     	setp.gtu.f64 	%p20, %fd59, 0d0000000000000000;
     	setp.geu.f64 	%p21, %fd1, 0dBF50624DD2F1A9FC;
     	or.pred  	%p22, %p21, %p20;
     	@%p22 bra 	$L__BB10_30;

     	add.s64 	%rd13, %rd3, %rd47;
     	mov.u32 	%r53, 1;
     	mov.u32 	%r54, 0;
     	atom.global.cas.b32 	%r55, [%rd13], %r54, %r53;
     	setp.ne.s32 	%p23, %r55, 0;
     	@%p23 bra 	$L__BB10_30;

     	@%p3 bra 	$L__BB10_22;

     	cvt.s64.s32 	%rd14, %r4;
     	mov.u32 	%r92, %r4;

     $L__BB10_13:
     	mul.wide.s32 	%rd76, %r92, 4;
     	add.s64 	%rd77, %rd2, %rd76;
     	ld.global.nc.u32 	%r56, [%rd77];
     	mul.wide.s32 	%rd78, %r56, 4;
     	add.s64 	%rd79, %rd3, %rd78;
     	mov.u32 	%r57, 1;
     	mov.u32 	%r58, 0;
     	atom.global.cas.b32 	%r59, [%rd79], %r58, %r57;
     	setp.eq.s32 	%p25, %r59, 0;
     	@%p25 bra 	$L__BB10_21;
     	bra.uni 	$L__BB10_14;

     $L__BB10_21:
     	add.s32 	%r92, %r92, 1;
     	setp.lt.s32 	%p31, %r92, %r5;
     	@%p31 bra 	$L__BB10_13;

     $L__BB10_22:
     	st.global.u32 	[%rd6], %r2;
     	cvta.to.global.u64 	%rd93, %rd30;
     	atom.global.add.u32 	%r73, [%rd93], 1;
     	@%p3 bra 	$L__BB10_29;

     	sub.s32 	%r74, %r5, %r4;
     	and.b32  	%r98, %r74, 3;
     	setp.eq.s32 	%p33, %r98, 0;
     	mov.u32 	%r99, %r4;
     	@%p33 bra 	$L__BB10_26;

     	mul.wide.s32 	%rd94, %r4, 4;
     	add.s64 	%rd111, %rd2, %rd94;
     	mov.u32 	%r99, %r4;

     $L__BB10_25:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r75, [%rd111];
     	mul.wide.s32 	%rd95, %r75, 4;
     	add.s64 	%rd96, %rd3, %rd95;
     	atom.global.exch.b32 	%r76, [%rd96], 0;
     	add.s32 	%r99, %r99, 1;
     	add.s64 	%rd111, %rd111, 4;
     	add.s32 	%r98, %r98, -1;
     	setp.ne.s32 	%p34, %r98, 0;
     	@%p34 bra 	$L__BB10_25;

     $L__BB10_26:
     	not.b32 	%r77, %r4;
     	add.s32 	%r78, %r5, %r77;
     	setp.lt.u32 	%p35, %r78, 3;
     	@%p35 bra 	$L__BB10_29;

     	mul.wide.s32 	%rd97, %r99, 4;
     	add.s64 	%rd98, %rd2, %rd97;
     	add.s64 	%rd112, %rd98, 8;

     $L__BB10_28:
     	ld.global.nc.u32 	%r79, [%rd112+-8];
     	mul.wide.s32 	%rd99, %r79, 4;
     	add.s64 	%rd100, %rd3, %rd99;
     	atom.global.exch.b32 	%r80, [%rd100], 0;
     	ld.global.nc.u32 	%r81, [%rd112+-4];
     	mul.wide.s32 	%rd101, %r81, 4;
     	add.s64 	%rd102, %rd3, %rd101;
     	atom.global.exch.b32 	%r82, [%rd102], 0;
     	ld.global.nc.u32 	%r83, [%rd112];
     	mul.wide.s32 	%rd103, %r83, 4;
     	add.s64 	%rd104, %rd3, %rd103;
     	atom.global.exch.b32 	%r84, [%rd104], 0;
     	ld.global.nc.u32 	%r85, [%rd112+4];
     	mul.wide.s32 	%rd105, %r85, 4;
     	add.s64 	%rd106, %rd3, %rd105;
     	atom.global.exch.b32 	%r86, [%rd106], 0;
     	add.s64 	%rd112, %rd112, 16;
     	add.s32 	%r99, %r99, 4;
     	setp.lt.s32 	%p36, %r99, %r5;
     	@%p36 bra 	$L__BB10_28;
     	bra.uni 	$L__BB10_29;

     $L__BB10_14:
     	setp.le.s32 	%p26, %r92, %r4;
     	@%p26 bra 	$L__BB10_29;

     	sub.s32 	%r60, %r92, %r4;
     	and.b32  	%r94, %r60, 3;
     	setp.eq.s32 	%p27, %r94, 0;
     	mov.u32 	%r95, %r4;
     	@%p27 bra 	$L__BB10_18;

     	shl.b64 	%rd80, %rd14, 2;
     	add.s64 	%rd109, %rd2, %rd80;
     	mov.u32 	%r95, %r4;

     $L__BB10_17:
     	.pragma "nounroll";
     	ld.global.nc.u32 	%r61, [%rd109];
     	mul.wide.s32 	%rd81, %r61, 4;
     	add.s64 	%rd82, %rd3, %rd81;
     	atom.global.exch.b32 	%r62, [%rd82], 0;
     	add.s32 	%r95, %r95, 1;
     	add.s64 	%rd109, %rd109, 4;
     	add.s32 	%r94, %r94, -1;
     	setp.ne.s32 	%p28, %r94, 0;
     	@%p28 bra 	$L__BB10_17;

     $L__BB10_18:
     	not.b32 	%r63, %r4;
     	add.s32 	%r64, %r92, %r63;
     	setp.lt.u32 	%p29, %r64, 3;
     	@%p29 bra 	$L__BB10_29;

     	mul.wide.s32 	%rd83, %r95, 4;
     	add.s64 	%rd84, %rd2, %rd83;
     	add.s64 	%rd110, %rd84, 8;

     $L__BB10_20:
     	ld.global.nc.u32 	%r65, [%rd110+-8];
     	mul.wide.s32 	%rd85, %r65, 4;
     	add.s64 	%rd86, %rd3, %rd85;
     	atom.global.exch.b32 	%r66, [%rd86], 0;
     	ld.global.nc.u32 	%r67, [%rd110+-4];
     	mul.wide.s32 	%rd87, %r67, 4;
     	add.s64 	%rd88, %rd3, %rd87;
     	atom.global.exch.b32 	%r68, [%rd88], 0;
     	ld.global.nc.u32 	%r69, [%rd110];
     	mul.wide.s32 	%rd89, %r69, 4;
     	add.s64 	%rd90, %rd3, %rd89;
     	atom.global.exch.b32 	%r70, [%rd90], 0;
     	ld.global.nc.u32 	%r71, [%rd110+4];
     	mul.wide.s32 	%rd91, %r71, 4;
     	add.s64 	%rd92, %rd3, %rd91;
     	atom.global.exch.b32 	%r72, [%rd92], 0;
     	add.s64 	%rd110, %rd110, 16;
     	add.s32 	%r95, %r95, 4;
     	setp.lt.s32 	%p30, %r95, %r92;
     	@%p30 bra 	$L__BB10_20;

     $L__BB10_29:
     	atom.global.exch.b32 	%r87, [%rd13], 0;

     $L__BB10_30:
     	ret;

     }
     	// .globl	compute_wavelet_priorities
     .visible .entry compute_wavelet_priorities(
     	.param .u64 compute_wavelet_priorities_param_0,
     	.param .u64 compute_wavelet_priorities_param_1,
     	.param .u64 compute_wavelet_priorities_param_2,
     	.param .u64 compute_wavelet_priorities_param_3,
     	.param .u64 compute_wavelet_priorities_param_4,
     	.param .u32 compute_wavelet_priorities_param_5
     )
     {
     	.reg .pred 	%p<4>;
     	.reg .f32 	%f<8>;
     	.reg .b32 	%r<8>;
     	.reg .f64 	%fd<6>;
     	.reg .b64 	%rd<20>;


     	ld.param.u64 	%rd3, [compute_wavelet_priorities_param_0];
     	ld.param.u64 	%rd4, [compute_wavelet_priorities_param_1];
     	ld.param.u64 	%rd5, [compute_wavelet_priorities_param_2];
     	ld.param.u64 	%rd6, [compute_wavelet_priorities_param_3];
     	ld.param.u64 	%rd7, [compute_wavelet_priorities_param_4];
     	ld.param.u32 	%r2, [compute_wavelet_priorities_param_5];
     	mov.u32 	%r3, %ntid.x;
     	mov.u32 	%r4, %ctaid.x;
     	mov.u32 	%r5, %tid.x;
     	mad.lo.s32 	%r1, %r4, %r3, %r5;
     	setp.ge.s32 	%p1, %r1, %r2;
     	@%p1 bra 	$L__BB11_4;

     	cvta.to.global.u64 	%rd8, %rd4;
     	cvt.s64.s32 	%rd1, %r1;
     	mul.wide.s32 	%rd9, %r1, 4;
     	add.s64 	%rd10, %rd8, %rd9;
     	ld.global.nc.f32 	%f1, [%rd10];
     	setp.lt.ftz.f32 	%p2, %f1, 0f3F000000;
     	cvta.to.global.u64 	%rd11, %rd7;
     	add.s64 	%rd2, %rd11, %rd9;
     	@%p2 bra 	$L__BB11_3;
     	bra.uni 	$L__BB11_2;

     $L__BB11_3:
     	mov.u32 	%r7, -1082130432;
     	st.global.u32 	[%rd2], %r7;
     	bra.uni 	$L__BB11_4;

     $L__BB11_2:
     	cvta.to.global.u64 	%rd12, %rd3;
     	shl.b64 	%rd13, %rd1, 3;
     	add.s64 	%rd14, %rd12, %rd13;
     	ld.global.nc.f64 	%fd1, [%rd14];
     	abs.f64 	%fd2, %fd1;
     	cvta.to.global.u64 	%rd15, %rd6;
     	shl.b64 	%rd16, %rd1, 2;
     	add.s64 	%rd17, %rd15, %rd16;
     	ld.global.nc.u32 	%r6, [%rd17];
     	setp.eq.s32 	%p3, %r6, 0;
     	selp.f32 	%f2, 0f00000000, 0f40000000, %p3;
     	mul.f64 	%fd3, %fd2, 0d4008000000000000;
     	cvt.rn.ftz.f32.f64 	%f3, %fd3;
     	add.ftz.f32 	%f4, %f1, %f3;
     	cvta.to.global.u64 	%rd18, %rd5;
     	add.s64 	%rd19, %rd18, %rd13;
     	ld.global.nc.f64 	%fd4, [%rd19];
     	mul.f64 	%fd5, %fd4, 0d3FE0000000000000;
     	cvt.rn.ftz.f32.f64 	%f5, %fd5;
     	add.ftz.f32 	%f6, %f4, %f5;
     	add.ftz.f32 	%f7, %f6, %f2;
     	st.global.f32 	[%rd2], %f7;

     $L__BB11_4:
     	ret;

     }
     	// .globl	whcr_iteration
     .visible .entry whcr_iteration(
     	.param .u64 whcr_iteration_param_0,
     	.param .align 8 .b8 whcr_iteration_param_1[32],
     	.param .align 8 .b8 whcr_iteration_param_2[40],
     	.param .u64 whcr_iteration_param_3,
     	.param .u64 whcr_iteration_param_4,
     	.param .u64 whcr_iteration_param_5,
     	.param .u64 whcr_iteration_param_6,
     	.param .u32 whcr_iteration_param_7,
     	.param .u32 whcr_iteration_param_8,
     	.param .u64 whcr_iteration_param_9
     )
     {
     	.reg .pred 	%p<29>;
     	.reg .f32 	%f<25>;
     	.reg .b32 	%r<88>;
     	.reg .f64 	%fd<40>;
     	.reg .b64 	%rd<85>;


     	ld.param.u64 	%rd29, [whcr_iteration_param_0];
     	ld.param.v2.u32 	{%r30, %r31}, [whcr_iteration_param_1];
     	ld.param.u64 	%rd26, [whcr_iteration_param_3];
     	ld.param.u64 	%rd27, [whcr_iteration_param_4];
     	ld.param.u32 	%r32, [whcr_iteration_param_7];
     	ld.param.u64 	%rd28, [whcr_iteration_param_9];
     	ld.param.u64 	%rd30, [whcr_iteration_param_2];
     	ld.param.u64 	%rd31, [whcr_iteration_param_1+16];
     	ld.param.u64 	%rd32, [whcr_iteration_param_1+8];
     	cvta.to.global.u64 	%rd33, %rd32;
     	cvta.to.global.u64 	%rd1, %rd31;
     	cvta.to.global.u64 	%rd2, %rd30;
     	mov.u32 	%r33, %ntid.x;
     	mov.u32 	%r2, %ctaid.x;
     	mov.u32 	%r3, %tid.x;
     	mad.lo.s32 	%r4, %r2, %r33, %r3;
     	// begin inline asm
     	mov.u32 %r28, %envreg2;
     	// end inline asm
     	cvt.u64.u32 	%rd34, %r28;
     	// begin inline asm
     	mov.u32 %r29, %envreg1;
     	// end inline asm
     	cvt.u64.u32 	%rd35, %r29;
     	bfi.b64 	%rd3, %rd35, %rd34, 32, 32;
     	cvt.s64.s32 	%rd4, %r4;
     	cvta.to.global.u64 	%rd5, %rd29;
     	mul.wide.s32 	%rd36, %r4, 4;
     	add.s64 	%rd6, %rd5, %rd36;
     	add.s64 	%rd7, %rd33, %rd36;
     	setp.eq.s32 	%p1, %r32, 0;
     	@%p1 bra 	$L__BB12_20;

     	setp.ge.s32 	%p2, %r4, %r30;
     	@%p2 bra 	$L__BB12_29;

     	ld.global.u32 	%r5, [%rd6];
     	ld.global.u32 	%r6, [%rd7];
     	ld.global.u32 	%r7, [%rd7+4];
     	setp.le.s32 	%p3, %r7, %r6;
     	mov.f64 	%fd35, 0d0000000000000000;
     	@%p3 bra 	$L__BB12_19;

     	sub.s32 	%r34, %r7, %r6;
     	and.b32  	%r81, %r34, 3;
     	setp.eq.s32 	%p4, %r81, 0;
     	mov.f64 	%fd35, 0d0000000000000000;
     	mov.u32 	%r82, %r6;
     	@%p4 bra 	$L__BB12_8;

     	mul.wide.s32 	%rd37, %r6, 4;
     	add.s64 	%rd81, %rd1, %rd37;
     	mov.f64 	%fd35, 0d0000000000000000;
     	mov.u32 	%r82, %r6;

     $L__BB12_5:
     	.pragma "nounroll";
     	ld.global.u32 	%r35, [%rd81];
     	cvt.s64.s32 	%rd10, %r35;
     	mul.wide.s32 	%rd38, %r35, 4;
     	add.s64 	%rd39, %rd5, %rd38;
     	ld.global.u32 	%r36, [%rd39];
     	setp.ne.s32 	%p5, %r36, %r5;
     	@%p5 bra 	$L__BB12_7;

     	shl.b64 	%rd40, %rd10, 3;
     	add.s64 	%rd41, %rd2, %rd40;
     	ld.global.f64 	%fd20, [%rd41];
     	fma.rn.f64 	%fd21, %fd20, 0d3FE0000000000000, 0d3FF0000000000000;
     	add.f64 	%fd35, %fd35, %fd21;

     $L__BB12_7:
     	add.s32 	%r82, %r82, 1;
     	add.s64 	%rd81, %rd81, 4;
     	add.s32 	%r81, %r81, -1;
     	setp.ne.s32 	%p6, %r81, 0;
     	@%p6 bra 	$L__BB12_5;

     $L__BB12_8:
     	not.b32 	%r37, %r6;
     	add.s32 	%r38, %r7, %r37;
     	setp.lt.u32 	%p7, %r38, 3;
     	@%p7 bra 	$L__BB12_19;

     	mul.wide.s32 	%rd42, %r82, 4;
     	add.s64 	%rd82, %rd1, %rd42;

     $L__BB12_10:
     	ld.global.u32 	%r39, [%rd82];
     	cvt.s64.s32 	%rd14, %r39;
     	mul.wide.s32 	%rd43, %r39, 4;
     	add.s64 	%rd44, %rd5, %rd43;
     	ld.global.u32 	%r40, [%rd44];
     	setp.ne.s32 	%p8, %r40, %r5;
     	@%p8 bra 	$L__BB12_12;

     	shl.b64 	%rd45, %rd14, 3;
     	add.s64 	%rd46, %rd2, %rd45;
     	ld.global.f64 	%fd22, [%rd46];
     	fma.rn.f64 	%fd23, %fd22, 0d3FE0000000000000, 0d3FF0000000000000;
     	add.f64 	%fd35, %fd35, %fd23;

     $L__BB12_12:
     	ld.global.u32 	%r41, [%rd82+4];
     	cvt.s64.s32 	%rd15, %r41;
     	mul.wide.s32 	%rd47, %r41, 4;
     	add.s64 	%rd48, %rd5, %rd47;
     	ld.global.u32 	%r42, [%rd48];
     	setp.ne.s32 	%p9, %r42, %r5;
     	@%p9 bra 	$L__BB12_14;

     	shl.b64 	%rd49, %rd15, 3;
     	add.s64 	%rd50, %rd2, %rd49;
     	ld.global.f64 	%fd24, [%rd50];
     	fma.rn.f64 	%fd25, %fd24, 0d3FE0000000000000, 0d3FF0000000000000;
     	add.f64 	%fd35, %fd35, %fd25;

     $L__BB12_14:
     	ld.global.u32 	%r43, [%rd82+8];
     	cvt.s64.s32 	%rd16, %r43;
     	mul.wide.s32 	%rd51, %r43, 4;
     	add.s64 	%rd52, %rd5, %rd51;
     	ld.global.u32 	%r44, [%rd52];
     	setp.ne.s32 	%p10, %r44, %r5;
     	@%p10 bra 	$L__BB12_16;

     	shl.b64 	%rd53, %rd16, 3;
     	add.s64 	%rd54, %rd2, %rd53;
     	ld.global.f64 	%fd26, [%rd54];
     	fma.rn.f64 	%fd27, %fd26, 0d3FE0000000000000, 0d3FF0000000000000;
     	add.f64 	%fd35, %fd35, %fd27;

     $L__BB12_16:
     	ld.global.u32 	%r45, [%rd82+12];
     	cvt.s64.s32 	%rd17, %r45;
     	mul.wide.s32 	%rd55, %r45, 4;
     	add.s64 	%rd56, %rd5, %rd55;
     	ld.global.u32 	%r46, [%rd56];
     	setp.ne.s32 	%p11, %r46, %r5;
     	@%p11 bra 	$L__BB12_18;

     	shl.b64 	%rd57, %rd17, 3;
     	add.s64 	%rd58, %rd2, %rd57;
     	ld.global.f64 	%fd28, [%rd58];
     	fma.rn.f64 	%fd29, %fd28, 0d3FE0000000000000, 0d3FF0000000000000;
     	add.f64 	%fd35, %fd35, %fd29;

     $L__BB12_18:
     	add.s64 	%rd82, %rd82, 16;
     	add.s32 	%r82, %r82, 4;
     	setp.lt.s32 	%p12, %r82, %r7;
     	@%p12 bra 	$L__BB12_10;

     $L__BB12_19:
     	cvta.to.global.u64 	%rd59, %rd27;
     	mul.wide.s32 	%rd60, %r4, 8;
     	add.s64 	%rd61, %rd59, %rd60;
     	st.global.f64 	[%rd61], %fd35;
     	bra.uni 	$L__BB12_29;

     $L__BB12_20:
     	setp.ge.s32 	%p13, %r4, %r30;
     	@%p13 bra 	$L__BB12_29;

     	ld.global.u32 	%r16, [%rd6];
     	ld.global.u32 	%r17, [%rd7];
     	ld.global.u32 	%r18, [%rd7+4];
     	setp.le.s32 	%p14, %r18, %r17;
     	mov.f32 	%f24, 0f00000000;
     	@%p14 bra 	$L__BB12_28;

     	sub.s32 	%r47, %r18, %r17;
     	and.b32  	%r85, %r47, 3;
     	setp.eq.s32 	%p15, %r85, 0;
     	mov.f32 	%f24, 0f00000000;
     	mov.u32 	%r86, %r17;
     	@%p15 bra 	$L__BB12_25;

     	mul.wide.s32 	%rd62, %r17, 4;
     	add.s64 	%rd83, %rd1, %rd62;
     	mov.f32 	%f24, 0f00000000;
     	mov.u32 	%r86, %r17;

     $L__BB12_24:
     	.pragma "nounroll";
     	ld.global.u32 	%r48, [%rd83];
     	mul.wide.s32 	%rd63, %r48, 4;
     	add.s64 	%rd64, %rd5, %rd63;
     	ld.global.u32 	%r49, [%rd64];
     	setp.eq.s32 	%p16, %r49, %r16;
     	add.ftz.f32 	%f12, %f24, 0f3F800000;
     	selp.f32 	%f24, %f12, %f24, %p16;
     	add.s32 	%r86, %r86, 1;
     	add.s64 	%rd83, %rd83, 4;
     	add.s32 	%r85, %r85, -1;
     	setp.ne.s32 	%p17, %r85, 0;
     	@%p17 bra 	$L__BB12_24;

     $L__BB12_25:
     	not.b32 	%r50, %r17;
     	add.s32 	%r51, %r18, %r50;
     	setp.lt.u32 	%p18, %r51, 3;
     	@%p18 bra 	$L__BB12_28;

     	mul.wide.s32 	%rd65, %r86, 4;
     	add.s64 	%rd66, %rd1, %rd65;
     	add.s64 	%rd84, %rd66, 8;

     $L__BB12_27:
     	ld.global.u32 	%r52, [%rd84+-8];
     	mul.wide.s32 	%rd67, %r52, 4;
     	add.s64 	%rd68, %rd5, %rd67;
     	ld.global.u32 	%r53, [%rd68];
     	setp.eq.s32 	%p19, %r53, %r16;
     	add.ftz.f32 	%f13, %f24, 0f3F800000;
     	selp.f32 	%f14, %f13, %f24, %p19;
     	ld.global.u32 	%r54, [%rd84+-4];
     	mul.wide.s32 	%rd69, %r54, 4;
     	add.s64 	%rd70, %rd5, %rd69;
     	ld.global.u32 	%r55, [%rd70];
     	setp.eq.s32 	%p20, %r55, %r16;
     	add.ftz.f32 	%f15, %f14, 0f3F800000;
     	selp.f32 	%f16, %f15, %f14, %p20;
     	ld.global.u32 	%r56, [%rd84];
     	mul.wide.s32 	%rd71, %r56, 4;
     	add.s64 	%rd72, %rd5, %rd71;
     	ld.global.u32 	%r57, [%rd72];
     	setp.eq.s32 	%p21, %r57, %r16;
     	add.ftz.f32 	%f17, %f16, 0f3F800000;
     	selp.f32 	%f18, %f17, %f16, %p21;
     	ld.global.u32 	%r58, [%rd84+4];
     	mul.wide.s32 	%rd73, %r58, 4;
     	add.s64 	%rd74, %rd5, %rd73;
     	ld.global.u32 	%r59, [%rd74];
     	setp.eq.s32 	%p22, %r59, %r16;
     	add.ftz.f32 	%f19, %f18, 0f3F800000;
     	selp.f32 	%f24, %f19, %f18, %p22;
     	add.s64 	%rd84, %rd84, 16;
     	add.s32 	%r86, %r86, 4;
     	setp.lt.s32 	%p23, %r86, %r18;
     	@%p23 bra 	$L__BB12_27;

     $L__BB12_28:
     	cvta.to.global.u64 	%rd75, %rd26;
     	shl.b64 	%rd76, %rd4, 2;
     	add.s64 	%rd77, %rd75, %rd76;
     	st.global.f32 	[%rd77], %f24;

     $L__BB12_29:
     	setp.ne.s64 	%p24, %rd3, 0;
     	@%p24 bra 	$L__BB12_31;

     	// begin inline asm
     	trap;
     	// end inline asm

     $L__BB12_31:
     	barrier.sync 	0;
     	mov.u32 	%r60, %tid.y;
     	add.s32 	%r61, %r3, %r60;
     	mov.u32 	%r62, %tid.z;
     	neg.s32 	%r63, %r62;
     	setp.ne.s32 	%p25, %r61, %r63;
     	@%p25 bra 	$L__BB12_34;

     	add.s64 	%rd78, %rd3, 4;
     	mov.u32 	%r66, %nctaid.x;
     	mov.u32 	%r67, %nctaid.y;
     	mul.lo.s32 	%r68, %r66, %r67;
     	mov.u32 	%r69, %nctaid.z;
     	mul.lo.s32 	%r70, %r68, %r69;
     	mov.u32 	%r71, %ctaid.y;
     	add.s32 	%r72, %r2, %r71;
     	mov.u32 	%r73, %ctaid.z;
     	neg.s32 	%r74, %r73;
     	setp.eq.s32 	%p26, %r72, %r74;
     	mov.u32 	%r75, -2147483647;
     	sub.s32 	%r76, %r75, %r70;
     	selp.b32 	%r65, %r76, 1, %p26;
     	// begin inline asm
     	atom.add.release.gpu.u32 %r64,[%rd78],%r65;
     	// end inline asm

     $L__BB12_33:
     	// begin inline asm
     	ld.acquire.gpu.u32 %r77,[%rd78];
     	// end inline asm
     	xor.b32  	%r78, %r77, %r64;
     	setp.gt.s32 	%p27, %r78, -1;
     	@%p27 bra 	$L__BB12_33;

     $L__BB12_34:
     	barrier.sync 	0;
     	setp.ne.s32 	%p28, %r4, 0;
     	@%p28 bra 	$L__BB12_36;

     	cvta.to.global.u64 	%rd80, %rd28;
     	mov.u32 	%r79, 1;
     	st.global.u32 	[%rd80], %r79;

     $L__BB12_36:
     	ret;

     }
     //
     // Generated by NVIDIA NVVM Compiler
     //
     // Compiler Build ID: CL-35059454
     // Cuda compilation tools, release 12.6, V12.6.85
     // Based on NVVM 7.0.1
     //

     .version 8.5
     .target sm_86
     .address_size 64

     .extern .shared .align 16 .b8 shared_mem[];
     .extern .shared .align 16 .b8 block_shared[];

     .func  (.param .b32 func_retval0) _Z7digammaf(
     	.param .b32 _Z7digammaf_param_0
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .f32 	%f<26>;


     	ld.param.f32 	%f4, [_Z7digammaf_param_0];
     	setp.lt.ftz.f32 	%p1, %f4, 0f40C00000;
     	@%p1 bra 	$L__BB0_2;
     	bra.uni 	$L__BB0_1;

     $L__BB0_2:
     	add.ftz.f32 	%f22, %f4, 0f3F800000;
     	{ // callseq 0, 0
     	.reg .b32 temp_param_reg;
     	.param .b32 param0;
     	st.param.f32 	[param0+0], %f22;
     	.param .b32 retval0;
     	call.uni (retval0), 
     	_Z7digammaf, 
     	(
     	param0
     	);
     	ld.param.f32 	%f23, [retval0+0];
     	} // callseq 0
     	rcp.approx.ftz.f32 	%f24, %f4;
     	sub.ftz.f32 	%f25, %f23, %f24;
     	bra.uni 	$L__BB0_3;

     $L__BB0_1:
     	lg2.approx.ftz.f32 	%f5, %f4;
     	mul.ftz.f32 	%f6, %f5, 0f3F317218;
     	mov.f32 	%f7, 0f3F000000;
     	div.approx.ftz.f32 	%f8, %f7, %f4;
     	sub.ftz.f32 	%f9, %f6, %f8;
     	mul.ftz.f32 	%f10, %f4, %f4;
     	mul.ftz.f32 	%f11, %f10, 0fC1400000;
     	rcp.approx.ftz.f32 	%f12, %f11;
     	add.ftz.f32 	%f13, %f9, %f12;
     	mul.ftz.f32 	%f14, %f10, 0f42F00000;
     	mul.ftz.f32 	%f15, %f10, %f14;
     	rcp.approx.ftz.f32 	%f16, %f15;
     	add.ftz.f32 	%f17, %f16, %f13;
     	mul.ftz.f32 	%f18, %f10, 0fC37C0000;
     	mul.ftz.f32 	%f19, %f10, %f18;
     	mul.ftz.f32 	%f20, %f10, %f19;
     	rcp.approx.ftz.f32 	%f21, %f20;
     	add.ftz.f32 	%f25, %f17, %f21;

     $L__BB0_3:
     	st.param.f32 	[func_retval0+0], %f25;
     	ret;

     }
     	// .globl	transfer_entropy_ksg_kernel
     .visible .entry transfer_entropy_ksg_kernel(
     	.param .u64 transfer_entropy_ksg_kernel_param_0,
     	.param .u64 transfer_entropy_ksg_kernel_param_1,
     	.param .u64 transfer_entropy_ksg_kernel_param_2,
     	.param .u64 transfer_entropy_ksg_kernel_param_3,
     	.param .align 4 .b8 transfer_entropy_ksg_kernel_param_4[24]
     )
     {
     	.local .align 16 .b8 	__local_depot1[272];
     	.reg .b64 	%SP;
     	.reg .b64 	%SPL;
     	.reg .pred 	%p<118>;
     	.reg .f32 	%f<351>;
     	.reg .b32 	%r<504>;
     	.reg .b64 	%rd<138>;


     	mov.u64 	%SPL, __local_depot1;
     	ld.param.u64 	%rd30, [transfer_entropy_ksg_kernel_param_0];
     	ld.param.u64 	%rd28, [transfer_entropy_ksg_kernel_param_1];
     	ld.param.u64 	%rd29, [transfer_entropy_ksg_kernel_param_2];
     	ld.param.u32 	%r193, [transfer_entropy_ksg_kernel_param_4+12];
     	ld.param.u32 	%r192, [transfer_entropy_ksg_kernel_param_4+8];
     	ld.param.u32 	%r191, [transfer_entropy_ksg_kernel_param_4+4];
     	ld.param.u32 	%r190, [transfer_entropy_ksg_kernel_param_4];
     	cvta.to.global.u64 	%rd1, %rd30;
     	add.u64 	%rd2, %SPL, 0;
     	add.u64 	%rd3, %SPL, 128;
     	add.u64 	%rd4, %SPL, 144;
     	mov.u32 	%r195, %ntid.x;
     	mov.u32 	%r196, %ctaid.x;
     	mov.u32 	%r1, %tid.x;
     	mad.lo.s32 	%r2, %r196, %r195, %r1;
     	mul.lo.s32 	%r197, %r190, %r190;
     	setp.ge.s32 	%p1, %r2, %r197;
     	@%p1 bra 	$L__BB1_138;

     	div.s32 	%r4, %r2, %r190;
     	mul.lo.s32 	%r198, %r4, %r190;
     	sub.s32 	%r5, %r2, %r198;
     	setp.eq.s32 	%p2, %r4, %r5;
     	cvta.to.global.u64 	%rd34, %rd28;
     	mul.wide.s32 	%rd35, %r2, 4;
     	add.s64 	%rd5, %rd34, %rd35;
     	cvta.to.global.u64 	%rd36, %rd29;
     	add.s64 	%rd6, %rd36, %rd35;
     	@%p2 bra 	$L__BB1_137;
     	bra.uni 	$L__BB1_2;

     $L__BB1_137:
     	mov.u32 	%r382, 0;
     	st.global.u32 	[%rd5], %r382;
     	mov.u32 	%r383, 1065353216;
     	st.global.u32 	[%rd6], %r383;
     	bra.uni 	$L__BB1_138;

     $L__BB1_2:
     	mul.lo.s32 	%r9, %r191, %r1;
     	sub.s32 	%r10, %r191, %r193;
     	setp.le.s32 	%p3, %r10, %r192;
     	mov.f32 	%f350, 0f00000000;
     	mov.u32 	%r503, 0;
     	mov.f32 	%f349, %f350;
     	@%p3 bra 	$L__BB1_134;

     	mul.lo.s32 	%r11, %r5, %r191;
     	mul.lo.s32 	%r12, %r4, %r191;
     	sub.s32 	%r201, %r191, %r192;
     	sub.s32 	%r13, %r201, %r193;
     	{ // callseq 1, 0
     	.reg .b32 temp_param_reg;
     	.param .b32 param0;
     	st.param.f32 	[param0+0], 0f40800000;
     	.param .b32 retval0;
     	call.uni (retval0), 
     	_Z7digammaf, 
     	(
     	param0
     	);
     	ld.param.f32 	%f1, [retval0+0];
     	} // callseq 1
     	mov.u32 	%r503, 0;
     	{ // callseq 2, 0
     	.reg .b32 temp_param_reg;
     	.param .b32 param0;
     	st.param.f32 	[param0+0], 0f40A00000;
     	.param .b32 retval0;
     	call.uni (retval0), 
     	_Z7digammaf, 
     	(
     	param0
     	);
     	ld.param.f32 	%f2, [retval0+0];
     	} // callseq 2
     	add.s32 	%r14, %r192, -1;
     	shl.b32 	%r202, %r192, 1;
     	max.s32 	%r15, %r202, 0;
     	or.b32  	%r203, %r15, 1;
     	add.s32 	%r204, %r191, -1;
     	sub.s32 	%r16, %r204, %r192;
     	sub.s32 	%r205, %r204, %r193;
     	sub.s32 	%r17, %r205, %r192;
     	and.b32  	%r18, %r192, 3;
     	sub.s32 	%r19, %r192, %r18;
     	and.b32  	%r20, %r203, 3;
     	sub.s32 	%r21, %r203, %r20;
     	and.b32  	%r22, %r13, 1;
     	sub.s32 	%r23, %r13, %r22;
     	sub.s32 	%r206, %r10, %r192;
     	and.b32  	%r24, %r206, 3;
     	sub.s32 	%r25, %r206, %r24;
     	mov.f32 	%f349, 0f00000000;
     	mov.u32 	%r418, %r192;
     	bra.uni 	$L__BB1_4;

     $L__BB1_96:
     	setp.lt.u32 	%p69, %r17, 3;
     	mov.u32 	%r474, 0;
     	mov.u32 	%r483, %r192;
     	mov.u32 	%r473, %r474;
     	@%p69 bra 	$L__BB1_107;

     	mov.u32 	%r474, 0;
     	mov.u32 	%r483, %r192;
     	mov.u32 	%r472, %r25;

     $L__BB1_98:
     	add.s32 	%r335, %r483, %r11;
     	add.s32 	%r336, %r335, %r193;
     	mul.wide.s32 	%rd98, %r336, 4;
     	add.s64 	%rd22, %rd1, %rd98;
     	setp.eq.s32 	%p70, %r483, %r418;
     	@%p70 bra 	$L__BB1_100;

     	ld.global.nc.f32 	%f191, [%rd22];
     	sub.ftz.f32 	%f192, %f5, %f191;
     	fma.rn.ftz.f32 	%f193, %f192, %f192, 0f00000000;
     	add.ftz.f32 	%f194, %f193, 0f2EDBE6FF;
     	mov.f32 	%f195, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f196, %f194;
     	setp.lt.ftz.f32 	%p71, %f196, %f47;
     	selp.u32 	%r337, 1, 0, %p71;
     	add.s32 	%r473, %r473, %r337;
     	sqrt.approx.ftz.f32 	%f197, %f195;
     	setp.lt.ftz.f32 	%p72, %f197, %f47;
     	selp.u32 	%r338, 1, 0, %p72;
     	add.s32 	%r474, %r474, %r338;

     $L__BB1_100:
     	add.s32 	%r339, %r483, 1;
     	setp.eq.s32 	%p73, %r339, %r418;
     	@%p73 bra 	$L__BB1_102;

     	ld.global.nc.f32 	%f198, [%rd22+4];
     	sub.ftz.f32 	%f199, %f5, %f198;
     	fma.rn.ftz.f32 	%f200, %f199, %f199, 0f00000000;
     	add.ftz.f32 	%f201, %f200, 0f2EDBE6FF;
     	mov.f32 	%f202, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f203, %f201;
     	setp.lt.ftz.f32 	%p74, %f203, %f47;
     	selp.u32 	%r340, 1, 0, %p74;
     	add.s32 	%r473, %r473, %r340;
     	sqrt.approx.ftz.f32 	%f204, %f202;
     	setp.lt.ftz.f32 	%p75, %f204, %f47;
     	selp.u32 	%r341, 1, 0, %p75;
     	add.s32 	%r474, %r474, %r341;

     $L__BB1_102:
     	add.s32 	%r342, %r483, 2;
     	setp.eq.s32 	%p76, %r342, %r418;
     	@%p76 bra 	$L__BB1_104;

     	ld.global.nc.f32 	%f205, [%rd22+8];
     	sub.ftz.f32 	%f206, %f5, %f205;
     	fma.rn.ftz.f32 	%f207, %f206, %f206, 0f00000000;
     	add.ftz.f32 	%f208, %f207, 0f2EDBE6FF;
     	mov.f32 	%f209, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f210, %f208;
     	setp.lt.ftz.f32 	%p77, %f210, %f47;
     	selp.u32 	%r343, 1, 0, %p77;
     	add.s32 	%r473, %r473, %r343;
     	sqrt.approx.ftz.f32 	%f211, %f209;
     	setp.lt.ftz.f32 	%p78, %f211, %f47;
     	selp.u32 	%r344, 1, 0, %p78;
     	add.s32 	%r474, %r474, %r344;

     $L__BB1_104:
     	add.s32 	%r345, %r483, 3;
     	setp.eq.s32 	%p79, %r345, %r418;
     	@%p79 bra 	$L__BB1_106;

     	ld.global.nc.f32 	%f212, [%rd22+12];
     	sub.ftz.f32 	%f213, %f5, %f212;
     	fma.rn.ftz.f32 	%f214, %f213, %f213, 0f00000000;
     	add.ftz.f32 	%f215, %f214, 0f2EDBE6FF;
     	mov.f32 	%f216, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f217, %f215;
     	setp.lt.ftz.f32 	%p80, %f217, %f47;
     	selp.u32 	%r346, 1, 0, %p80;
     	add.s32 	%r473, %r473, %r346;
     	sqrt.approx.ftz.f32 	%f218, %f216;
     	setp.lt.ftz.f32 	%p81, %f218, %f47;
     	selp.u32 	%r347, 1, 0, %p81;
     	add.s32 	%r474, %r474, %r347;

     $L__BB1_106:
     	add.s32 	%r483, %r483, 4;
     	add.s32 	%r472, %r472, -4;
     	setp.ne.s32 	%p82, %r472, 0;
     	@%p82 bra 	$L__BB1_98;

     $L__BB1_107:
     	setp.eq.s32 	%p83, %r24, 0;
     	@%p83 bra 	$L__BB1_133;

     	setp.eq.s32 	%p84, %r483, %r418;
     	add.s32 	%r348, %r483, %r11;
     	add.s32 	%r349, %r348, %r193;
     	mul.wide.s32 	%rd99, %r349, 4;
     	add.s64 	%rd23, %rd1, %rd99;
     	@%p84 bra 	$L__BB1_110;

     	ld.global.nc.f32 	%f219, [%rd23];
     	sub.ftz.f32 	%f220, %f5, %f219;
     	fma.rn.ftz.f32 	%f221, %f220, %f220, 0f00000000;
     	add.ftz.f32 	%f222, %f221, 0f2EDBE6FF;
     	mov.f32 	%f223, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f224, %f222;
     	setp.lt.ftz.f32 	%p85, %f224, %f47;
     	selp.u32 	%r350, 1, 0, %p85;
     	add.s32 	%r473, %r473, %r350;
     	sqrt.approx.ftz.f32 	%f225, %f223;
     	setp.lt.ftz.f32 	%p86, %f225, %f47;
     	selp.u32 	%r351, 1, 0, %p86;
     	add.s32 	%r474, %r474, %r351;

     $L__BB1_110:
     	setp.eq.s32 	%p87, %r24, 1;
     	@%p87 bra 	$L__BB1_133;

     	add.s32 	%r352, %r483, 1;
     	setp.eq.s32 	%p88, %r352, %r418;
     	@%p88 bra 	$L__BB1_113;

     	ld.global.nc.f32 	%f226, [%rd23+4];
     	sub.ftz.f32 	%f227, %f5, %f226;
     	fma.rn.ftz.f32 	%f228, %f227, %f227, 0f00000000;
     	add.ftz.f32 	%f229, %f228, 0f2EDBE6FF;
     	mov.f32 	%f230, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f231, %f229;
     	setp.lt.ftz.f32 	%p89, %f231, %f47;
     	selp.u32 	%r353, 1, 0, %p89;
     	add.s32 	%r473, %r473, %r353;
     	sqrt.approx.ftz.f32 	%f232, %f230;
     	setp.lt.ftz.f32 	%p90, %f232, %f47;
     	selp.u32 	%r354, 1, 0, %p90;
     	add.s32 	%r474, %r474, %r354;

     $L__BB1_113:
     	add.s32 	%r355, %r483, 2;
     	setp.eq.s32 	%p91, %r355, %r418;
     	setp.eq.s32 	%p92, %r24, 2;
     	or.pred  	%p93, %p92, %p91;
     	@%p93 bra 	$L__BB1_133;

     	ld.global.nc.f32 	%f233, [%rd23+8];
     	sub.ftz.f32 	%f234, %f5, %f233;
     	fma.rn.ftz.f32 	%f235, %f234, %f234, 0f00000000;
     	add.ftz.f32 	%f236, %f235, 0f2EDBE6FF;
     	mov.f32 	%f237, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f238, %f236;
     	setp.lt.ftz.f32 	%p94, %f238, %f47;
     	selp.u32 	%r356, 1, 0, %p94;
     	add.s32 	%r473, %r473, %r356;
     	sqrt.approx.ftz.f32 	%f239, %f237;
     	setp.lt.ftz.f32 	%p95, %f239, %f47;
     	selp.u32 	%r357, 1, 0, %p95;
     	add.s32 	%r474, %r474, %r357;
     	bra.uni 	$L__BB1_133;

     $L__BB1_4:
     	mov.u32 	%r430, 0;
     	setp.lt.s32 	%p4, %r192, 1;
     	@%p4 bra 	$L__BB1_12;

     	setp.lt.u32 	%p5, %r14, 3;
     	mov.u32 	%r421, 0;
     	@%p5 bra 	$L__BB1_8;

     	mov.u32 	%r421, 0;
     	mov.u32 	%r420, %r19;

     $L__BB1_7:
     	add.s32 	%r392, %r418, %r12;
     	sub.s32 	%r213, %r392, %r421;
     	mul.wide.s32 	%rd37, %r213, 4;
     	add.s64 	%rd38, %rd1, %rd37;
     	mul.wide.s32 	%rd39, %r421, 4;
     	add.s64 	%rd40, %rd2, %rd39;
     	add.s32 	%r421, %r421, 4;
     	ld.global.nc.f32 	%f72, [%rd38+-12];
     	ld.global.nc.f32 	%f73, [%rd38+-8];
     	ld.global.nc.f32 	%f74, [%rd38+-4];
     	ld.global.nc.f32 	%f75, [%rd38];
     	st.local.v4.f32 	[%rd40], {%f75, %f74, %f73, %f72};
     	add.s32 	%r420, %r420, -4;
     	setp.ne.s32 	%p6, %r420, 0;
     	@%p6 bra 	$L__BB1_7;

     $L__BB1_8:
     	setp.eq.s32 	%p7, %r18, 0;
     	mov.u32 	%r430, %r421;
     	@%p7 bra 	$L__BB1_12;

     	add.s32 	%r393, %r418, %r12;
     	setp.eq.s32 	%p8, %r18, 1;
     	sub.s32 	%r214, %r393, %r421;
     	mul.wide.s32 	%rd41, %r214, 4;
     	add.s64 	%rd7, %rd1, %rd41;
     	ld.global.nc.f32 	%f76, [%rd7];
     	add.s32 	%r430, %r421, 1;
     	mul.wide.s32 	%rd42, %r421, 4;
     	add.s64 	%rd8, %rd2, %rd42;
     	st.local.f32 	[%rd8], %f76;
     	@%p8 bra 	$L__BB1_12;

     	mul.wide.s32 	%rd121, %r421, 4;
     	add.s64 	%rd120, %rd2, %rd121;
     	setp.eq.s32 	%p9, %r18, 2;
     	ld.global.nc.f32 	%f77, [%rd7+-4];
     	add.s32 	%r430, %r421, 2;
     	st.local.f32 	[%rd120+4], %f77;
     	@%p9 bra 	$L__BB1_12;

     	mul.wide.s32 	%rd123, %r421, 4;
     	add.s64 	%rd122, %rd2, %rd123;
     	ld.global.nc.f32 	%f78, [%rd7+-8];
     	add.s32 	%r430, %r421, 3;
     	st.local.f32 	[%rd122+8], %f78;

     $L__BB1_12:
     	add.s32 	%r41, %r418, %r11;
     	@%p4 bra 	$L__BB1_20;

     	setp.lt.u32 	%p11, %r14, 3;
     	mov.u32 	%r428, 0;
     	mov.u32 	%r427, %r430;
     	@%p11 bra 	$L__BB1_16;

     	mov.u32 	%r428, 0;
     	mov.u32 	%r427, %r430;
     	mov.u32 	%r426, %r19;

     $L__BB1_15:
     	sub.s32 	%r218, %r41, %r428;
     	mul.wide.s32 	%rd43, %r218, 4;
     	add.s64 	%rd44, %rd1, %rd43;
     	ld.global.nc.f32 	%f79, [%rd44];
     	mul.wide.s32 	%rd45, %r427, 4;
     	add.s64 	%rd46, %rd2, %rd45;
     	st.local.f32 	[%rd46], %f79;
     	not.b32 	%r219, %r428;
     	add.s32 	%r220, %r41, %r219;
     	mul.wide.s32 	%rd47, %r220, 4;
     	add.s64 	%rd48, %rd1, %rd47;
     	ld.global.nc.f32 	%f80, [%rd48];
     	st.local.f32 	[%rd46+4], %f80;
     	ld.global.nc.f32 	%f81, [%rd44+-8];
     	st.local.f32 	[%rd46+8], %f81;
     	ld.global.nc.f32 	%f82, [%rd44+-12];
     	add.s32 	%r427, %r427, 4;
     	st.local.f32 	[%rd46+12], %f82;
     	add.s32 	%r428, %r428, 4;
     	add.s32 	%r426, %r426, -4;
     	setp.ne.s32 	%p12, %r426, 0;
     	@%p12 bra 	$L__BB1_15;

     $L__BB1_16:
     	setp.eq.s32 	%p13, %r18, 0;
     	mov.u32 	%r430, %r427;
     	@%p13 bra 	$L__BB1_20;

     	setp.eq.s32 	%p14, %r18, 1;
     	sub.s32 	%r221, %r41, %r428;
     	mul.wide.s32 	%rd49, %r221, 4;
     	add.s64 	%rd50, %rd1, %rd49;
     	ld.global.nc.f32 	%f83, [%rd50];
     	add.s32 	%r430, %r427, 1;
     	mul.wide.s32 	%rd51, %r427, 4;
     	add.s64 	%rd9, %rd2, %rd51;
     	st.local.f32 	[%rd9], %f83;
     	@%p14 bra 	$L__BB1_20;

     	mul.wide.s32 	%rd125, %r427, 4;
     	add.s64 	%rd124, %rd2, %rd125;
     	setp.eq.s32 	%p15, %r18, 2;
     	not.b32 	%r222, %r428;
     	add.s32 	%r223, %r41, %r222;
     	mul.wide.s32 	%rd52, %r223, 4;
     	add.s64 	%rd53, %rd1, %rd52;
     	ld.global.nc.f32 	%f84, [%rd53];
     	add.s32 	%r430, %r427, 2;
     	st.local.f32 	[%rd124+4], %f84;
     	@%p15 bra 	$L__BB1_20;

     	mul.wide.s32 	%rd127, %r427, 4;
     	add.s64 	%rd126, %rd2, %rd127;
     	add.s32 	%r224, %r428, 2;
     	sub.s32 	%r225, %r41, %r224;
     	mul.wide.s32 	%rd54, %r225, 4;
     	add.s64 	%rd55, %rd1, %rd54;
     	ld.global.nc.f32 	%f85, [%rd55];
     	add.s32 	%r430, %r427, 3;
     	st.local.f32 	[%rd126+8], %f85;

     $L__BB1_20:
     	add.s32 	%r226, %r41, %r193;
     	mul.wide.s32 	%rd56, %r226, 4;
     	add.s64 	%rd57, %rd1, %rd56;
     	ld.global.nc.f32 	%f5, [%rd57];
     	mul.wide.s32 	%rd58, %r430, 4;
     	add.s64 	%rd59, %rd2, %rd58;
     	st.local.f32 	[%rd59], %f5;
     	setp.gt.s32 	%p16, %r192, 0;
     	@%p16 bra 	$L__BB1_30;
     	bra.uni 	$L__BB1_21;

     $L__BB1_30:
     	mov.u32 	%r435, %r192;

     $L__BB1_31:
     	setp.lt.u32 	%p22, %r14, 3;
     	mov.u32 	%r438, 0;
     	@%p22 bra 	$L__BB1_34;

     	mov.u32 	%r438, 0;
     	mov.u32 	%r437, %r19;

     $L__BB1_33:
     	add.s32 	%r394, %r435, %r12;
     	sub.s32 	%r240, %r394, %r438;
     	mul.wide.s32 	%rd65, %r240, 4;
     	add.s64 	%rd66, %rd1, %rd65;
     	mul.wide.s32 	%rd67, %r438, 4;
     	add.s64 	%rd68, %rd4, %rd67;
     	add.s32 	%r438, %r438, 4;
     	ld.global.nc.f32 	%f123, [%rd66+-12];
     	ld.global.nc.f32 	%f124, [%rd66+-8];
     	ld.global.nc.f32 	%f125, [%rd66+-4];
     	ld.global.nc.f32 	%f126, [%rd66];
     	st.local.v4.f32 	[%rd68], {%f126, %f125, %f124, %f123};
     	add.s32 	%r437, %r437, -4;
     	setp.ne.s32 	%p23, %r437, 0;
     	@%p23 bra 	$L__BB1_33;

     $L__BB1_34:
     	setp.eq.s32 	%p24, %r18, 0;
     	mov.u32 	%r444, %r438;
     	@%p24 bra 	$L__BB1_38;

     	add.s32 	%r395, %r435, %r12;
     	setp.eq.s32 	%p25, %r18, 1;
     	sub.s32 	%r241, %r395, %r438;
     	mul.wide.s32 	%rd69, %r241, 4;
     	add.s64 	%rd16, %rd1, %rd69;
     	ld.global.nc.f32 	%f127, [%rd16];
     	add.s32 	%r444, %r438, 1;
     	mul.wide.s32 	%rd70, %r438, 4;
     	add.s64 	%rd17, %rd4, %rd70;
     	st.local.f32 	[%rd17], %f127;
     	@%p25 bra 	$L__BB1_38;

     	mul.wide.s32 	%rd129, %r438, 4;
     	add.s64 	%rd128, %rd4, %rd129;
     	setp.eq.s32 	%p26, %r18, 2;
     	ld.global.nc.f32 	%f128, [%rd16+-4];
     	add.s32 	%r444, %r438, 2;
     	st.local.f32 	[%rd128+4], %f128;
     	@%p26 bra 	$L__BB1_38;

     	mul.wide.s32 	%rd131, %r438, 4;
     	add.s64 	%rd130, %rd4, %rd131;
     	ld.global.nc.f32 	%f129, [%rd16+-8];
     	add.s32 	%r444, %r438, 3;
     	st.local.f32 	[%rd130+8], %f129;

     $L__BB1_38:
     	setp.lt.u32 	%p116, %r14, 3;
     	mov.u32 	%r445, 0;
     	@%p116 bra 	$L__BB1_41;

     	mov.u32 	%r445, 0;
     	mov.u32 	%r443, %r19;

     $L__BB1_40:
     	add.s32 	%r396, %r435, %r11;
     	sub.s32 	%r245, %r396, %r445;
     	mul.wide.s32 	%rd71, %r245, 4;
     	add.s64 	%rd72, %rd1, %rd71;
     	ld.global.nc.f32 	%f130, [%rd72];
     	mul.wide.s32 	%rd73, %r444, 4;
     	add.s64 	%rd74, %rd4, %rd73;
     	st.local.f32 	[%rd74], %f130;
     	not.b32 	%r246, %r445;
     	add.s32 	%r247, %r396, %r246;
     	mul.wide.s32 	%rd75, %r247, 4;
     	add.s64 	%rd76, %rd1, %rd75;
     	ld.global.nc.f32 	%f131, [%rd76];
     	st.local.f32 	[%rd74+4], %f131;
     	ld.global.nc.f32 	%f132, [%rd72+-8];
     	st.local.f32 	[%rd74+8], %f132;
     	ld.global.nc.f32 	%f133, [%rd72+-12];
     	add.s32 	%r444, %r444, 4;
     	st.local.f32 	[%rd74+12], %f133;
     	add.s32 	%r445, %r445, 4;
     	add.s32 	%r443, %r443, -4;
     	setp.ne.s32 	%p28, %r443, 0;
     	@%p28 bra 	$L__BB1_40;

     $L__BB1_41:
     	setp.eq.s32 	%p117, %r18, 0;
     	mov.u32 	%r447, %r444;
     	@%p117 bra 	$L__BB1_45;

     	add.s32 	%r397, %r435, %r11;
     	setp.eq.s32 	%p30, %r18, 1;
     	sub.s32 	%r248, %r397, %r445;
     	mul.wide.s32 	%rd77, %r248, 4;
     	add.s64 	%rd78, %rd1, %rd77;
     	ld.global.nc.f32 	%f134, [%rd78];
     	add.s32 	%r447, %r444, 1;
     	mul.wide.s32 	%rd79, %r444, 4;
     	add.s64 	%rd18, %rd4, %rd79;
     	st.local.f32 	[%rd18], %f134;
     	@%p30 bra 	$L__BB1_45;

     	mul.wide.s32 	%rd133, %r444, 4;
     	add.s64 	%rd132, %rd4, %rd133;
     	add.s32 	%r398, %r435, %r11;
     	setp.eq.s32 	%p31, %r18, 2;
     	not.b32 	%r249, %r445;
     	add.s32 	%r250, %r398, %r249;
     	mul.wide.s32 	%rd80, %r250, 4;
     	add.s64 	%rd81, %rd1, %rd80;
     	ld.global.nc.f32 	%f135, [%rd81];
     	add.s32 	%r447, %r444, 2;
     	st.local.f32 	[%rd132+4], %f135;
     	@%p31 bra 	$L__BB1_45;

     	mul.wide.s32 	%rd135, %r444, 4;
     	add.s64 	%rd134, %rd4, %rd135;
     	add.s32 	%r399, %r435, %r11;
     	add.s32 	%r251, %r445, 2;
     	sub.s32 	%r252, %r399, %r251;
     	mul.wide.s32 	%rd82, %r252, 4;
     	add.s64 	%rd83, %rd1, %rd82;
     	ld.global.nc.f32 	%f136, [%rd83];
     	add.s32 	%r447, %r444, 3;
     	st.local.f32 	[%rd134+8], %f136;

     $L__BB1_45:
     	add.s32 	%r400, %r435, %r11;
     	add.s32 	%r253, %r400, %r193;
     	mul.wide.s32 	%rd84, %r253, 4;
     	add.s64 	%rd85, %rd1, %rd84;
     	ld.global.nc.f32 	%f138, [%rd85];
     	mul.wide.s32 	%rd86, %r447, 4;
     	add.s64 	%rd87, %rd4, %rd86;
     	st.local.f32 	[%rd87], %f138;
     	setp.lt.s32 	%p32, %r192, 0;
     	mov.f32 	%f326, 0f00000000;
     	@%p32 bra 	$L__BB1_51;

     	setp.lt.u32 	%p33, %r15, 3;
     	mov.f32 	%f325, 0f00000000;
     	mov.u32 	%r450, 0;
     	@%p33 bra 	$L__BB1_49;

     	mov.f32 	%f325, 0f00000000;
     	mov.u32 	%r450, 0;
     	mov.u32 	%r449, %r21;

     $L__BB1_48:
     	mul.wide.s32 	%rd88, %r450, 4;
     	add.s64 	%rd89, %rd2, %rd88;
     	ld.local.v4.f32 	{%f141, %f142, %f143, %f144}, [%rd89];
     	add.s64 	%rd90, %rd4, %rd88;
     	ld.local.v4.f32 	{%f149, %f150, %f151, %f152}, [%rd90];
     	sub.ftz.f32 	%f157, %f141, %f149;
     	fma.rn.ftz.f32 	%f158, %f157, %f157, %f325;
     	sub.ftz.f32 	%f159, %f142, %f150;
     	fma.rn.ftz.f32 	%f160, %f159, %f159, %f158;
     	sub.ftz.f32 	%f161, %f143, %f151;
     	fma.rn.ftz.f32 	%f162, %f161, %f161, %f160;
     	sub.ftz.f32 	%f163, %f144, %f152;
     	fma.rn.ftz.f32 	%f325, %f163, %f163, %f162;
     	add.s32 	%r450, %r450, 4;
     	add.s32 	%r449, %r449, -4;
     	setp.ne.s32 	%p34, %r449, 0;
     	@%p34 bra 	$L__BB1_48;

     $L__BB1_49:
     	mul.wide.s32 	%rd91, %r450, 4;
     	add.s64 	%rd19, %rd2, %rd91;
     	add.s64 	%rd20, %rd4, %rd91;
     	ld.local.f32 	%f164, [%rd20];
     	ld.local.f32 	%f165, [%rd19];
     	sub.ftz.f32 	%f166, %f165, %f164;
     	fma.rn.ftz.f32 	%f326, %f166, %f166, %f325;
     	setp.eq.s32 	%p35, %r20, 1;
     	@%p35 bra 	$L__BB1_51;

     	ld.local.f32 	%f167, [%rd20+4];
     	ld.local.f32 	%f168, [%rd19+4];
     	sub.ftz.f32 	%f169, %f168, %f167;
     	fma.rn.ftz.f32 	%f170, %f169, %f169, %f326;
     	ld.local.f32 	%f171, [%rd20+8];
     	ld.local.f32 	%f172, [%rd19+8];
     	sub.ftz.f32 	%f173, %f172, %f171;
     	fma.rn.ftz.f32 	%f326, %f173, %f173, %f170;

     $L__BB1_51:
     	add.ftz.f32 	%f174, %f326, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f175, %f174;
     	add.s32 	%r256, %r435, %r9;
     	shl.b32 	%r257, %r256, 2;
     	mov.u32 	%r258, shared_mem;
     	add.s32 	%r259, %r258, %r257;
     	st.shared.f32 	[%r259], %f175;
     	add.s32 	%r435, %r435, 1;
     	setp.lt.s32 	%p36, %r435, %r10;
     	@%p36 bra 	$L__BB1_31;
     	bra.uni 	$L__BB1_52;

     $L__BB1_21:
     	mov.u32 	%r431, %r192;

     $L__BB1_22:
     	add.s32 	%r227, %r431, %r11;
     	add.s32 	%r228, %r227, %r193;
     	mul.wide.s32 	%rd60, %r228, 4;
     	add.s64 	%rd61, %rd1, %rd60;
     	ld.global.nc.f32 	%f321, [%rd61];
     	st.local.f32 	[%rd4], %f321;
     	setp.lt.s32 	%p17, %r192, 0;
     	mov.f32 	%f323, 0f00000000;
     	@%p17 bra 	$L__BB1_29;

     	setp.lt.u32 	%p18, %r15, 3;
     	mov.f32 	%f322, 0f00000000;
     	mov.u32 	%r434, 0;
     	@%p18 bra 	$L__BB1_27;

     	mov.f32 	%f322, 0f00000000;
     	mov.u32 	%r434, 0;
     	mov.u64 	%rd136, %rd2;
     	mov.u64 	%rd137, %rd4;
     	mov.u32 	%r433, %r21;

     $L__BB1_25:
     	mov.u64 	%rd11, %rd137;
     	ld.local.v4.f32 	{%f89, %f90, %f91, %f92}, [%rd136];
     	ld.local.v4.f32 	{%f97, %f98, %f99, %f100}, [%rd11];
     	sub.ftz.f32 	%f105, %f89, %f97;
     	fma.rn.ftz.f32 	%f106, %f105, %f105, %f322;
     	sub.ftz.f32 	%f107, %f90, %f98;
     	fma.rn.ftz.f32 	%f108, %f107, %f107, %f106;
     	sub.ftz.f32 	%f109, %f91, %f99;
     	fma.rn.ftz.f32 	%f110, %f109, %f109, %f108;
     	sub.ftz.f32 	%f111, %f92, %f100;
     	fma.rn.ftz.f32 	%f322, %f111, %f111, %f110;
     	add.s32 	%r434, %r434, 4;
     	add.s64 	%rd136, %rd136, 16;
     	add.s32 	%r433, %r433, -4;
     	setp.ne.s32 	%p19, %r433, 0;
     	add.s64 	%rd137, %rd11, 16;
     	@%p19 bra 	$L__BB1_25;

     	ld.local.f32 	%f321, [%rd11+16];

     $L__BB1_27:
     	mul.wide.s32 	%rd62, %r434, 4;
     	add.s64 	%rd15, %rd2, %rd62;
     	ld.local.f32 	%f112, [%rd15];
     	sub.ftz.f32 	%f113, %f112, %f321;
     	fma.rn.ftz.f32 	%f323, %f113, %f113, %f322;
     	setp.eq.s32 	%p20, %r20, 1;
     	@%p20 bra 	$L__BB1_29;

     	add.s32 	%r231, %r434, 1;
     	mul.wide.s32 	%rd63, %r231, 4;
     	add.s64 	%rd64, %rd4, %rd63;
     	ld.local.f32 	%f114, [%rd64];
     	ld.local.f32 	%f115, [%rd15+4];
     	sub.ftz.f32 	%f116, %f115, %f114;
     	fma.rn.ftz.f32 	%f117, %f116, %f116, %f323;
     	ld.local.f32 	%f118, [%rd64+4];
     	ld.local.f32 	%f119, [%rd15+8];
     	sub.ftz.f32 	%f120, %f119, %f118;
     	fma.rn.ftz.f32 	%f323, %f120, %f120, %f117;

     $L__BB1_29:
     	add.ftz.f32 	%f121, %f323, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f122, %f121;
     	add.s32 	%r232, %r431, %r9;
     	shl.b32 	%r233, %r232, 2;
     	mov.u32 	%r234, shared_mem;
     	add.s32 	%r235, %r234, %r233;
     	st.shared.f32 	[%r235], %f122;
     	add.s32 	%r431, %r431, 1;
     	setp.lt.s32 	%p21, %r431, %r10;
     	@%p21 bra 	$L__BB1_22;

     $L__BB1_52:
     	setp.lt.s32 	%p37, %r13, 1;
     	mov.u32 	%r463, -1;
     	st.local.v4.u32 	[%rd3], {%r463, %r463, %r463, %r463};
     	@%p37 bra 	$L__BB1_87;

     	setp.eq.s32 	%p38, %r16, %r193;
     	mov.f32 	%f335, 0f7F7FFFFF;
     	mov.u32 	%r466, 0;
     	mov.u32 	%r463, -1;
     	mov.u32 	%r464, %r466;
     	@%p38 bra 	$L__BB1_84;

     	mov.f32 	%f335, 0f7F7FFFFF;
     	mov.u32 	%r466, 0;
     	mov.u32 	%r463, -1;
     	mov.u32 	%r454, %r23;

     $L__BB1_55:
     	setp.eq.s32 	%p39, %r466, %r418;
     	@%p39 bra 	$L__BB1_69;

     	add.s32 	%r404, %r466, %r9;
     	shl.b32 	%r403, %r404, 2;
     	mov.u32 	%r402, shared_mem;
     	add.s32 	%r401, %r402, %r403;
     	ld.shared.f32 	%f178, [%r401];
     	setp.geu.ftz.f32 	%p40, %f178, %f335;
     	@%p40 bra 	$L__BB1_69;

     	mov.f32 	%f335, 0f00000000;
     	mul.wide.s32 	%rd92, %r464, 4;
     	add.s64 	%rd93, %rd3, %rd92;
     	st.local.u32 	[%rd93], %r466;
     	ld.local.u32 	%r463, [%rd3];
     	setp.lt.s32 	%p41, %r463, 0;
     	@%p41 bra 	$L__BB1_60;

     	mov.f32 	%f335, 0f00000000;
     	mov.u32 	%r408, shared_mem;
     	add.s32 	%r270, %r463, %r9;
     	shl.b32 	%r271, %r270, 2;
     	add.s32 	%r273, %r408, %r271;
     	ld.shared.f32 	%f22, [%r273];
     	setp.leu.ftz.f32 	%p42, %f22, 0f00000000;
     	@%p42 bra 	$L__BB1_60;

     	mov.u32 	%r464, 0;
     	mov.f32 	%f335, %f22;

     $L__BB1_60:
     	ld.local.u32 	%r102, [%rd3+4];
     	setp.lt.s32 	%p43, %r102, 0;
     	@%p43 bra 	$L__BB1_63;

     	mov.u32 	%r407, shared_mem;
     	add.s32 	%r275, %r102, %r9;
     	shl.b32 	%r276, %r275, 2;
     	add.s32 	%r278, %r407, %r276;
     	ld.shared.f32 	%f24, [%r278];
     	setp.leu.ftz.f32 	%p44, %f24, %f335;
     	@%p44 bra 	$L__BB1_63;

     	mov.u32 	%r464, 1;
     	mov.f32 	%f335, %f24;

     $L__BB1_63:
     	ld.local.u32 	%r104, [%rd3+8];
     	setp.lt.s32 	%p45, %r104, 0;
     	@%p45 bra 	$L__BB1_66;

     	mov.u32 	%r406, shared_mem;
     	add.s32 	%r280, %r104, %r9;
     	shl.b32 	%r281, %r280, 2;
     	add.s32 	%r283, %r406, %r281;
     	ld.shared.f32 	%f26, [%r283];
     	setp.leu.ftz.f32 	%p46, %f26, %f335;
     	@%p46 bra 	$L__BB1_66;

     	mov.u32 	%r464, 2;
     	mov.f32 	%f335, %f26;

     $L__BB1_66:
     	ld.local.u32 	%r106, [%rd3+12];
     	setp.lt.s32 	%p47, %r106, 0;
     	@%p47 bra 	$L__BB1_69;

     	mov.u32 	%r405, shared_mem;
     	add.s32 	%r285, %r106, %r9;
     	shl.b32 	%r286, %r285, 2;
     	add.s32 	%r288, %r405, %r286;
     	ld.shared.f32 	%f28, [%r288];
     	setp.leu.ftz.f32 	%p48, %f28, %f335;
     	@%p48 bra 	$L__BB1_69;

     	mov.u32 	%r464, 3;
     	mov.f32 	%f335, %f28;

     $L__BB1_69:
     	add.s32 	%r109, %r466, 1;
     	setp.eq.s32 	%p49, %r109, %r418;
     	@%p49 bra 	$L__BB1_83;

     	add.s32 	%r412, %r466, %r9;
     	shl.b32 	%r411, %r412, 2;
     	mov.u32 	%r410, shared_mem;
     	add.s32 	%r409, %r410, %r411;
     	ld.shared.f32 	%f181, [%r409+4];
     	setp.geu.ftz.f32 	%p50, %f181, %f335;
     	@%p50 bra 	$L__BB1_83;

     	mov.f32 	%f335, 0f00000000;
     	mul.wide.s32 	%rd94, %r464, 4;
     	add.s64 	%rd95, %rd3, %rd94;
     	add.s32 	%r384, %r466, 1;
     	st.local.u32 	[%rd95], %r384;
     	ld.local.u32 	%r463, [%rd3];
     	setp.lt.s32 	%p51, %r463, 0;
     	@%p51 bra 	$L__BB1_74;

     	mov.f32 	%f335, 0f00000000;
     	mov.u32 	%r416, shared_mem;
     	add.s32 	%r290, %r463, %r9;
     	shl.b32 	%r291, %r290, 2;
     	add.s32 	%r293, %r416, %r291;
     	ld.shared.f32 	%f30, [%r293];
     	setp.leu.ftz.f32 	%p52, %f30, 0f00000000;
     	@%p52 bra 	$L__BB1_74;

     	mov.u32 	%r464, 0;
     	mov.f32 	%f335, %f30;

     $L__BB1_74:
     	ld.local.u32 	%r112, [%rd3+4];
     	setp.lt.s32 	%p53, %r112, 0;
     	@%p53 bra 	$L__BB1_77;

     	mov.u32 	%r415, shared_mem;
     	add.s32 	%r295, %r112, %r9;
     	shl.b32 	%r296, %r295, 2;
     	add.s32 	%r298, %r415, %r296;
     	ld.shared.f32 	%f32, [%r298];
     	setp.leu.ftz.f32 	%p54, %f32, %f335;
     	@%p54 bra 	$L__BB1_77;

     	mov.u32 	%r464, 1;
     	mov.f32 	%f335, %f32;

     $L__BB1_77:
     	ld.local.u32 	%r114, [%rd3+8];
     	setp.lt.s32 	%p55, %r114, 0;
     	@%p55 bra 	$L__BB1_80;

     	mov.u32 	%r414, shared_mem;
     	add.s32 	%r300, %r114, %r9;
     	shl.b32 	%r301, %r300, 2;
     	add.s32 	%r303, %r414, %r301;
     	ld.shared.f32 	%f34, [%r303];
     	setp.leu.ftz.f32 	%p56, %f34, %f335;
     	@%p56 bra 	$L__BB1_80;

     	mov.u32 	%r464, 2;
     	mov.f32 	%f335, %f34;

     $L__BB1_80:
     	ld.local.u32 	%r116, [%rd3+12];
     	setp.lt.s32 	%p57, %r116, 0;
     	@%p57 bra 	$L__BB1_83;

     	mov.u32 	%r413, shared_mem;
     	add.s32 	%r305, %r116, %r9;
     	shl.b32 	%r306, %r305, 2;
     	add.s32 	%r308, %r413, %r306;
     	ld.shared.f32 	%f36, [%r308];
     	setp.leu.ftz.f32 	%p58, %f36, %f335;
     	@%p58 bra 	$L__BB1_83;

     	mov.u32 	%r464, 3;
     	mov.f32 	%f335, %f36;

     $L__BB1_83:
     	add.s32 	%r466, %r466, 2;
     	add.s32 	%r454, %r454, -2;
     	setp.ne.s32 	%p59, %r454, 0;
     	@%p59 bra 	$L__BB1_55;

     $L__BB1_84:
     	setp.eq.s32 	%p60, %r466, %r418;
     	setp.eq.s32 	%p61, %r22, 0;
     	or.pred  	%p62, %p61, %p60;
     	@%p62 bra 	$L__BB1_87;

     	add.s32 	%r310, %r466, %r9;
     	shl.b32 	%r311, %r310, 2;
     	mov.u32 	%r312, shared_mem;
     	add.s32 	%r313, %r312, %r311;
     	ld.shared.f32 	%f184, [%r313];
     	setp.geu.ftz.f32 	%p63, %f184, %f335;
     	@%p63 bra 	$L__BB1_87;

     	mul.wide.s32 	%rd96, %r464, 4;
     	add.s64 	%rd97, %rd3, %rd96;
     	st.local.u32 	[%rd97], %r466;
     	ld.local.u32 	%r463, [%rd3];

     $L__BB1_87:
     	setp.lt.s32 	%p64, %r463, 0;
     	mov.f32 	%f338, 0f00000000;
     	@%p64 bra 	$L__BB1_89;

     	add.s32 	%r314, %r463, %r9;
     	shl.b32 	%r315, %r314, 2;
     	mov.u32 	%r316, shared_mem;
     	add.s32 	%r317, %r316, %r315;
     	ld.shared.f32 	%f186, [%r317];
     	mov.f32 	%f187, 0f00000000;
     	max.ftz.f32 	%f338, %f187, %f186;

     $L__BB1_89:
     	ld.local.u32 	%r126, [%rd3+4];
     	setp.lt.s32 	%p65, %r126, 0;
     	@%p65 bra 	$L__BB1_91;

     	add.s32 	%r318, %r126, %r9;
     	shl.b32 	%r319, %r318, 2;
     	mov.u32 	%r320, shared_mem;
     	add.s32 	%r321, %r320, %r319;
     	ld.shared.f32 	%f188, [%r321];
     	max.ftz.f32 	%f338, %f338, %f188;

     $L__BB1_91:
     	ld.local.u32 	%r127, [%rd3+8];
     	setp.lt.s32 	%p66, %r127, 0;
     	@%p66 bra 	$L__BB1_93;

     	add.s32 	%r322, %r127, %r9;
     	shl.b32 	%r323, %r322, 2;
     	mov.u32 	%r324, shared_mem;
     	add.s32 	%r325, %r324, %r323;
     	ld.shared.f32 	%f189, [%r325];
     	max.ftz.f32 	%f338, %f338, %f189;

     $L__BB1_93:
     	ld.local.u32 	%r128, [%rd3+12];
     	setp.lt.s32 	%p67, %r128, 0;
     	@%p67 bra 	$L__BB1_95;

     	add.s32 	%r326, %r128, %r9;
     	shl.b32 	%r327, %r326, 2;
     	mov.u32 	%r328, shared_mem;
     	add.s32 	%r329, %r328, %r327;
     	ld.shared.f32 	%f190, [%r329];
     	max.ftz.f32 	%f338, %f338, %f190;

     $L__BB1_95:
     	setp.gt.s32 	%p115, %r192, 0;
     	ld.param.f32 	%f314, [transfer_entropy_ksg_kernel_param_4+16];
     	add.ftz.f32 	%f47, %f314, %f338;
     	@%p115 bra 	$L__BB1_115;
     	bra.uni 	$L__BB1_96;

     $L__BB1_115:
     	mov.u32 	%r474, 0;
     	mov.u32 	%r490, %r192;
     	mov.u32 	%r473, %r474;

     $L__BB1_116:
     	setp.eq.s32 	%p96, %r490, %r418;
     	@%p96 bra 	$L__BB1_132;

     	setp.lt.u32 	%p97, %r14, 3;
     	mov.f32 	%f344, 0f00000000;
     	mov.u32 	%r495, 0;
     	@%p97 bra 	$L__BB1_120;

     	mov.f32 	%f344, 0f00000000;
     	mov.u32 	%r495, 0;
     	mov.u32 	%r494, %r19;

     $L__BB1_119:
     	add.s32 	%r390, %r490, %r11;
     	sub.s32 	%r362, %r41, %r495;
     	mul.wide.s32 	%rd100, %r362, 4;
     	add.s64 	%rd101, %rd1, %rd100;
     	sub.s32 	%r363, %r390, %r495;
     	mul.wide.s32 	%rd102, %r363, 4;
     	add.s64 	%rd103, %rd1, %rd102;
     	ld.global.nc.f32 	%f243, [%rd103];
     	ld.global.nc.f32 	%f244, [%rd101];
     	sub.ftz.f32 	%f245, %f244, %f243;
     	fma.rn.ftz.f32 	%f246, %f245, %f245, %f344;
     	ld.global.nc.f32 	%f247, [%rd103+-4];
     	ld.global.nc.f32 	%f248, [%rd101+-4];
     	sub.ftz.f32 	%f249, %f248, %f247;
     	fma.rn.ftz.f32 	%f250, %f249, %f249, %f246;
     	ld.global.nc.f32 	%f251, [%rd103+-8];
     	ld.global.nc.f32 	%f252, [%rd101+-8];
     	sub.ftz.f32 	%f253, %f252, %f251;
     	fma.rn.ftz.f32 	%f254, %f253, %f253, %f250;
     	ld.global.nc.f32 	%f255, [%rd103+-12];
     	ld.global.nc.f32 	%f256, [%rd101+-12];
     	sub.ftz.f32 	%f257, %f256, %f255;
     	fma.rn.ftz.f32 	%f344, %f257, %f257, %f254;
     	add.s32 	%r495, %r495, 4;
     	add.s32 	%r494, %r494, -4;
     	setp.ne.s32 	%p98, %r494, 0;
     	@%p98 bra 	$L__BB1_119;

     $L__BB1_120:
     	setp.eq.s32 	%p99, %r18, 0;
     	@%p99 bra 	$L__BB1_124;

     	add.s32 	%r391, %r490, %r11;
     	setp.eq.s32 	%p100, %r18, 1;
     	sub.s32 	%r364, %r41, %r495;
     	mul.wide.s32 	%rd104, %r364, 4;
     	add.s64 	%rd105, %rd1, %rd104;
     	sub.s32 	%r365, %r391, %r495;
     	mul.wide.s32 	%rd106, %r365, 4;
     	add.s64 	%rd24, %rd1, %rd106;
     	ld.global.nc.f32 	%f258, [%rd24];
     	ld.global.nc.f32 	%f259, [%rd105];
     	sub.ftz.f32 	%f260, %f259, %f258;
     	fma.rn.ftz.f32 	%f344, %f260, %f260, %f344;
     	@%p100 bra 	$L__BB1_124;

     	setp.eq.s32 	%p101, %r18, 2;
     	add.s32 	%r366, %r495, 1;
     	sub.s32 	%r367, %r41, %r366;
     	mul.wide.s32 	%rd107, %r367, 4;
     	add.s64 	%rd25, %rd1, %rd107;
     	ld.global.nc.f32 	%f261, [%rd24+-4];
     	ld.global.nc.f32 	%f262, [%rd25];
     	sub.ftz.f32 	%f263, %f262, %f261;
     	fma.rn.ftz.f32 	%f344, %f263, %f263, %f344;
     	@%p101 bra 	$L__BB1_124;

     	ld.global.nc.f32 	%f264, [%rd24+-8];
     	ld.global.nc.f32 	%f265, [%rd25+-4];
     	sub.ftz.f32 	%f266, %f265, %f264;
     	fma.rn.ftz.f32 	%f344, %f266, %f266, %f344;

     $L__BB1_124:
     	add.s32 	%r385, %r490, %r11;
     	setp.lt.u32 	%p113, %r14, 3;
     	add.s32 	%r369, %r385, %r193;
     	mul.wide.s32 	%rd108, %r369, 4;
     	add.s64 	%rd109, %rd1, %rd108;
     	ld.global.nc.f32 	%f269, [%rd109];
     	sub.ftz.f32 	%f270, %f5, %f269;
     	fma.rn.ftz.f32 	%f56, %f270, %f270, %f344;
     	mov.f32 	%f348, 0f00000000;
     	mov.u32 	%r498, 0;
     	@%p113 bra 	$L__BB1_127;

     	mov.f32 	%f348, 0f00000000;
     	mov.u32 	%r498, 0;
     	mov.u32 	%r497, %r19;

     $L__BB1_126:
     	add.s32 	%r386, %r490, %r11;
     	sub.s32 	%r371, %r41, %r498;
     	mul.wide.s32 	%rd110, %r371, 4;
     	add.s64 	%rd111, %rd1, %rd110;
     	sub.s32 	%r372, %r386, %r498;
     	mul.wide.s32 	%rd112, %r372, 4;
     	add.s64 	%rd113, %rd1, %rd112;
     	ld.global.nc.f32 	%f272, [%rd113];
     	ld.global.nc.f32 	%f273, [%rd111];
     	sub.ftz.f32 	%f274, %f273, %f272;
     	fma.rn.ftz.f32 	%f275, %f274, %f274, %f348;
     	ld.global.nc.f32 	%f276, [%rd113+-4];
     	ld.global.nc.f32 	%f277, [%rd111+-4];
     	sub.ftz.f32 	%f278, %f277, %f276;
     	fma.rn.ftz.f32 	%f279, %f278, %f278, %f275;
     	ld.global.nc.f32 	%f280, [%rd113+-8];
     	ld.global.nc.f32 	%f281, [%rd111+-8];
     	sub.ftz.f32 	%f282, %f281, %f280;
     	fma.rn.ftz.f32 	%f283, %f282, %f282, %f279;
     	ld.global.nc.f32 	%f284, [%rd113+-12];
     	ld.global.nc.f32 	%f285, [%rd111+-12];
     	sub.ftz.f32 	%f286, %f285, %f284;
     	fma.rn.ftz.f32 	%f348, %f286, %f286, %f283;
     	add.s32 	%r498, %r498, 4;
     	add.s32 	%r497, %r497, -4;
     	setp.ne.s32 	%p103, %r497, 0;
     	@%p103 bra 	$L__BB1_126;

     $L__BB1_127:
     	setp.eq.s32 	%p114, %r18, 0;
     	@%p114 bra 	$L__BB1_131;

     	add.s32 	%r387, %r490, %r11;
     	setp.eq.s32 	%p105, %r18, 1;
     	sub.s32 	%r373, %r41, %r498;
     	mul.wide.s32 	%rd114, %r373, 4;
     	add.s64 	%rd115, %rd1, %rd114;
     	sub.s32 	%r374, %r387, %r498;
     	mul.wide.s32 	%rd116, %r374, 4;
     	add.s64 	%rd117, %rd1, %rd116;
     	ld.global.nc.f32 	%f287, [%rd117];
     	ld.global.nc.f32 	%f288, [%rd115];
     	sub.ftz.f32 	%f289, %f288, %f287;
     	fma.rn.ftz.f32 	%f348, %f289, %f289, %f348;
     	@%p105 bra 	$L__BB1_131;

     	add.s32 	%r388, %r490, %r11;
     	add.s32 	%r375, %r498, 1;
     	setp.eq.s32 	%p106, %r18, 2;
     	sub.s32 	%r376, %r41, %r375;
     	mul.wide.s32 	%rd118, %r376, 4;
     	add.s64 	%rd26, %rd1, %rd118;
     	sub.s32 	%r377, %r388, %r375;
     	mul.wide.s32 	%rd119, %r377, 4;
     	add.s64 	%rd27, %rd1, %rd119;
     	ld.global.nc.f32 	%f290, [%rd27];
     	ld.global.nc.f32 	%f291, [%rd26];
     	sub.ftz.f32 	%f292, %f291, %f290;
     	fma.rn.ftz.f32 	%f348, %f292, %f292, %f348;
     	@%p106 bra 	$L__BB1_131;

     	ld.global.nc.f32 	%f293, [%rd27+-4];
     	ld.global.nc.f32 	%f294, [%rd26+-4];
     	sub.ftz.f32 	%f295, %f294, %f293;
     	fma.rn.ftz.f32 	%f348, %f295, %f295, %f348;

     $L__BB1_131:
     	add.ftz.f32 	%f296, %f348, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f297, %f296;
     	setp.lt.ftz.f32 	%p107, %f297, %f47;
     	selp.u32 	%r378, 1, 0, %p107;
     	add.s32 	%r474, %r474, %r378;
     	add.ftz.f32 	%f298, %f56, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f299, %f298;
     	setp.lt.ftz.f32 	%p108, %f299, %f47;
     	selp.u32 	%r379, 1, 0, %p108;
     	add.s32 	%r473, %r473, %r379;

     $L__BB1_132:
     	add.s32 	%r490, %r490, 1;
     	setp.lt.s32 	%p109, %r490, %r10;
     	@%p109 bra 	$L__BB1_116;

     $L__BB1_133:
     	add.s32 	%r380, %r474, 1;
     	cvt.rn.f32.s32 	%f300, %r380;
     	{ // callseq 3, 0
     	.reg .b32 temp_param_reg;
     	.param .b32 param0;
     	st.param.f32 	[param0+0], %f300;
     	.param .b32 retval0;
     	call.uni (retval0), 
     	_Z7digammaf, 
     	(
     	param0
     	);
     	ld.param.f32 	%f301, [retval0+0];
     	} // callseq 3
     	add.ftz.f32 	%f302, %f1, %f301;
     	add.s32 	%r381, %r473, 1;
     	cvt.rn.f32.s32 	%f303, %r381;
     	{ // callseq 4, 0
     	.reg .b32 temp_param_reg;
     	.param .b32 param0;
     	st.param.f32 	[param0+0], %f303;
     	.param .b32 retval0;
     	call.uni (retval0), 
     	_Z7digammaf, 
     	(
     	param0
     	);
     	ld.param.f32 	%f304, [retval0+0];
     	} // callseq 4
     	sub.ftz.f32 	%f305, %f302, %f304;
     	sub.ftz.f32 	%f306, %f305, %f2;
     	add.ftz.f32 	%f349, %f349, %f306;
     	add.s32 	%r503, %r503, 1;
     	add.s32 	%r418, %r418, 1;
     	setp.lt.s32 	%p110, %r418, %r10;
     	@%p110 bra 	$L__BB1_4;

     $L__BB1_134:
     	setp.eq.s32 	%p111, %r503, 0;
     	@%p111 bra 	$L__BB1_136;

     	cvt.rn.f32.s32 	%f308, %r503;
     	div.approx.ftz.f32 	%f350, %f349, %f308;

     $L__BB1_136:
     	mov.f32 	%f309, 0f00000000;
     	max.ftz.f32 	%f310, %f309, %f350;
     	st.global.f32 	[%rd5], %f310;
     	setp.gt.ftz.f32 	%p112, %f310, 0f3C23D70A;
     	selp.f32 	%f311, 0f3D4CCCCD, 0f3F800000, %p112;
     	st.global.f32 	[%rd6], %f311;

     $L__BB1_138:
     	ret;

     }
     	// .globl	conditional_te_kernel
     .visible .entry conditional_te_kernel(
     	.param .u64 conditional_te_kernel_param_0,
     	.param .u64 conditional_te_kernel_param_1,
     	.param .u64 conditional_te_kernel_param_2,
     	.param .align 4 .b8 conditional_te_kernel_param_3[24],
     	.param .u32 conditional_te_kernel_param_4
     )
     {
     	.reg .pred 	%p<3>;
     	.reg .f32 	%f<6>;
     	.reg .b32 	%r<12>;
     	.reg .b64 	%rd<5>;


     	ld.param.u64 	%rd2, [conditional_te_kernel_param_2];
     	ld.param.u32 	%r3, [conditional_te_kernel_param_4];
     	ld.param.u32 	%r2, [conditional_te_kernel_param_3];
     	mov.u32 	%r4, %ntid.x;
     	mov.u32 	%r5, %ctaid.x;
     	mov.u32 	%r6, %tid.x;
     	mad.lo.s32 	%r1, %r5, %r4, %r6;
     	mul.lo.s32 	%r7, %r2, %r2;
     	setp.ge.s32 	%p1, %r1, %r7;
     	@%p1 bra 	$L__BB2_4;

     	div.s32 	%r8, %r1, %r2;
     	mul.lo.s32 	%r9, %r8, %r2;
     	sub.s32 	%r10, %r1, %r9;
     	setp.eq.s32 	%p2, %r8, %r10;
     	cvta.to.global.u64 	%rd3, %rd2;
     	mul.wide.s32 	%rd4, %r1, 4;
     	add.s64 	%rd1, %rd3, %rd4;
     	@%p2 bra 	$L__BB2_3;
     	bra.uni 	$L__BB2_2;

     $L__BB2_3:
     	mov.u32 	%r11, 0;
     	st.global.u32 	[%rd1], %r11;
     	bra.uni 	$L__BB2_4;

     $L__BB2_2:
     	cvt.rn.f32.s32 	%f1, %r3;
     	mul.ftz.f32 	%f2, %f1, 0fBDCCCCCD;
     	mul.ftz.f32 	%f3, %f2, 0f3FB8AA3B;
     	ex2.approx.ftz.f32 	%f4, %f3;
     	mul.ftz.f32 	%f5, %f4, 0f3DCCCCCD;
     	st.global.f32 	[%rd1], %f5;

     $L__BB2_4:
     	ret;

     }
     	// .globl	multivariate_te_kernel
     .visible .entry multivariate_te_kernel(
     	.param .u64 multivariate_te_kernel_param_0,
     	.param .u64 multivariate_te_kernel_param_1,
     	.param .u32 multivariate_te_kernel_param_2,
     	.param .u32 multivariate_te_kernel_param_3,
     	.param .u64 multivariate_te_kernel_param_4,
     	.param .align 4 .b8 multivariate_te_kernel_param_5[24]
     )
     {
     	.reg .pred 	%p<7>;
     	.reg .f32 	%f<6>;
     	.reg .b32 	%r<22>;
     	.reg .b64 	%rd<3>;


     	ld.param.u64 	%rd2, [multivariate_te_kernel_param_4];
     	ld.param.u32 	%r8, [multivariate_te_kernel_param_5+12];
     	ld.param.u32 	%r9, [multivariate_te_kernel_param_5+8];
     	ld.param.u32 	%r10, [multivariate_te_kernel_param_5+4];
     	cvta.to.global.u64 	%rd1, %rd2;
     	mov.u32 	%r1, %ntid.x;
     	mov.u32 	%r2, %ctaid.x;
     	mov.u32 	%r3, %tid.x;
     	mad.lo.s32 	%r11, %r2, %r1, %r3;
     	sub.s32 	%r12, %r10, %r9;
     	sub.s32 	%r13, %r12, %r8;
     	setp.lt.s32 	%p1, %r11, %r13;
     	@%p1 bra 	$L__BB3_3;
     	bra.uni 	$L__BB3_1;

     $L__BB3_3:
     	shl.b32 	%r16, %r3, 2;
     	mov.u32 	%r17, block_shared;
     	add.s32 	%r4, %r17, %r16;
     	mov.u32 	%r18, 0;
     	st.shared.u32 	[%r4], %r18;
     	bar.sync 	0;
     	shr.u32 	%r21, %r1, 1;
     	setp.eq.s32 	%p3, %r21, 0;
     	@%p3 bra 	$L__BB3_7;

     $L__BB3_4:
     	setp.ge.s32 	%p4, %r3, %r21;
     	@%p4 bra 	$L__BB3_6;

     	shl.b32 	%r19, %r21, 2;
     	add.s32 	%r20, %r4, %r19;
     	ld.shared.f32 	%f1, [%r4];
     	ld.shared.f32 	%f2, [%r20];
     	add.ftz.f32 	%f3, %f2, %f1;
     	st.shared.f32 	[%r4], %f3;

     $L__BB3_6:
     	bar.sync 	0;
     	shr.u32 	%r21, %r21, 1;
     	setp.ne.s32 	%p5, %r21, 0;
     	@%p5 bra 	$L__BB3_4;

     $L__BB3_7:
     	setp.ne.s32 	%p6, %r3, 0;
     	@%p6 bra 	$L__BB3_9;

     	ld.shared.f32 	%f4, [block_shared];
     	atom.global.add.f32 	%f5, [%rd1], %f4;
     	bra.uni 	$L__BB3_9;

     $L__BB3_1:
     	or.b32  	%r14, %r3, %r2;
     	setp.ne.s32 	%p2, %r14, 0;
     	@%p2 bra 	$L__BB3_9;

     	mov.u32 	%r15, 0;
     	st.global.u32 	[%rd1], %r15;

     $L__BB3_9:
     	ret;

     }
     	// .globl	sliding_window_te_kernel
     .visible .entry sliding_window_te_kernel(
     	.param .u64 sliding_window_te_kernel_param_0,
     	.param .u64 sliding_window_te_kernel_param_1,
     	.param .u32 sliding_window_te_kernel_param_2,
     	.param .u32 sliding_window_te_kernel_param_3,
     	.param .u32 sliding_window_te_kernel_param_4,
     	.param .u32 sliding_window_te_kernel_param_5,
     	.param .align 4 .b8 sliding_window_te_kernel_param_6[24]
     )
     {
     	.reg .pred 	%p<10>;
     	.reg .f32 	%f<46>;
     	.reg .b32 	%r<66>;
     	.reg .b64 	%rd<24>;


     	ld.param.u64 	%rd13, [sliding_window_te_kernel_param_0];
     	ld.param.u64 	%rd12, [sliding_window_te_kernel_param_1];
     	ld.param.u32 	%r24, [sliding_window_te_kernel_param_2];
     	ld.param.u32 	%r25, [sliding_window_te_kernel_param_3];
     	ld.param.u32 	%r26, [sliding_window_te_kernel_param_4];
     	ld.param.u32 	%r27, [sliding_window_te_kernel_param_5];
     	ld.param.u32 	%r31, [sliding_window_te_kernel_param_6+12];
     	ld.param.u32 	%r30, [sliding_window_te_kernel_param_6+8];
     	ld.param.u32 	%r29, [sliding_window_te_kernel_param_6+4];
     	cvta.to.global.u64 	%rd1, %rd13;
     	mov.u32 	%r33, %ntid.x;
     	mov.u32 	%r34, %ctaid.x;
     	mov.u32 	%r35, %tid.x;
     	mad.lo.s32 	%r1, %r34, %r33, %r35;
     	sub.s32 	%r36, %r29, %r26;
     	div.s32 	%r37, %r36, %r27;
     	setp.gt.s32 	%p1, %r1, %r37;
     	@%p1 bra 	$L__BB4_11;

     	mul.lo.s32 	%r5, %r1, %r27;
     	add.s32 	%r39, %r5, %r26;
     	add.s32 	%r61, %r30, %r5;
     	sub.s32 	%r40, %r39, %r31;
     	setp.ge.s32 	%p2, %r61, %r40;
     	setp.ge.s32 	%p3, %r61, %r29;
     	mov.u32 	%r65, 0;
     	mov.f32 	%f45, 0f00000000;
     	or.pred  	%p4, %p2, %p3;
     	mov.f32 	%f44, %f45;
     	@%p4 bra 	$L__BB4_8;

     	add.s32 	%r43, %r31, %r30;
     	sub.s32 	%r44, %r43, %r26;
     	sub.s32 	%r45, %r61, %r29;
     	max.u32 	%r7, %r44, %r45;
     	neg.s32 	%r46, %r7;
     	mov.u32 	%r65, 0;
     	and.b32  	%r64, %r46, 3;
     	setp.gt.u32 	%p5, %r7, -4;
     	mov.f32 	%f44, 0f00000000;
     	@%p5 bra 	$L__BB4_5;

     	mad.lo.s32 	%r49, %r29, %r25, %r43;
     	add.s32 	%r50, %r49, %r5;
     	mad.lo.s32 	%r51, %r29, %r24, %r30;
     	add.s32 	%r52, %r51, %r5;
     	mul.wide.s32 	%rd2, %r52, 4;
     	mul.wide.s32 	%rd3, %r50, 4;
     	add.s32 	%r53, %r7, %r64;
     	neg.s32 	%r57, %r53;
     	mov.f32 	%f44, 0f00000000;
     	mov.u32 	%r65, 0;
     	mov.u64 	%rd21, %rd1;

     $L__BB4_4:
     	add.s64 	%rd14, %rd21, %rd2;
     	add.s64 	%rd15, %rd21, %rd3;
     	ld.global.nc.f32 	%f15, [%rd15];
     	ld.global.nc.f32 	%f16, [%rd14];
     	mul.ftz.f32 	%f17, %f16, %f15;
     	abs.ftz.f32 	%f18, %f17;
     	fma.rn.ftz.f32 	%f19, %f18, 0f3C23D70A, %f44;
     	ld.global.nc.f32 	%f20, [%rd15+4];
     	ld.global.nc.f32 	%f21, [%rd14+4];
     	mul.ftz.f32 	%f22, %f21, %f20;
     	abs.ftz.f32 	%f23, %f22;
     	fma.rn.ftz.f32 	%f24, %f23, 0f3C23D70A, %f19;
     	ld.global.nc.f32 	%f25, [%rd15+8];
     	ld.global.nc.f32 	%f26, [%rd14+8];
     	mul.ftz.f32 	%f27, %f26, %f25;
     	abs.ftz.f32 	%f28, %f27;
     	fma.rn.ftz.f32 	%f29, %f28, 0f3C23D70A, %f24;
     	ld.global.nc.f32 	%f30, [%rd15+12];
     	ld.global.nc.f32 	%f31, [%rd14+12];
     	mul.ftz.f32 	%f32, %f31, %f30;
     	abs.ftz.f32 	%f33, %f32;
     	fma.rn.ftz.f32 	%f44, %f33, 0f3C23D70A, %f29;
     	add.s32 	%r65, %r65, 4;
     	add.s32 	%r61, %r61, 4;
     	add.s64 	%rd21, %rd21, 16;
     	add.s32 	%r57, %r57, -4;
     	setp.ne.s32 	%p6, %r57, 0;
     	@%p6 bra 	$L__BB4_4;

     $L__BB4_5:
     	setp.eq.s32 	%p7, %r64, 0;
     	@%p7 bra 	$L__BB4_8;

     	add.s32 	%r54, %r31, %r61;
     	mad.lo.s32 	%r55, %r29, %r25, %r54;
     	mul.wide.s32 	%rd16, %r55, 4;
     	add.s64 	%rd23, %rd1, %rd16;
     	mad.lo.s32 	%r56, %r29, %r24, %r61;
     	mul.wide.s32 	%rd17, %r56, 4;
     	add.s64 	%rd22, %rd1, %rd17;

     $L__BB4_7:
     	.pragma "nounroll";
     	ld.global.nc.f32 	%f34, [%rd23];
     	ld.global.nc.f32 	%f35, [%rd22];
     	mul.ftz.f32 	%f36, %f35, %f34;
     	abs.ftz.f32 	%f37, %f36;
     	fma.rn.ftz.f32 	%f44, %f37, 0f3C23D70A, %f44;
     	add.s32 	%r65, %r65, 1;
     	add.s64 	%rd23, %rd23, 4;
     	add.s64 	%rd22, %rd22, 4;
     	add.s32 	%r64, %r64, -1;
     	setp.ne.s32 	%p8, %r64, 0;
     	@%p8 bra 	$L__BB4_7;

     $L__BB4_8:
     	setp.eq.s32 	%p9, %r65, 0;
     	@%p9 bra 	$L__BB4_10;

     	cvt.rn.f32.s32 	%f39, %r65;
     	div.approx.ftz.f32 	%f45, %f44, %f39;

     $L__BB4_10:
     	cvta.to.global.u64 	%rd18, %rd12;
     	mul.wide.s32 	%rd19, %r1, 4;
     	add.s64 	%rd20, %rd18, %rd19;
     	st.global.f32 	[%rd20], %f45;

     $L__BB4_11:
     	ret;

     }
     	// .globl	ensemble_te_kernel
     .visible .entry ensemble_te_kernel(
     	.param .u64 ensemble_te_kernel_param_0,
     	.param .u64 ensemble_te_kernel_param_1,
     	.param .u64 ensemble_te_kernel_param_2,
     	.param .u32 ensemble_te_kernel_param_3
     )
     {
     	.reg .pred 	%p<7>;
     	.reg .f32 	%f<55>;
     	.reg .b32 	%r<30>;
     	.reg .b64 	%rd<7>;


     	ld.param.u64 	%rd1, [ensemble_te_kernel_param_1];
     	ld.param.u64 	%rd2, [ensemble_te_kernel_param_2];
     	ld.param.u32 	%r12, [ensemble_te_kernel_param_3];
     	mov.u32 	%r13, %ntid.x;
     	mov.u32 	%r14, %ctaid.x;
     	mov.u32 	%r15, %tid.x;
     	mad.lo.s32 	%r1, %r14, %r13, %r15;
     	cvta.to.global.u64 	%rd3, %rd2;
     	ld.global.nc.u32 	%r16, [%rd3];
     	mul.lo.s32 	%r17, %r16, %r16;
     	setp.ge.s32 	%p1, %r1, %r17;
     	@%p1 bra 	$L__BB5_9;

     	setp.lt.s32 	%p2, %r12, 1;
     	mov.f32 	%f53, 0f00000000;
     	mov.f32 	%f54, %f53;
     	@%p2 bra 	$L__BB5_8;

     	add.s32 	%r19, %r12, -1;
     	and.b32  	%r29, %r12, 3;
     	setp.lt.u32 	%p3, %r19, 3;
     	mov.f32 	%f54, 0f00000000;
     	mov.u32 	%r27, 0;
     	mov.f32 	%f53, %f54;
     	@%p3 bra 	$L__BB5_5;

     	sub.s32 	%r3, %r29, %r12;
     	mov.f32 	%f54, 0f00000000;
     	mov.u32 	%r27, 0;

     $L__BB5_4:
     	add.s32 	%r21, %r27, 1;
     	cvt.rn.f32.s32 	%f22, %r21;
     	mul.ftz.f32 	%f23, %f22, 0f3DCCCCCD;
     	add.ftz.f32 	%f24, %f53, %f23;
     	fma.rn.ftz.f32 	%f25, %f23, %f23, %f54;
     	add.s32 	%r22, %r27, 2;
     	cvt.rn.f32.s32 	%f26, %r22;
     	mul.ftz.f32 	%f27, %f26, 0f3DCCCCCD;
     	add.ftz.f32 	%f28, %f24, %f27;
     	fma.rn.ftz.f32 	%f29, %f27, %f27, %f25;
     	add.s32 	%r23, %r27, 3;
     	cvt.rn.f32.s32 	%f30, %r23;
     	mul.ftz.f32 	%f31, %f30, 0f3DCCCCCD;
     	add.ftz.f32 	%f32, %f28, %f31;
     	fma.rn.ftz.f32 	%f33, %f31, %f31, %f29;
     	add.s32 	%r27, %r27, 4;
     	cvt.rn.f32.s32 	%f34, %r27;
     	mul.ftz.f32 	%f35, %f34, 0f3DCCCCCD;
     	add.ftz.f32 	%f53, %f32, %f35;
     	fma.rn.ftz.f32 	%f54, %f35, %f35, %f33;
     	add.s32 	%r24, %r3, %r27;
     	setp.ne.s32 	%p4, %r24, 0;
     	@%p4 bra 	$L__BB5_4;

     $L__BB5_5:
     	setp.eq.s32 	%p5, %r29, 0;
     	@%p5 bra 	$L__BB5_8;

     	add.s32 	%r28, %r27, 1;

     $L__BB5_7:
     	.pragma "nounroll";
     	cvt.rn.f32.s32 	%f36, %r28;
     	mul.ftz.f32 	%f37, %f36, 0f3DCCCCCD;
     	add.ftz.f32 	%f53, %f53, %f37;
     	fma.rn.ftz.f32 	%f54, %f37, %f37, %f54;
     	add.s32 	%r28, %r28, 1;
     	add.s32 	%r29, %r29, -1;
     	setp.ne.s32 	%p6, %r29, 0;
     	@%p6 bra 	$L__BB5_7;

     $L__BB5_8:
     	cvt.rn.f32.s32 	%f38, %r12;
     	div.approx.ftz.f32 	%f39, %f53, %f38;
     	div.approx.ftz.f32 	%f40, %f54, %f38;
     	mul.ftz.f32 	%f41, %f39, %f39;
     	sub.ftz.f32 	%f42, %f40, %f41;
     	add.ftz.f32 	%f43, %f42, 0f2EDBE6FF;
     	sqrt.approx.ftz.f32 	%f44, %f43;
     	shl.b32 	%r25, %r1, 1;
     	cvta.to.global.u64 	%rd4, %rd1;
     	mul.wide.s32 	%rd5, %r25, 4;
     	add.s64 	%rd6, %rd4, %rd5;
     	st.global.f32 	[%rd6], %f39;
     	st.global.f32 	[%rd6+4], %f44;

     $L__BB5_9:
     	ret;

     }
     	// .globl	te_performance_metrics
     .visible .entry te_performance_metrics(
     	.param .u64 te_performance_metrics_param_0,
     	.param .u64 te_performance_metrics_param_1,
     	.param .u32 te_performance_metrics_param_2
     )
     {
     	.reg .pred 	%p<13>;
     	.reg .f32 	%f<72>;
     	.reg .b32 	%r<52>;
     	.reg .b64 	%rd<12>;


     	ld.param.u64 	%rd7, [te_performance_metrics_param_0];
     	ld.param.u64 	%rd6, [te_performance_metrics_param_1];
     	ld.param.u32 	%r26, [te_performance_metrics_param_2];
     	cvta.to.global.u64 	%rd1, %rd7;
     	mov.u32 	%r27, %ctaid.x;
     	mov.u32 	%r28, %tid.x;
     	or.b32  	%r29, %r28, %r27;
     	setp.ne.s32 	%p1, %r29, 0;
     	@%p1 bra 	$L__BB6_21;

     	mul.lo.s32 	%r1, %r26, %r26;
     	setp.eq.s32 	%p2, %r1, 0;
     	mov.f32 	%f71, 0f00000000;
     	mov.u32 	%r41, 0;
     	mov.f32 	%f53, %f71;
     	mov.f32 	%f54, %f71;
     	@%p2 bra 	$L__BB6_18;

     	add.s32 	%r34, %r1, -1;
     	and.b32  	%r49, %r1, 3;
     	setp.lt.u32 	%p3, %r34, 3;
     	mov.u32 	%r46, 0;
     	mov.f32 	%f54, 0f00000000;
     	mov.f32 	%f53, %f54;
     	mov.u32 	%r41, %r46;
     	@%p3 bra 	$L__BB6_13;

     	sub.s32 	%r40, %r1, %r49;
     	mov.u32 	%r46, 0;
     	mov.f32 	%f54, 0f00000000;

     $L__BB6_4:
     	mul.wide.s32 	%rd8, %r46, 4;
     	add.s64 	%rd2, %rd1, %rd8;
     	ld.global.nc.f32 	%f3, [%rd2];
     	setp.leu.ftz.f32 	%p4, %f3, 0f3C23D70A;
     	@%p4 bra 	$L__BB6_6;

     	add.s32 	%r41, %r41, 1;
     	add.ftz.f32 	%f53, %f53, %f3;
     	max.ftz.f32 	%f54, %f54, %f3;

     $L__BB6_6:
     	ld.global.nc.f32 	%f8, [%rd2+4];
     	setp.leu.ftz.f32 	%p5, %f8, 0f3C23D70A;
     	@%p5 bra 	$L__BB6_8;

     	add.s32 	%r41, %r41, 1;
     	add.ftz.f32 	%f53, %f53, %f8;
     	max.ftz.f32 	%f54, %f54, %f8;

     $L__BB6_8:
     	ld.global.nc.f32 	%f13, [%rd2+8];
     	setp.leu.ftz.f32 	%p6, %f13, 0f3C23D70A;
     	@%p6 bra 	$L__BB6_10;

     	add.s32 	%r41, %r41, 1;
     	add.ftz.f32 	%f53, %f53, %f13;
     	max.ftz.f32 	%f54, %f54, %f13;

     $L__BB6_10:
     	ld.global.nc.f32 	%f18, [%rd2+12];
     	setp.leu.ftz.f32 	%p7, %f18, 0f3C23D70A;
     	@%p7 bra 	$L__BB6_12;

     	add.s32 	%r41, %r41, 1;
     	add.ftz.f32 	%f53, %f53, %f18;
     	max.ftz.f32 	%f54, %f54, %f18;

     $L__BB6_12:
     	add.s32 	%r46, %r46, 4;
     	add.s32 	%r40, %r40, -4;
     	setp.ne.s32 	%p8, %r40, 0;
     	@%p8 bra 	$L__BB6_4;

     $L__BB6_13:
     	setp.eq.s32 	%p9, %r49, 0;
     	@%p9 bra 	$L__BB6_18;

     	mul.wide.s32 	%rd9, %r46, 4;
     	add.s64 	%rd11, %rd1, %rd9;

     $L__BB6_15:
     	.pragma "nounroll";
     	ld.global.nc.f32 	%f29, [%rd11];
     	setp.leu.ftz.f32 	%p10, %f29, 0f3C23D70A;
     	@%p10 bra 	$L__BB6_17;

     	add.s32 	%r41, %r41, 1;
     	add.ftz.f32 	%f53, %f53, %f29;
     	max.ftz.f32 	%f54, %f54, %f29;

     $L__BB6_17:
     	add.s64 	%rd11, %rd11, 4;
     	add.s32 	%r49, %r49, -1;
     	setp.ne.s32 	%p11, %r49, 0;
     	@%p11 bra 	$L__BB6_15;

     $L__BB6_18:
     	cvt.rn.f32.s32 	%f36, %r41;
     	setp.lt.s32 	%p12, %r41, 1;
     	@%p12 bra 	$L__BB6_20;

     	div.approx.ftz.f32 	%f71, %f53, %f36;

     $L__BB6_20:
     	cvta.to.global.u64 	%rd10, %rd6;
     	sub.s32 	%r37, %r1, %r26;
     	cvt.rn.f32.s32 	%f47, %r37;
     	div.approx.ftz.f32 	%f48, %f36, %f47;
     	mov.f32 	%f49, 0f3F800000;
     	sub.ftz.f32 	%f50, %f49, %f48;
     	st.global.f32 	[%rd10], %f50;
     	st.global.f32 	[%rd10+4], %f71;
     	st.global.f32 	[%rd10+8], %f54;

     $L__BB6_21:
     	ret;

     }
     //
     // Generated by NVIDIA NVVM Compiler
     //
     // Compiler Build ID: CL-35059454
     // Cuda compilation tools, release 12.6, V12.6.85
     // Based on NVVM 7.0.1
     //

     .version 8.5
     .target sm_86
     .address_size 64

     	// .globl	gemv_kernel
     .extern .shared .align 16 .b8 sdata[];

     .visible .entry gemv_kernel(
     	.param .u64 gemv_kernel_param_0,
     	.param .u64 gemv_kernel_param_1,
     	.param .u64 gemv_kernel_param_2,
     	.param .f64 gemv_kernel_param_3,
     	.param .f64 gemv_kernel_param_4,
     	.param .u32 gemv_kernel_param_5,
     	.param .u32 gemv_kernel_param_6
     )
     {
     	.reg .pred 	%p<7>;
     	.reg .b32 	%r<25>;
     	.reg .f64 	%fd<35>;
     	.reg .b64 	%rd<27>;


     	ld.param.u64 	%rd15, [gemv_kernel_param_0];
     	ld.param.u64 	%rd16, [gemv_kernel_param_1];
     	ld.param.u64 	%rd14, [gemv_kernel_param_2];
     	ld.param.f64 	%fd8, [gemv_kernel_param_3];
     	ld.param.f64 	%fd9, [gemv_kernel_param_4];
     	ld.param.u32 	%r12, [gemv_kernel_param_5];
     	ld.param.u32 	%r11, [gemv_kernel_param_6];
     	cvta.to.global.u64 	%rd1, %rd16;
     	cvta.to.global.u64 	%rd2, %rd15;
     	mov.u32 	%r13, %ntid.x;
     	mov.u32 	%r14, %ctaid.x;
     	mov.u32 	%r15, %tid.x;
     	mad.lo.s32 	%r1, %r14, %r13, %r15;
     	setp.ge.s32 	%p1, %r1, %r12;
     	@%p1 bra 	$L__BB0_9;

     	setp.lt.s32 	%p2, %r11, 1;
     	mov.f64 	%fd34, 0d0000000000000000;
     	@%p2 bra 	$L__BB0_8;

     	add.s32 	%r17, %r11, -1;
     	and.b32  	%r24, %r11, 3;
     	setp.lt.u32 	%p3, %r17, 3;
     	mov.f64 	%fd34, 0d0000000000000000;
     	mov.u32 	%r23, 0;
     	@%p3 bra 	$L__BB0_5;

     	sub.s32 	%r22, %r11, %r24;
     	mul.lo.s32 	%r19, %r11, %r1;
     	mul.wide.s32 	%rd17, %r19, 8;
     	add.s64 	%rd24, %rd2, %rd17;
     	mov.f64 	%fd34, 0d0000000000000000;
     	mov.u32 	%r23, 0;
     	mov.u64 	%rd23, %rd1;

     $L__BB0_4:
     	ld.global.f64 	%fd14, [%rd23];
     	ld.global.f64 	%fd15, [%rd24];
     	fma.rn.f64 	%fd16, %fd15, %fd14, %fd34;
     	ld.global.f64 	%fd17, [%rd23+8];
     	ld.global.f64 	%fd18, [%rd24+8];
     	fma.rn.f64 	%fd19, %fd18, %fd17, %fd16;
     	ld.global.f64 	%fd20, [%rd23+16];
     	ld.global.f64 	%fd21, [%rd24+16];
     	fma.rn.f64 	%fd22, %fd21, %fd20, %fd19;
     	ld.global.f64 	%fd23, [%rd23+24];
     	ld.global.f64 	%fd24, [%rd24+24];
     	fma.rn.f64 	%fd34, %fd24, %fd23, %fd22;
     	add.s32 	%r23, %r23, 4;
     	add.s64 	%rd24, %rd24, 32;
     	add.s64 	%rd23, %rd23, 32;
     	add.s32 	%r22, %r22, -4;
     	setp.ne.s32 	%p4, %r22, 0;
     	@%p4 bra 	$L__BB0_4;

     $L__BB0_5:
     	setp.eq.s32 	%p5, %r24, 0;
     	@%p5 bra 	$L__BB0_8;

     	mul.wide.s32 	%rd18, %r23, 8;
     	add.s64 	%rd26, %rd1, %rd18;
     	mad.lo.s32 	%r20, %r11, %r1, %r23;
     	mul.wide.s32 	%rd19, %r20, 8;
     	add.s64 	%rd25, %rd2, %rd19;

     $L__BB0_7:
     	.pragma "nounroll";
     	ld.global.f64 	%fd25, [%rd26];
     	ld.global.f64 	%fd26, [%rd25];
     	fma.rn.f64 	%fd34, %fd26, %fd25, %fd34;
     	add.s64 	%rd26, %rd26, 8;
     	add.s64 	%rd25, %rd25, 8;
     	add.s32 	%r24, %r24, -1;
     	setp.ne.s32 	%p6, %r24, 0;
     	@%p6 bra 	$L__BB0_7;

     $L__BB0_8:
     	cvta.to.global.u64 	%rd20, %rd14;
     	mul.wide.s32 	%rd21, %r1, 8;
     	add.s64 	%rd22, %rd20, %rd21;
     	ld.global.f64 	%fd27, [%rd22];
     	mul.f64 	%fd28, %fd27, %fd9;
     	fma.rn.f64 	%fd29, %fd34, %fd8, %fd28;
     	st.global.f64 	[%rd22], %fd29;

     $L__BB0_9:
     	ret;

     }
     	// .globl	prediction_error_kernel
     .visible .entry prediction_error_kernel(
     	.param .u64 prediction_error_kernel_param_0,
     	.param .u64 prediction_error_kernel_param_1,
     	.param .u64 prediction_error_kernel_param_2,
     	.param .u64 prediction_error_kernel_param_3,
     	.param .u32 prediction_error_kernel_param_4
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .b32 	%r<6>;
     	.reg .f64 	%fd<6>;
     	.reg .b64 	%rd<14>;


     	ld.param.u64 	%rd1, [prediction_error_kernel_param_0];
     	ld.param.u64 	%rd2, [prediction_error_kernel_param_1];
     	ld.param.u64 	%rd3, [prediction_error_kernel_param_2];
     	ld.param.u64 	%rd4, [prediction_error_kernel_param_3];
     	ld.param.u32 	%r2, [prediction_error_kernel_param_4];
     	mov.u32 	%r3, %ctaid.x;
     	mov.u32 	%r4, %ntid.x;
     	mov.u32 	%r5, %tid.x;
     	mad.lo.s32 	%r1, %r3, %r4, %r5;
     	setp.ge.s32 	%p1, %r1, %r2;
     	@%p1 bra 	$L__BB1_2;

     	cvta.to.global.u64 	%rd5, %rd2;
     	mul.wide.s32 	%rd6, %r1, 8;
     	add.s64 	%rd7, %rd5, %rd6;
     	cvta.to.global.u64 	%rd8, %rd3;
     	add.s64 	%rd9, %rd8, %rd6;
     	ld.global.f64 	%fd1, [%rd9];
     	ld.global.f64 	%fd2, [%rd7];
     	sub.f64 	%fd3, %fd2, %fd1;
     	cvta.to.global.u64 	%rd10, %rd4;
     	add.s64 	%rd11, %rd10, %rd6;
     	ld.global.f64 	%fd4, [%rd11];
     	mul.f64 	%fd5, %fd4, %fd3;
     	cvta.to.global.u64 	%rd12, %rd1;
     	add.s64 	%rd13, %rd12, %rd6;
     	st.global.f64 	[%rd13], %fd5;

     $L__BB1_2:
     	ret;

     }
     	// .globl	belief_update_kernel
     .visible .entry belief_update_kernel(
     	.param .u64 belief_update_kernel_param_0,
     	.param .u64 belief_update_kernel_param_1,
     	.param .f64 belief_update_kernel_param_2,
     	.param .u32 belief_update_kernel_param_3
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .b32 	%r<6>;
     	.reg .f64 	%fd<9>;
     	.reg .b64 	%rd<8>;


     	ld.param.u64 	%rd1, [belief_update_kernel_param_0];
     	ld.param.u64 	%rd2, [belief_update_kernel_param_1];
     	ld.param.f64 	%fd1, [belief_update_kernel_param_2];
     	ld.param.u32 	%r2, [belief_update_kernel_param_3];
     	mov.u32 	%r3, %ctaid.x;
     	mov.u32 	%r4, %ntid.x;
     	mov.u32 	%r5, %tid.x;
     	mad.lo.s32 	%r1, %r3, %r4, %r5;
     	setp.ge.s32 	%p1, %r1, %r2;
     	@%p1 bra 	$L__BB2_2;

     	cvta.to.global.u64 	%rd3, %rd1;
     	cvta.to.global.u64 	%rd4, %rd2;
     	mul.wide.s32 	%rd5, %r1, 8;
     	add.s64 	%rd6, %rd4, %rd5;
     	ld.global.f64 	%fd2, [%rd6];
     	add.s64 	%rd7, %rd3, %rd5;
     	ld.global.f64 	%fd3, [%rd7];
     	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
     	mov.f64 	%fd5, 0d3FF0000000000000;
     	min.f64 	%fd6, %fd5, %fd4;
     	mov.f64 	%fd7, 0d0000000000000000;
     	max.f64 	%fd8, %fd7, %fd6;
     	st.global.f64 	[%rd7], %fd8;

     $L__BB2_2:
     	ret;

     }
     	// .globl	precision_weight_kernel
     .visible .entry precision_weight_kernel(
     	.param .u64 precision_weight_kernel_param_0,
     	.param .u64 precision_weight_kernel_param_1,
     	.param .u64 precision_weight_kernel_param_2,
     	.param .u64 precision_weight_kernel_param_3,
     	.param .u32 precision_weight_kernel_param_4
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .b32 	%r<6>;
     	.reg .f64 	%fd<7>;
     	.reg .b64 	%rd<14>;


     	ld.param.u64 	%rd1, [precision_weight_kernel_param_0];
     	ld.param.u64 	%rd2, [precision_weight_kernel_param_1];
     	ld.param.u64 	%rd3, [precision_weight_kernel_param_2];
     	ld.param.u64 	%rd4, [precision_weight_kernel_param_3];
     	ld.param.u32 	%r2, [precision_weight_kernel_param_4];
     	mov.u32 	%r3, %ctaid.x;
     	mov.u32 	%r4, %ntid.x;
     	mov.u32 	%r5, %tid.x;
     	mad.lo.s32 	%r1, %r3, %r4, %r5;
     	setp.ge.s32 	%p1, %r1, %r2;
     	@%p1 bra 	$L__BB3_2;

     	cvta.to.global.u64 	%rd5, %rd2;
     	mul.wide.s32 	%rd6, %r1, 8;
     	add.s64 	%rd7, %rd5, %rd6;
     	cvta.to.global.u64 	%rd8, %rd3;
     	add.s64 	%rd9, %rd8, %rd6;
     	ld.global.f64 	%fd1, [%rd9];
     	ld.global.f64 	%fd2, [%rd7];
     	sub.f64 	%fd3, %fd2, %fd1;
     	cvta.to.global.u64 	%rd10, %rd4;
     	add.s64 	%rd11, %rd10, %rd6;
     	ld.global.f64 	%fd4, [%rd11];
     	mul.f64 	%fd5, %fd4, %fd3;
     	mul.f64 	%fd6, %fd3, %fd5;
     	cvta.to.global.u64 	%rd12, %rd1;
     	add.s64 	%rd13, %rd12, %rd6;
     	st.global.f64 	[%rd13], %fd6;

     $L__BB3_2:
     	ret;

     }
     	// .globl	kl_divergence_kernel
     .visible .entry kl_divergence_kernel(
     	.param .u64 kl_divergence_kernel_param_0,
     	.param .u64 kl_divergence_kernel_param_1,
     	.param .u64 kl_divergence_kernel_param_2,
     	.param .u64 kl_divergence_kernel_param_3,
     	.param .u64 kl_divergence_kernel_param_4,
     	.param .u32 kl_divergence_kernel_param_5
     )
     {
     	.reg .pred 	%p<7>;
     	.reg .f32 	%f<2>;
     	.reg .b32 	%r<33>;
     	.reg .f64 	%fd<80>;
     	.reg .b64 	%rd<19>;


     	ld.param.u64 	%rd2, [kl_divergence_kernel_param_0];
     	ld.param.u64 	%rd3, [kl_divergence_kernel_param_1];
     	ld.param.u64 	%rd4, [kl_divergence_kernel_param_2];
     	ld.param.u64 	%rd5, [kl_divergence_kernel_param_3];
     	ld.param.u64 	%rd6, [kl_divergence_kernel_param_4];
     	ld.param.u32 	%r12, [kl_divergence_kernel_param_5];
     	mov.u32 	%r13, %ntid.x;
     	mov.u32 	%r14, %ctaid.x;
     	mov.u32 	%r15, %tid.x;
     	mad.lo.s32 	%r1, %r14, %r13, %r15;
     	setp.ge.s32 	%p1, %r1, %r12;
     	@%p1 bra 	$L__BB4_11;

     	cvta.to.global.u64 	%rd7, %rd4;
     	cvt.s64.s32 	%rd1, %r1;
     	mul.wide.s32 	%rd8, %r1, 8;
     	add.s64 	%rd9, %rd7, %rd8;
     	ld.global.f64 	%fd16, [%rd9];
     	mov.f64 	%fd17, 0d3DDB7CDFD9D7BDBB;
     	max.f64 	%fd1, %fd16, %fd17;
     	cvta.to.global.u64 	%rd10, %rd5;
     	add.s64 	%rd11, %rd10, %rd8;
     	ld.global.f64 	%fd18, [%rd11];
     	max.f64 	%fd2, %fd18, %fd17;
     	cvta.to.global.u64 	%rd12, %rd3;
     	add.s64 	%rd13, %rd12, %rd8;
     	cvta.to.global.u64 	%rd14, %rd2;
     	add.s64 	%rd15, %rd14, %rd8;
     	ld.global.f64 	%fd19, [%rd15];
     	ld.global.f64 	%fd20, [%rd13];
     	sub.f64 	%fd3, %fd20, %fd19;
     	div.rn.f64 	%fd76, %fd2, %fd1;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r29}, %fd76;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r30, %temp}, %fd76;
     	}
     	setp.gt.s32 	%p2, %r29, 1048575;
     	mov.u32 	%r31, -1023;
     	@%p2 bra 	$L__BB4_3;

     	mul.f64 	%fd76, %fd76, 0d4350000000000000;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r29}, %fd76;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r30, %temp}, %fd76;
     	}
     	mov.u32 	%r31, -1077;

     $L__BB4_3:
     	add.s32 	%r18, %r29, -1;
     	setp.lt.u32 	%p3, %r18, 2146435071;
     	@%p3 bra 	$L__BB4_5;
     	bra.uni 	$L__BB4_4;

     $L__BB4_5:
     	shr.u32 	%r20, %r29, 20;
     	add.s32 	%r32, %r31, %r20;
     	and.b32  	%r21, %r29, -2146435073;
     	or.b32  	%r22, %r21, 1072693248;
     	mov.b64 	%fd77, {%r30, %r22};
     	setp.lt.s32 	%p5, %r22, 1073127583;
     	@%p5 bra 	$L__BB4_7;

     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r23, %temp}, %fd77;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r24}, %fd77;
     	}
     	add.s32 	%r25, %r24, -1048576;
     	mov.b64 	%fd77, {%r23, %r25};
     	add.s32 	%r32, %r32, 1;

     $L__BB4_7:
     	add.f64 	%fd23, %fd77, 0d3FF0000000000000;
     	mov.f64 	%fd24, 0d3FF0000000000000;
     	rcp.approx.ftz.f64 	%fd25, %fd23;
     	neg.f64 	%fd26, %fd23;
     	fma.rn.f64 	%fd27, %fd26, %fd25, %fd24;
     	fma.rn.f64 	%fd28, %fd27, %fd27, %fd27;
     	fma.rn.f64 	%fd29, %fd28, %fd25, %fd25;
     	add.f64 	%fd30, %fd77, 0dBFF0000000000000;
     	mul.f64 	%fd31, %fd30, %fd29;
     	fma.rn.f64 	%fd32, %fd30, %fd29, %fd31;
     	mul.f64 	%fd33, %fd32, %fd32;
     	mov.f64 	%fd34, 0d3ED0EE258B7A8B04;
     	mov.f64 	%fd35, 0d3EB1380B3AE80F1E;
     	fma.rn.f64 	%fd36, %fd35, %fd33, %fd34;
     	mov.f64 	%fd37, 0d3EF3B2669F02676F;
     	fma.rn.f64 	%fd38, %fd36, %fd33, %fd37;
     	mov.f64 	%fd39, 0d3F1745CBA9AB0956;
     	fma.rn.f64 	%fd40, %fd38, %fd33, %fd39;
     	mov.f64 	%fd41, 0d3F3C71C72D1B5154;
     	fma.rn.f64 	%fd42, %fd40, %fd33, %fd41;
     	mov.f64 	%fd43, 0d3F624924923BE72D;
     	fma.rn.f64 	%fd44, %fd42, %fd33, %fd43;
     	mov.f64 	%fd45, 0d3F8999999999A3C4;
     	fma.rn.f64 	%fd46, %fd44, %fd33, %fd45;
     	mov.f64 	%fd47, 0d3FB5555555555554;
     	fma.rn.f64 	%fd48, %fd46, %fd33, %fd47;
     	sub.f64 	%fd49, %fd30, %fd32;
     	add.f64 	%fd50, %fd49, %fd49;
     	neg.f64 	%fd51, %fd32;
     	fma.rn.f64 	%fd52, %fd51, %fd30, %fd50;
     	mul.f64 	%fd53, %fd29, %fd52;
     	mul.f64 	%fd54, %fd33, %fd48;
     	fma.rn.f64 	%fd55, %fd54, %fd32, %fd53;
     	xor.b32  	%r26, %r32, -2147483648;
     	mov.u32 	%r27, -2147483648;
     	mov.u32 	%r28, 1127219200;
     	mov.b64 	%fd56, {%r26, %r28};
     	mov.b64 	%fd57, {%r27, %r28};
     	sub.f64 	%fd58, %fd56, %fd57;
     	mov.f64 	%fd59, 0d3FE62E42FEFA39EF;
     	fma.rn.f64 	%fd60, %fd58, %fd59, %fd32;
     	neg.f64 	%fd61, %fd58;
     	fma.rn.f64 	%fd62, %fd61, %fd59, %fd60;
     	sub.f64 	%fd63, %fd62, %fd32;
     	sub.f64 	%fd64, %fd55, %fd63;
     	mov.f64 	%fd65, 0d3C7ABC9E3B39803F;
     	fma.rn.f64 	%fd66, %fd58, %fd65, %fd64;
     	add.f64 	%fd78, %fd60, %fd66;
     	bra.uni 	$L__BB4_8;

     $L__BB4_4:
     	mov.f64 	%fd21, 0d7FF0000000000000;
     	fma.rn.f64 	%fd22, %fd76, %fd21, %fd21;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r19}, %fd76;
     	}
     	mov.b32 	%f1, %r19;
     	setp.eq.ftz.f32 	%p4, %f1, 0f00000000;
     	selp.f64 	%fd78, 0dFFF0000000000000, %fd22, %p4;

     $L__BB4_8:
     	div.rn.f64 	%fd68, %fd1, %fd2;
     	add.f64 	%fd69, %fd78, 0dBFF0000000000000;
     	add.f64 	%fd70, %fd68, %fd69;
     	mul.f64 	%fd71, %fd3, %fd3;
     	div.rn.f64 	%fd72, %fd71, %fd2;
     	add.f64 	%fd73, %fd72, %fd70;
     	mul.f64 	%fd13, %fd73, 0d3FE0000000000000;
     	abs.f64 	%fd74, %fd13;
     	setp.gtu.f64 	%p6, %fd74, 0d7FF0000000000000;
     	mov.f64 	%fd79, 0d0000000000000000;
     	@%p6 bra 	$L__BB4_10;

     	mov.f64 	%fd75, 0d0000000000000000;
     	max.f64 	%fd79, %fd75, %fd13;

     $L__BB4_10:
     	cvta.to.global.u64 	%rd16, %rd6;
     	shl.b64 	%rd17, %rd1, 3;
     	add.s64 	%rd18, %rd16, %rd17;
     	st.global.f64 	[%rd18], %fd79;

     $L__BB4_11:
     	ret;

     }
     	// .globl	accuracy_kernel
     .visible .entry accuracy_kernel(
     	.param .u64 accuracy_kernel_param_0,
     	.param .u64 accuracy_kernel_param_1,
     	.param .u64 accuracy_kernel_param_2,
     	.param .u32 accuracy_kernel_param_3
     )
     {
     	.reg .pred 	%p<6>;
     	.reg .f32 	%f<2>;
     	.reg .b32 	%r<33>;
     	.reg .f64 	%fd<68>;
     	.reg .b64 	%rd<13>;


     	ld.param.u64 	%rd2, [accuracy_kernel_param_0];
     	ld.param.u64 	%rd3, [accuracy_kernel_param_1];
     	ld.param.u64 	%rd4, [accuracy_kernel_param_2];
     	ld.param.u32 	%r12, [accuracy_kernel_param_3];
     	mov.u32 	%r13, %ctaid.x;
     	mov.u32 	%r14, %ntid.x;
     	mov.u32 	%r15, %tid.x;
     	mad.lo.s32 	%r1, %r13, %r14, %r15;
     	setp.ge.s32 	%p1, %r1, %r12;
     	@%p1 bra 	$L__BB5_9;

     	cvta.to.global.u64 	%rd5, %rd3;
     	cvt.s64.s32 	%rd1, %r1;
     	mul.wide.s32 	%rd6, %r1, 8;
     	add.s64 	%rd7, %rd5, %rd6;
     	ld.global.f64 	%fd13, [%rd7];
     	mov.f64 	%fd14, 0d3DDB7CDFD9D7BDBB;
     	max.f64 	%fd1, %fd13, %fd14;
     	cvta.to.global.u64 	%rd8, %rd2;
     	add.s64 	%rd9, %rd8, %rd6;
     	ld.global.f64 	%fd2, [%rd9];
     	mov.f64 	%fd15, 0d401921FB54442D18;
     	div.rn.f64 	%fd65, %fd15, %fd1;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r29}, %fd65;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r30, %temp}, %fd65;
     	}
     	setp.gt.s32 	%p2, %r29, 1048575;
     	mov.u32 	%r31, -1023;
     	@%p2 bra 	$L__BB5_3;

     	mul.f64 	%fd65, %fd65, 0d4350000000000000;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r29}, %fd65;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r30, %temp}, %fd65;
     	}
     	mov.u32 	%r31, -1077;

     $L__BB5_3:
     	add.s32 	%r18, %r29, -1;
     	setp.lt.u32 	%p3, %r18, 2146435071;
     	mul.f64 	%fd16, %fd2, %fd2;
     	mul.f64 	%fd6, %fd1, %fd16;
     	@%p3 bra 	$L__BB5_5;
     	bra.uni 	$L__BB5_4;

     $L__BB5_5:
     	shr.u32 	%r20, %r29, 20;
     	add.s32 	%r32, %r31, %r20;
     	and.b32  	%r21, %r29, -2146435073;
     	or.b32  	%r22, %r21, 1072693248;
     	mov.b64 	%fd66, {%r30, %r22};
     	setp.lt.s32 	%p5, %r22, 1073127583;
     	@%p5 bra 	$L__BB5_7;

     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%r23, %temp}, %fd66;
     	}
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r24}, %fd66;
     	}
     	add.s32 	%r25, %r24, -1048576;
     	mov.b64 	%fd66, {%r23, %r25};
     	add.s32 	%r32, %r32, 1;

     $L__BB5_7:
     	add.f64 	%fd19, %fd66, 0d3FF0000000000000;
     	mov.f64 	%fd20, 0d3FF0000000000000;
     	rcp.approx.ftz.f64 	%fd21, %fd19;
     	neg.f64 	%fd22, %fd19;
     	fma.rn.f64 	%fd23, %fd22, %fd21, %fd20;
     	fma.rn.f64 	%fd24, %fd23, %fd23, %fd23;
     	fma.rn.f64 	%fd25, %fd24, %fd21, %fd21;
     	add.f64 	%fd26, %fd66, 0dBFF0000000000000;
     	mul.f64 	%fd27, %fd26, %fd25;
     	fma.rn.f64 	%fd28, %fd26, %fd25, %fd27;
     	mul.f64 	%fd29, %fd28, %fd28;
     	mov.f64 	%fd30, 0d3ED0EE258B7A8B04;
     	mov.f64 	%fd31, 0d3EB1380B3AE80F1E;
     	fma.rn.f64 	%fd32, %fd31, %fd29, %fd30;
     	mov.f64 	%fd33, 0d3EF3B2669F02676F;
     	fma.rn.f64 	%fd34, %fd32, %fd29, %fd33;
     	mov.f64 	%fd35, 0d3F1745CBA9AB0956;
     	fma.rn.f64 	%fd36, %fd34, %fd29, %fd35;
     	mov.f64 	%fd37, 0d3F3C71C72D1B5154;
     	fma.rn.f64 	%fd38, %fd36, %fd29, %fd37;
     	mov.f64 	%fd39, 0d3F624924923BE72D;
     	fma.rn.f64 	%fd40, %fd38, %fd29, %fd39;
     	mov.f64 	%fd41, 0d3F8999999999A3C4;
     	fma.rn.f64 	%fd42, %fd40, %fd29, %fd41;
     	mov.f64 	%fd43, 0d3FB5555555555554;
     	fma.rn.f64 	%fd44, %fd42, %fd29, %fd43;
     	sub.f64 	%fd45, %fd26, %fd28;
     	add.f64 	%fd46, %fd45, %fd45;
     	neg.f64 	%fd47, %fd28;
     	fma.rn.f64 	%fd48, %fd47, %fd26, %fd46;
     	mul.f64 	%fd49, %fd25, %fd48;
     	mul.f64 	%fd50, %fd29, %fd44;
     	fma.rn.f64 	%fd51, %fd50, %fd28, %fd49;
     	xor.b32  	%r26, %r32, -2147483648;
     	mov.u32 	%r27, -2147483648;
     	mov.u32 	%r28, 1127219200;
     	mov.b64 	%fd52, {%r26, %r28};
     	mov.b64 	%fd53, {%r27, %r28};
     	sub.f64 	%fd54, %fd52, %fd53;
     	mov.f64 	%fd55, 0d3FE62E42FEFA39EF;
     	fma.rn.f64 	%fd56, %fd54, %fd55, %fd28;
     	neg.f64 	%fd57, %fd54;
     	fma.rn.f64 	%fd58, %fd57, %fd55, %fd56;
     	sub.f64 	%fd59, %fd58, %fd28;
     	sub.f64 	%fd60, %fd51, %fd59;
     	mov.f64 	%fd61, 0d3C7ABC9E3B39803F;
     	fma.rn.f64 	%fd62, %fd54, %fd61, %fd60;
     	add.f64 	%fd67, %fd56, %fd62;
     	bra.uni 	$L__BB5_8;

     $L__BB5_4:
     	mov.f64 	%fd17, 0d7FF0000000000000;
     	fma.rn.f64 	%fd18, %fd65, %fd17, %fd17;
     	{
     	.reg .b32 %temp; 
     	mov.b64 	{%temp, %r19}, %fd65;
     	}
     	mov.b32 	%f1, %r19;
     	setp.eq.ftz.f32 	%p4, %f1, 0f00000000;
     	selp.f64 	%fd67, 0dFFF0000000000000, %fd18, %p4;

     $L__BB5_8:
     	add.f64 	%fd63, %fd6, %fd67;
     	mul.f64 	%fd64, %fd63, 0dBFE0000000000000;
     	cvta.to.global.u64 	%rd10, %rd4;
     	shl.b64 	%rd11, %rd1, 3;
     	add.s64 	%rd12, %rd10, %rd11;
     	st.global.f64 	[%rd12], %fd64;

     $L__BB5_9:
     	ret;

     }
     	// .globl	sum_reduction_kernel
     .visible .entry sum_reduction_kernel(
     	.param .u64 sum_reduction_kernel_param_0,
     	.param .u64 sum_reduction_kernel_param_1,
     	.param .u32 sum_reduction_kernel_param_2
     )
     {
     	.reg .pred 	%p<6>;
     	.reg .b32 	%r<15>;
     	.reg .f64 	%fd<10>;
     	.reg .b64 	%rd<7>;


     	ld.param.u64 	%rd1, [sum_reduction_kernel_param_0];
     	ld.param.u64 	%rd2, [sum_reduction_kernel_param_1];
     	ld.param.u32 	%r8, [sum_reduction_kernel_param_2];
     	mov.u32 	%r1, %ntid.x;
     	mov.u32 	%r9, %ctaid.x;
     	mov.u32 	%r2, %tid.x;
     	mad.lo.s32 	%r3, %r9, %r1, %r2;
     	setp.ge.u32 	%p1, %r3, %r8;
     	mov.f64 	%fd9, 0d0000000000000000;
     	@%p1 bra 	$L__BB6_2;

     	cvta.to.global.u64 	%rd3, %rd1;
     	mul.wide.u32 	%rd4, %r3, 8;
     	add.s64 	%rd5, %rd3, %rd4;
     	ld.global.f64 	%fd9, [%rd5];

     $L__BB6_2:
     	shl.b32 	%r10, %r2, 3;
     	mov.u32 	%r11, sdata;
     	add.s32 	%r4, %r11, %r10;
     	st.shared.f64 	[%r4], %fd9;
     	bar.sync 	0;
     	shr.u32 	%r14, %r1, 1;
     	setp.eq.s32 	%p2, %r14, 0;
     	@%p2 bra 	$L__BB6_6;

     $L__BB6_3:
     	setp.ge.u32 	%p3, %r2, %r14;
     	@%p3 bra 	$L__BB6_5;

     	shl.b32 	%r12, %r14, 3;
     	add.s32 	%r13, %r4, %r12;
     	ld.shared.f64 	%fd4, [%r4];
     	ld.shared.f64 	%fd5, [%r13];
     	add.f64 	%fd6, %fd5, %fd4;
     	st.shared.f64 	[%r4], %fd6;

     $L__BB6_5:
     	bar.sync 	0;
     	shr.u32 	%r14, %r14, 1;
     	setp.ne.s32 	%p4, %r14, 0;
     	@%p4 bra 	$L__BB6_3;

     $L__BB6_6:
     	setp.ne.s32 	%p5, %r2, 0;
     	@%p5 bra 	$L__BB6_8;

     	ld.shared.f64 	%fd7, [sdata];
     	cvta.to.global.u64 	%rd6, %rd2;
     	atom.global.add.f64 	%fd8, [%rd6], %fd7;

     $L__BB6_8:
     	ret;

     }
     	// .globl	axpby_kernel
     .visible .entry axpby_kernel(
     	.param .u64 axpby_kernel_param_0,
     	.param .u64 axpby_kernel_param_1,
     	.param .f64 axpby_kernel_param_2,
     	.param .f64 axpby_kernel_param_3,
     	.param .u32 axpby_kernel_param_4
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .b32 	%r<6>;
     	.reg .f64 	%fd<7>;
     	.reg .b64 	%rd<8>;


     	ld.param.u64 	%rd1, [axpby_kernel_param_0];
     	ld.param.u64 	%rd2, [axpby_kernel_param_1];
     	ld.param.f64 	%fd1, [axpby_kernel_param_2];
     	ld.param.f64 	%fd2, [axpby_kernel_param_3];
     	ld.param.u32 	%r2, [axpby_kernel_param_4];
     	mov.u32 	%r3, %ctaid.x;
     	mov.u32 	%r4, %ntid.x;
     	mov.u32 	%r5, %tid.x;
     	mad.lo.s32 	%r1, %r3, %r4, %r5;
     	setp.ge.s32 	%p1, %r1, %r2;
     	@%p1 bra 	$L__BB7_2;

     	cvta.to.global.u64 	%rd3, %rd2;
     	cvta.to.global.u64 	%rd4, %rd1;
     	mul.wide.s32 	%rd5, %r1, 8;
     	add.s64 	%rd6, %rd4, %rd5;
     	ld.global.f64 	%fd3, [%rd6];
     	add.s64 	%rd7, %rd3, %rd5;
     	ld.global.f64 	%fd4, [%rd7];
     	mul.f64 	%fd5, %fd4, %fd2;
     	fma.rn.f64 	%fd6, %fd3, %fd1, %fd5;
     	st.global.f64 	[%rd7], %fd6;

     $L__BB7_2:
     	ret;

     }
     	// .globl	init_amplitudes_kernel
     .visible .entry init_amplitudes_kernel(
     	.param .u64 init_amplitudes_kernel_param_0,
     	.param .u32 init_amplitudes_kernel_param_1,
     	.param .u32 init_amplitudes_kernel_param_2
     )
     {
     	.reg .pred 	%p<2>;
     	.reg .b32 	%r<8>;
     	.reg .f64 	%fd<4>;
     	.reg .b64 	%rd<5>;


     	ld.param.u64 	%rd1, [init_amplitudes_kernel_param_0];
     	ld.param.u32 	%r3, [init_amplitudes_kernel_param_1];
     	ld.param.u32 	%r2, [init_amplitudes_kernel_param_2];
     	mov.u32 	%r4, %ctaid.x;
     	mov.u32 	%r5, %ntid.x;
     	mov.u32 	%r6, %tid.x;
     	mad.lo.s32 	%r1, %r4, %r5, %r6;
     	mul.lo.s32 	%r7, %r2, %r3;
     	setp.ge.s32 	%p1, %r1, %r7;
     	@%p1 bra 	$L__BB8_2;

     	cvta.to.global.u64 	%rd2, %rd1;
     	cvt.rn.f64.s32 	%fd1, %r2;
     	sqrt.rn.f64 	%fd2, %fd1;
     	rcp.rn.f64 	%fd3, %fd2;
     	mul.wide.s32 	%rd3, %r1, 8;
     	add.s64 	%rd4, %rd2, %rd3;
     	st.global.f64 	[%rd4], %fd3;

     $L__BB8_2:
     	ret;

     }
     	// .globl	compute_vertex_uncertainty
     .visible .entry compute_vertex_uncertainty(
     	.param .u64 compute_vertex_uncertainty_param_0,
     	.param .u64 compute_vertex_uncertainty_param_1,
     	.param .u64 compute_vertex_uncertainty_param_2,
     	.param .u64 compute_vertex_uncertainty_param_3,
     	.param .u64 compute_vertex_uncertainty_param_4,
     	.param .u32 compute_vertex_uncertainty_param_5
     )
     {
     	.reg .pred 	%p<10>;
     	.reg .b32 	%r<40>;
     	.reg .f64 	%fd<20>;
     	.reg .b64 	%rd<29>;


     	ld.param.u64 	%rd7, [compute_vertex_uncertainty_param_0];
     	ld.param.u64 	%rd8, [compute_vertex_uncertainty_param_1];
     	ld.param.u64 	%rd4, [compute_vertex_uncertainty_param_2];
     	ld.param.u64 	%rd5, [compute_vertex_uncertainty_param_3];
     	ld.param.u64 	%rd6, [compute_vertex_uncertainty_param_4];
     	ld.param.u32 	%r21, [compute_vertex_uncertainty_param_5];
     	cvta.to.global.u64 	%rd1, %rd8;
     	cvta.to.global.u64 	%rd2, %rd7;
     	mov.u32 	%r22, %ntid.x;
     	mov.u32 	%r23, %ctaid.x;
     	mov.u32 	%r24, %tid.x;
     	mad.lo.s32 	%r1, %r23, %r22, %r24;
     	setp.ge.s32 	%p1, %r1, %r21;
     	mov.f64 	%fd19, 0d0000000000000000;
     	@%p1 bra 	$L__BB9_14;

     	cvta.to.global.u64 	%rd9, %rd4;
     	cvt.s64.s32 	%rd3, %r1;
     	mul.wide.s32 	%rd10, %r1, 4;
     	add.s64 	%rd11, %rd2, %rd10;
     	ld.global.u32 	%r2, [%rd11+4];
     	ld.global.u32 	%r3, [%rd11];
     	sub.s32 	%r4, %r2, %r3;
     	mul.wide.s32 	%rd12, %r1, 8;
     	add.s64 	%rd13, %rd9, %rd12;
     	ld.global.f64 	%fd1, [%rd13];
     	cvta.to.global.u64 	%rd14, %rd5;
     	add.s64 	%rd15, %rd14, %rd12;
     	ld.global.f64 	%fd2, [%rd15];
     	setp.lt.u32 	%p2, %r4, 2;
     	@%p2 bra 	$L__BB9_13;

     	setp.le.u32 	%p3, %r2, %r3;
     	mov.u32 	%r35, 0;
     	@%p3 bra 	$L__BB9_11;

     	mov.u32 	%r35, 0;
     	mov.u32 	%r32, %r3;

     $L__BB9_4:
     	mul.wide.u32 	%rd16, %r32, 4;
     	add.s64 	%rd17, %rd1, %rd16;
     	ld.global.u32 	%r27, [%rd17];
     	mul.wide.u32 	%rd18, %r27, 4;
     	add.s64 	%rd19, %rd2, %rd18;
     	add.s32 	%r28, %r27, 1;
     	mul.wide.u32 	%rd20, %r28, 4;
     	add.s64 	%rd21, %rd2, %rd20;
     	ld.global.u32 	%r7, [%rd21];
     	ld.global.u32 	%r34, [%rd19];
     	setp.ge.u32 	%p4, %r34, %r7;
     	@%p4 bra 	$L__BB9_10;

     $L__BB9_5:
     	mul.wide.u32 	%rd22, %r34, 4;
     	add.s64 	%rd23, %rd1, %rd22;
     	ld.global.u32 	%r11, [%rd23];
     	mov.u32 	%r36, %r3;

     $L__BB9_6:
     	mul.wide.u32 	%rd24, %r36, 4;
     	add.s64 	%rd25, %rd1, %rd24;
     	ld.global.u32 	%r29, [%rd25];
     	setp.eq.s32 	%p5, %r29, %r11;
     	add.s32 	%r36, %r36, 1;
     	@%p5 bra 	$L__BB9_8;

     	setp.lt.u32 	%p6, %r36, %r2;
     	@%p6 bra 	$L__BB9_6;
     	bra.uni 	$L__BB9_9;

     $L__BB9_8:
     	add.s32 	%r35, %r35, 1;

     $L__BB9_9:
     	add.s32 	%r34, %r34, 1;
     	setp.lt.u32 	%p7, %r34, %r7;
     	@%p7 bra 	$L__BB9_5;

     $L__BB9_10:
     	add.s32 	%r32, %r32, 1;
     	setp.lt.u32 	%p8, %r32, %r2;
     	@%p8 bra 	$L__BB9_4;

     $L__BB9_11:
     	add.s32 	%r30, %r4, -1;
     	mul.lo.s32 	%r31, %r30, %r4;
     	shr.u32 	%r20, %r31, 1;
     	setp.eq.s32 	%p9, %r20, 0;
     	@%p9 bra 	$L__BB9_13;

     	cvt.rn.f64.u32 	%fd7, %r35;
     	cvt.rn.f64.u32 	%fd8, %r20;
     	div.rn.f64 	%fd9, %fd7, %fd8;
     	mul.f64 	%fd19, %fd9, 0d3FE0000000000000;

     $L__BB9_13:
     	add.f64 	%fd10, %fd2, 0d3FF0000000000000;
     	mov.f64 	%fd11, 0d3FF0000000000000;
     	mov.f64 	%fd12, 0d3DDB7CDFD9D7BDBB;
     	max.f64 	%fd13, %fd1, %fd12;
     	rcp.rn.f64 	%fd14, %fd13;
     	mul.f64 	%fd15, %fd14, %fd10;
     	sub.f64 	%fd16, %fd11, %fd19;
     	mul.f64 	%fd17, %fd15, %fd16;
     	cvta.to.global.u64 	%rd26, %rd6;
     	shl.b64 	%rd27, %rd3, 3;
     	add.s64 	%rd28, %rd26, %rd27;
     	st.global.f64 	[%rd28], %fd17;

     $L__BB9_14:
     	ret;

     }
     /**
      * PRISM DR-WHCR-AI-Q-PT Ultra Fused Kernel
      *
      * Ultra-optimized GPU kernel combining 8 advanced optimization techniques:
      * 1. W-Cycle Multigrid (4-level hierarchical coarsening)
      * 2. Dendritic Reservoir Computing (8-branch neuromorphic processing)
      * 3. Quantum Tunneling (6-state superposition)
      * 4. TPTP Persistent Homology (topological phase transition detection)
      * 5. Active Inference (belief-driven planning)
      * 6. Parallel Tempering (12 temperature replicas)
      * 7. WHCR Conflict Repair (wavelet-hierarchical optimization)
      * 8. Wavelet-guided prioritization
      *
      * Copyright (c) 2024 PRISM Research Team | Delfictus I/O Inc.
      * Los Angeles, CA 90013
      * Contact: IS@Delfictus.com
      * All Rights Reserved.
      */

     #include <cuda_runtime.h>
     #include <cooperative_groups.h>
     #include <curand_kernel.h>
     #include <math.h>

     namespace cg = cooperative_groups;

     // 
     // CONSTANTS
     // 

     #define BLOCK_SIZE 256
     #define MAX_VERTICES_PER_BLOCK 256  // Reduced to fit 100KB shared memory
     #define MAX_COLORS 64
     #define NUM_BRANCHES 8
     #define NUM_LEVELS 4
     #define NUM_REPLICAS 12
     #define NUM_QUANTUM_STATES 6
     #define MAX_NEIGHBORS 128
     #define WARP_SIZE 32

     // Precision constants
     #define EPSILON 1e-8f
     #define PI 3.14159265358979323846f

     // 
     // CONFIGURATION STRUCTURES
     // 

     /**
      * RuntimeConfig - FFI-compatible configuration from Rust
      * Must match crates/prism-core/src/runtime_config.rs exactly
      */
     struct RuntimeConfig {
         // WHCR Parameters
         float stress_weight;
         float persistence_weight;
         float belief_weight;
         float hotspot_multiplier;

         // Dendritic Reservoir (8-branch)
         float tau_decay[8];
         float branch_weights[8];
         float reservoir_leak_rate;
         float spectral_radius;
         float input_scaling;
         float reservoir_sparsity;

         // W-Cycle Multigrid
         int num_levels;
         float coarsening_ratio;
         float restriction_weight;
         float prolongation_weight;
         int pre_smooth_iterations;
         int post_smooth_iterations;

         // Quantum Tunneling
         float tunneling_prob_base;
         float tunneling_prob_boost;
         float chemical_potential;
         float transverse_field;
         float interference_decay;
         int num_quantum_states;

         // Parallel Tempering
         float temperatures[8];
         int num_replicas;
         int swap_interval;
         float swap_probability;

         // TPTP (Topological Phase Transition Predictor)
         float betti_0_threshold;
         float betti_1_threshold;
         float betti_2_threshold;
         float persistence_threshold;
         int stability_window;
         float transition_sensitivity;

         // Active Inference
         float free_energy_threshold;
         float belief_update_rate;
         float precision_weight;
         float policy_temperature;

         // Meta/Control
         int iteration;
         int phase_id;
         float global_temperature;
         float learning_rate;
         float exploration_rate;

         // Flags
         int flags;

         // Padding
         float _padding;
     };

     /**
      * KernelTelemetry - Output metrics from kernel
      * Must match crates/prism-core/src/runtime_config.rs exactly
      */
     struct KernelTelemetry {
         int conflicts;
         int colors_used;
         int moves_applied;
         int tunneling_events;
         int phase_transitions;
         float betti_numbers[3];
         float reservoir_activity;
         float free_energy;
         int best_replica;
         int iteration_time_us;
         float _padding[4];
     };

     // 
     // SHARED MEMORY STATE (~98KB)
     // 

     /**
      * Dendritic State - 8-branch neuromorphic processing
      */
     struct DendriticState {
         float activation[8];     // Per-branch activation
         float calcium;           // Long-term potentiation accumulator
         float threshold;         // Adaptive firing threshold
         float refractory;        // Refractory period counter
     };

     /**
      * Quantum Vertex - 6-state superposition
      */
     struct QuantumVertex {
         float amplitude_real[6]; // Real part of quantum amplitude
         float amplitude_imag[6]; // Imaginary part of quantum amplitude
         int color_idx[6];        // Color for each superposition state
         float tunneling_prob;    // Current tunneling probability
         float phase;             // Global quantum phase
     };

     /**
      * Tempering Replica - Parallel tempering state
      */
     struct TemperingReplica {
         int coloring[64];        // Replica's coloring (subset of vertices)
         float energy;            // Current energy
         int conflicts;           // Number of conflicts
     };

     /**
      * Persistent Homology State - TPTP topological tracking
      */
     struct PersistentHomologyState {
         float betti[3];                 // Betti numbers (0, 1, 2)
         float max_persistence;          // Maximum persistence value
         float stability_score;          // Stability measure
         int transition_detected;        // Phase boundary flag
         float betti_1_derivative;       // Rate of change of 1
         float persistence_diagram[64];  // Birth-death pairs (32 intervals)
     };

     /**
      * Ultra Shared Memory State
      * Total: ~95KB (optimized to fit in RTX 3060's 100KB shared memory)
      * Reduced from 153KB by limiting per-block vertices to 256
      */
     struct UltraSharedState {
         // 
         // W-CYCLE MULTIGRID HIERARCHY (4 levels) - OPTIMIZED
         // 
         // Level 0 (Fine): 256 vertices max per block
         int coloring_L0[256];               // 1KB
         float conflict_signal_L0[256];      // 1KB

         // Level 1: 64 vertices
         int coloring_L1[64];                // 256B
         float conflict_signal_L1[64];       // 256B
         int projection_L0_to_L1[256];       // 1KB (finecoarse mapping)

         // Level 2: 16 vertices
         int coloring_L2[16];                // 64B
         float conflict_signal_L2[16];       // 64B
         int projection_L1_to_L2[64];        // 256B

         // Level 3 (Coarsest): 4 vertices
         int coloring_L3[4];                 // 16B
         float conflict_signal_L3[4];        // 16B
         int projection_L2_to_L3[16];        // 64B

         // Wavelet coefficients (4 levels) - REDUCED FOR 100KB LIMIT
         float wavelet_approx[2][256];       // 2KB (approximation, 2 levels)
         float wavelet_detail[2][256];       // 2KB (detail, 2 levels)

         // 
         // DENDRITIC RESERVOIR (8-branch) - OPTIMIZED FOR 100KB LIMIT
         // 
         DendriticState dendrite[256];       // 12KB (256 vertices)
         float soma_potential[256];          // 1KB
         float spike_history[256];           // 1KB
         float reservoir_state[1024];        // 4KB (echo state, reduced)

         // 
         // QUANTUM TUNNELING STATE - OPTIMIZED FOR 100KB LIMIT
         // 
         QuantumVertex quantum[256];         // 20KB (256 vertices)

         // 
         // PARALLEL TEMPERING (12 replicas)
         // 
         TemperingReplica replica[12];       // 3.6KB
         float temperatures[16];             // 64B (temperature ladder)

         // 
         // TPTP: PERSISTENT HOMOLOGY STATE
         // 
         PersistentHomologyState tda;        // ~0.3KB

         // 
         // ACTIVE INFERENCE - REDUCED FOR 100KB LIMIT
         // 
         float belief_distribution[256][12]; // 12KB (beliefs over 12 colors, 256 vertices)
         float expected_free_energy[256];    // 1KB
         float precision_weights[256];       // 1KB

         // 
         // WORK BUFFERS - OPTIMIZED FOR 100KB LIMIT
         // 
         int conflict_vertices[256];         // 1KB (vertices with conflicts)
         int num_conflict_vertices;          // 4B
         float move_deltas[256];             // 1KB (best move delta per vertex)
         int best_colors[256];               // 1KB (best new color)
         int locks[256];                     // 1KB (vertex locks for atomic updates)

         // Total: ~95KB (optimized from 153KB)
     };

     // 
     // FEATURE FLAG HELPERS (inline device functions)
     // 

     #define FLAG_QUANTUM_ENABLED (1 << 0)
     #define FLAG_TPTP_ENABLED (1 << 1)
     #define FLAG_DENDRITIC_ENABLED (1 << 2)
     #define FLAG_PARALLEL_TEMPERING_ENABLED (1 << 3)
     #define FLAG_ACTIVE_INFERENCE_ENABLED (1 << 4)
     #define FLAG_MULTIGRID_ENABLED (1 << 5)

     __device__ __forceinline__ bool quantum_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_QUANTUM_ENABLED) != 0;
     }

     __device__ __forceinline__ bool tptp_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_TPTP_ENABLED) != 0;
     }

     __device__ __forceinline__ bool dendritic_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_DENDRITIC_ENABLED) != 0;
     }

     __device__ __forceinline__ bool tempering_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_PARALLEL_TEMPERING_ENABLED) != 0;
     }

     __device__ __forceinline__ bool active_inference_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_ACTIVE_INFERENCE_ENABLED) != 0;
     }

     __device__ __forceinline__ bool multigrid_enabled(const RuntimeConfig* cfg) {
         return (cfg->flags & FLAG_MULTIGRID_ENABLED) != 0;
     }

     // 
     // FORWARD DECLARATIONS
     // 

     __device__ void restrict_to_coarse(UltraSharedState* state, int level, const RuntimeConfig* cfg);
     __device__ void prolongate_to_fine(UltraSharedState* state, int level, const RuntimeConfig* cfg, curandState* rng);
     __device__ void smooth_iteration(UltraSharedState* state, int level, const RuntimeConfig* cfg, cg::thread_block block);
     __device__ void dendritic_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
     __device__ void quantum_evolve(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
     __device__ void tptp_update(UltraSharedState* state, const RuntimeConfig* cfg);
     __device__ bool should_tunnel(UltraSharedState* state, int vertex, const RuntimeConfig* cfg, curandState* rng);
     __device__ void active_inference_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg);
     __device__ void tempering_step(UltraSharedState* state, int replica, const RuntimeConfig* cfg, curandState* rng);
     __device__ void replica_exchange(UltraSharedState* state, const RuntimeConfig* cfg, curandState* rng);

     // 
     // MAIN ULTRA KERNEL
     // 

     /**
      * DR-WHCR-AI-Q-PT-TDA Ultra Fused Kernel
      *
      * Single kernel that performs complete optimization iteration combining all 8 components.
      *
      * @param graph_row_ptr CSR row pointers [num_vertices+1]
      * @param graph_col_idx CSR column indices [num_edges]
      * @param coloring Current vertex coloring [num_vertices] (modified in-place)
      * @param config RuntimeConfig struct with all 50+ parameters
      * @param telemetry Output telemetry struct
      * @param num_vertices Number of vertices
      * @param num_edges Number of edges
      * @param seed Random seed for this iteration
      */
     extern "C" __global__ void dr_whcr_ultra_kernel(
         const int* __restrict__ graph_row_ptr,
         const int* __restrict__ graph_col_idx,
         int* __restrict__ coloring,
         const RuntimeConfig* __restrict__ config,
         KernelTelemetry* __restrict__ telemetry,
         int num_vertices,
         int num_edges,
         unsigned long long seed
     ) {
         // Shared memory allocation
         __shared__ UltraSharedState state;

         // Cooperative groups for synchronization
         cg::thread_block block = cg::this_thread_block();

         int tid = threadIdx.x;
         int bid = blockIdx.x;
         int gid = blockIdx.x * blockDim.x + threadIdx.x;

         // Initialize RNG
         curandState rng;
         curand_init(seed, gid, 0, &rng);

         // 
         // PHASE 1: LOAD DATA INTO SHARED MEMORY
         // 

         // Calculate vertex range for this block
         int vertices_per_block = (num_vertices + gridDim.x - 1) / gridDim.x;
         int block_start = bid * vertices_per_block;
         int block_end = min(block_start + vertices_per_block, num_vertices);
         int block_size = block_end - block_start;

         // Initialize locks
         for (int i = tid; i < MAX_VERTICES_PER_BLOCK; i += BLOCK_SIZE) {
             state.locks[i] = 0;
         }

         // Initialize projection mappings (simple 4:1 ratio)
         for (int i = tid; i < 512; i += BLOCK_SIZE) {
             state.projection_L0_to_L1[i] = i / 4;
         }
         for (int i = tid; i < 128; i += BLOCK_SIZE) {
             state.projection_L1_to_L2[i] = i / 4;
         }
         for (int i = tid; i < 32; i += BLOCK_SIZE) {
             state.projection_L2_to_L3[i] = i / 4;
         }

         // Initialize temperatures
         for (int i = tid; i < 16; i += BLOCK_SIZE) {
             if (i < 8) {
                 state.temperatures[i] = config->temperatures[i];
             } else {
                 state.temperatures[i] = 1.0f;
             }
         }

         // Initialize quantum states
         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             QuantumVertex* q = &state.quantum[i];

             // Equal superposition initialization
             float amp = 1.0f / sqrtf((float)NUM_QUANTUM_STATES);
             for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
                 q->amplitude_real[s] = amp;
                 q->amplitude_imag[s] = 0.0f;
                 q->color_idx[s] = s % MAX_COLORS;
             }
             q->tunneling_prob = config->tunneling_prob_base;
             q->phase = 0.0f;
         }

         // Initialize dendrites
         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             DendriticState* d = &state.dendrite[i];
             for (int b = 0; b < NUM_BRANCHES; b++) {
                 d->activation[b] = 0.0f;
             }
             d->calcium = 0.0f;
             d->threshold = 0.5f;
             d->refractory = 0.0f;

             state.soma_potential[i] = 0.0f;
             state.spike_history[i] = 0.0f;
         }

         // Initialize belief distributions (uniform prior)
         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             for (int c = 0; c < 16; c++) {
                 state.belief_distribution[i][c] = 1.0f / 16.0f;
             }
             state.expected_free_energy[i] = 0.0f;
             state.precision_weights[i] = 1.0f;
         }

         // Initialize TPTP state
         if (tid == 0) {
             state.tda.betti[0] = 0.0f;
             state.tda.betti[1] = 0.0f;
             state.tda.betti[2] = 0.0f;
             state.tda.max_persistence = 0.0f;
             state.tda.stability_score = 0.0f;
             state.tda.transition_detected = 0;
             state.tda.betti_1_derivative = 0.0f;
             for (int i = 0; i < 64; i++) {
                 state.tda.persistence_diagram[i] = 0.0f;
             }
         }

         block.sync();

         // Load coloring into shared memory
         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             int v = block_start + i;
             if (v < num_vertices && i < MAX_VERTICES_PER_BLOCK) {
                 state.coloring_L0[i] = coloring[v];
             }
         }
         block.sync();

         // 
         // PHASE 2: COUNT INITIAL CONFLICTS
         // 

         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             int v = block_start + i;
             if (v >= num_vertices) continue;

             int my_color = state.coloring_L0[i];
             int start = graph_row_ptr[v];
             int end = graph_row_ptr[v + 1];

             float conflict_count = 0.0f;
             for (int e = start; e < end; e++) {
                 int neighbor = graph_col_idx[e];
                 // Check if neighbor is in this block
                 if (neighbor >= block_start && neighbor < block_end) {
                     int local_idx = neighbor - block_start;
                     if (local_idx < MAX_VERTICES_PER_BLOCK && state.coloring_L0[local_idx] == my_color) {
                         conflict_count += 1.0f;
                     }
                 } else {
                     // Global memory access for out-of-block neighbors
                     if (coloring[neighbor] == my_color) {
                         conflict_count += 1.0f;
                     }
                 }
             }
             state.conflict_signal_L0[i] = conflict_count;
         }
         block.sync();

         // 
         // PHASE 3: W-CYCLE MULTIGRID
         // 

         if (multigrid_enabled(config)) {
             // Pre-smoothing at fine level
             for (int s = 0; s < config->pre_smooth_iterations; s++) {
                 smooth_iteration(&state, 0, config, block);
                 block.sync();
             }

             // Restriction: L0  L1  L2  L3
             for (int level = 0; level < config->num_levels - 1; level++) {
                 restrict_to_coarse(&state, level, config);
                 block.sync();
             }

             // Solve at coarsest level (L3) - direct greedy coloring
             if (tid < 8) {
                 int v = tid;
                 int used_colors = 0;
                 for (int c = 0; c < MAX_COLORS; c++) {
                     bool can_use = true;
                     // Simplified connectivity check at coarse level
                     for (int other = 0; other < 8; other++) {
                         if (other != v && state.coloring_L3[other] == c) {
                             // Assume all coarse vertices are connected (worst case)
                             if (state.conflict_signal_L3[v] > 0.0f) {
                                 can_use = false;
                                 break;
                             }
                         }
                     }
                     if (can_use) {
                         state.coloring_L3[v] = c;
                         break;
                     }
                 }
             }
             block.sync();

             // Prolongation: L3  L2  L1  L0
             for (int level = config->num_levels - 2; level >= 0; level--) {
                 prolongate_to_fine(&state, level, config, &rng);
                 block.sync();

                 // Post-smoothing
                 for (int s = 0; s < config->post_smooth_iterations; s++) {
                     smooth_iteration(&state, level, config, block);
                     block.sync();
                 }
             }
         }

         // 
         // PHASE 4: DENDRITIC RESERVOIR UPDATE
         // 

         if (dendritic_enabled(config)) {
             for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
                 dendritic_update(&state, i, config);
             }
             block.sync();

             // Compute reservoir output priorities
             for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
                 float priority = 0.0f;

                 // Proximal branch drives immediate priority
                 priority += state.dendrite[i].activation[0] * 2.0f;

                 // Distal branches indicate structural issues
                 for (int b = 1; b < NUM_BRANCHES; b++) {
                     priority += state.dendrite[i].activation[b] * config->branch_weights[b];
                 }

                 // Calcium indicates long-term problematic vertex
                 priority += state.dendrite[i].calcium * 3.0f;

                 // Soma potential indicates accumulated pressure
                 priority += state.soma_potential[i] * 0.5f;

                 state.move_deltas[i] = priority; // Temporarily store priority here
             }
             block.sync();
         }

         // 
         // PHASE 5: TPTP PERSISTENT HOMOLOGY UPDATE
         // 

         if (tptp_enabled(config)) {
             // Thread 0 computes global homology (simplified)
             if (tid == 0) {
                 tptp_update(&state, config);
             }
             block.sync();
         }

         // 
         // PHASE 6: QUANTUM TUNNELING
         // 

         if (quantum_enabled(config)) {
             for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
                 // Evolve quantum state
                 quantum_evolve(&state, i, config);

                 // Check for tunneling
                 if (should_tunnel(&state, i, config, &rng)) {
                     // Tunnel to new color based on quantum state
                     float max_prob = 0.0f;
                     int best_state = 0;
                     for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
                         float prob = state.quantum[i].amplitude_real[s] * state.quantum[i].amplitude_real[s] +
                                     state.quantum[i].amplitude_imag[s] * state.quantum[i].amplitude_imag[s];
                         if (prob > max_prob) {
                             max_prob = prob;
                             best_state = s;
                         }
                     }
                     state.coloring_L0[i] = state.quantum[i].color_idx[best_state];

                     // Track tunneling event (atomic for global telemetry)
                     if (gid == 0) {
                         atomicAdd(&telemetry->tunneling_events, 1);
                     }
                 }
             }
             block.sync();
         }

         // 
         // PHASE 7: ACTIVE INFERENCE BELIEF UPDATE
         // 

         if (active_inference_enabled(config)) {
             for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
                 active_inference_update(&state, i, config);
             }
             block.sync();
         }

         // 
         // PHASE 8: PARALLEL TEMPERING
         // 

         if (tempering_enabled(config)) {
             // Each warp handles one replica
             int warp_id = tid / 32;
             if (warp_id < config->num_replicas && warp_id < NUM_REPLICAS) {
                 tempering_step(&state, warp_id, config, &rng);
             }
             block.sync();

             // Replica exchange (thread 0 coordinates)
             if (tid == 0 && config->iteration % config->swap_interval == 0) {
                 replica_exchange(&state, config, &rng);
             }
             block.sync();
         }

         // 
         // PHASE 9: WHCR MOVE EVALUATION AND APPLICATION
         // 

         // Identify conflict vertices
         if (tid == 0) {
             state.num_conflict_vertices = 0;
         }
         block.sync();

         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             if (state.conflict_signal_L0[i] > 0.5f) {
                 int idx = atomicAdd(&state.num_conflict_vertices, 1);
                 if (idx < MAX_VERTICES_PER_BLOCK) {
                     state.conflict_vertices[idx] = i;
                 }
             }
         }
         block.sync();

         // Evaluate best moves for conflict vertices
         int num_cv = min(state.num_conflict_vertices, MAX_VERTICES_PER_BLOCK);
         for (int cv_idx = tid; cv_idx < num_cv; cv_idx += BLOCK_SIZE) {
             int i = state.conflict_vertices[cv_idx];
             int v = block_start + i;
             if (v >= num_vertices) continue;

             int current_color = state.coloring_L0[i];

             // Count neighbor colors
             int neighbor_colors[MAX_COLORS];
             for (int c = 0; c < MAX_COLORS; c++) neighbor_colors[c] = 0;

             int start = graph_row_ptr[v];
             int end = graph_row_ptr[v + 1];

             for (int e = start; e < end; e++) {
                 int neighbor = graph_col_idx[e];
                 int n_color;
                 if (neighbor >= block_start && neighbor < block_end) {
                     int local_idx = neighbor - block_start;
                     if (local_idx < MAX_VERTICES_PER_BLOCK) {
                         n_color = state.coloring_L0[local_idx];
                     } else {
                         n_color = coloring[neighbor];
                     }
                 } else {
                     n_color = coloring[neighbor];
                 }
                 if (n_color >= 0 && n_color < MAX_COLORS) {
                     neighbor_colors[n_color]++;
                 }
             }

             // Find best color
             int current_conf = neighbor_colors[current_color];
             float best_delta = 0.0f;
             int best_color = current_color;

             for (int c = 0; c < MAX_COLORS; c++) {
                 if (c == current_color) continue;

                 float delta = (float)(neighbor_colors[c] - current_conf);

                 // Chemical potential penalty
                 delta += config->chemical_potential * ((float)c - (float)current_color) / (float)MAX_COLORS;

                 // Belief guidance from active inference
                 if (active_inference_enabled(config) && c < 16) {
                     float belief_diff = state.belief_distribution[i][c] -
                                        state.belief_distribution[i][min(current_color, 15)];
                     delta -= config->belief_weight * belief_diff;
                 }

                 // Reservoir priority modulation
                 if (dendritic_enabled(config)) {
                     delta -= state.move_deltas[i] * 0.1f;
                 }

                 if (delta < best_delta) {
                     best_delta = delta;
                     best_color = c;
                 }
             }

             state.best_colors[i] = best_color;
             state.move_deltas[i] = best_delta;
         }
         block.sync();

         // Apply moves with locking
         for (int cv_idx = tid; cv_idx < num_cv; cv_idx += BLOCK_SIZE) {
             int i = state.conflict_vertices[cv_idx];
             int new_color = state.best_colors[i];
             float delta = state.move_deltas[i];

             if (new_color == state.coloring_L0[i]) continue;
             if (delta >= -0.001f) continue; // Only apply improving moves

             // Try to acquire lock
             if (atomicCAS(&state.locks[i], 0, 1) == 0) {
                 state.coloring_L0[i] = new_color;
                 atomicExch(&state.locks[i], 0);

                 // Track move application
                 if (gid == 0) {
                     atomicAdd(&telemetry->moves_applied, 1);
                 }
             }
         }
         block.sync();

         // 
         // PHASE 10: WRITE BACK TO GLOBAL MEMORY
         // 

         for (int i = tid; i < min(block_size, MAX_VERTICES_PER_BLOCK); i += BLOCK_SIZE) {
             int v = block_start + i;
             if (v < num_vertices) {
                 coloring[v] = state.coloring_L0[i];
             }
         }

         // Write telemetry (thread 0 only)
         if (gid == 0) {
             // Count total conflicts
             int total_conflicts = 0;
             int max_color = 0;
             for (int v = 0; v < num_vertices; v++) {
                 int c = coloring[v];
                 if (c > max_color) max_color = c;

                 int start = graph_row_ptr[v];
                 int end = graph_row_ptr[v + 1];
                 for (int e = start; e < end; e++) {
                     if (coloring[graph_col_idx[e]] == c) {
                         total_conflicts++;
                     }
                 }
             }

             telemetry->conflicts = total_conflicts / 2;
             telemetry->colors_used = max_color + 1;
             telemetry->betti_numbers[0] = state.tda.betti[0];
             telemetry->betti_numbers[1] = state.tda.betti[1];
             telemetry->betti_numbers[2] = state.tda.betti[2];
             telemetry->phase_transitions = state.tda.transition_detected;

             // Compute average reservoir activity
             float total_activity = 0.0f;
             int active_count = 0;
             for (int i = 0; i < min(block_size, MAX_VERTICES_PER_BLOCK); i++) {
                 if (state.spike_history[i] > 0.1f) {
                     total_activity += state.spike_history[i];
                     active_count++;
                 }
             }
             telemetry->reservoir_activity = (active_count > 0) ? (total_activity / active_count) : 0.0f;

             // Compute average free energy
             float total_fe = 0.0f;
             for (int i = 0; i < min(block_size, MAX_VERTICES_PER_BLOCK); i++) {
                 total_fe += state.expected_free_energy[i];
             }
             telemetry->free_energy = total_fe / fmaxf(1.0f, (float)min(block_size, MAX_VERTICES_PER_BLOCK));
         }
     }

     // 
     // DEVICE FUNCTION IMPLEMENTATIONS
     // 

     /**
      * Multigrid Restriction: Project fine level to coarse level
      */
     __device__ void restrict_to_coarse(UltraSharedState* state, int level, const RuntimeConfig* cfg) {
         int tid = threadIdx.x;

         // Get source and destination arrays based on level
         int* src_coloring;
         float* src_signal;
         int* dst_coloring;
         float* dst_signal;
         int* projection;
         int src_size, dst_size;

         switch (level) {
             case 0:
                 src_coloring = state->coloring_L0;
                 src_signal = state->conflict_signal_L0;
                 dst_coloring = state->coloring_L1;
                 dst_signal = state->conflict_signal_L1;
                 projection = state->projection_L0_to_L1;
                 src_size = 512; dst_size = 128;
                 break;
             case 1:
                 src_coloring = state->coloring_L1;
                 src_signal = state->conflict_signal_L1;
                 dst_coloring = state->coloring_L2;
                 dst_signal = state->conflict_signal_L2;
                 projection = state->projection_L1_to_L2;
                 src_size = 128; dst_size = 32;
                 break;
             case 2:
                 src_coloring = state->coloring_L2;
                 src_signal = state->conflict_signal_L2;
                 dst_coloring = state->coloring_L3;
                 dst_signal = state->conflict_signal_L3;
                 projection = state->projection_L2_to_L3;
                 src_size = 32; dst_size = 8;
                 break;
             default:
                 return;
         }

         // Aggregate fine vertices to coarse
         for (int c = tid; c < dst_size; c += BLOCK_SIZE) {
             float signal_sum = 0.0f;
             int color_votes[MAX_COLORS];
             for (int i = 0; i < MAX_COLORS; i++) color_votes[i] = 0;
             int count = 0;

             for (int f = 0; f < src_size; f++) {
                 if (projection[f] == c) {
                     signal_sum += src_signal[f];
                     int color = src_coloring[f];
                     if (color >= 0 && color < MAX_COLORS) {
                         color_votes[color]++;
                     }
                     count++;
                 }
             }

             // Majority vote for color
             int best_color = 0;
             int best_votes = 0;
             for (int i = 0; i < MAX_COLORS; i++) {
                 if (color_votes[i] > best_votes) {
                     best_votes = color_votes[i];
                     best_color = i;
                 }
             }

             dst_coloring[c] = best_color;
             dst_signal[c] = signal_sum / fmaxf(1.0f, (float)count);
         }
     }

     /**
      * Multigrid Prolongation: Interpolate coarse solution to fine level
      */
     __device__ void prolongate_to_fine(UltraSharedState* state, int level, const RuntimeConfig* cfg, curandState* rng) {
         int tid = threadIdx.x;

         int* src_coloring;
         int* dst_coloring;
         int* projection;
         int dst_size;

         switch (level) {
             case 0:
                 src_coloring = state->coloring_L1;
                 dst_coloring = state->coloring_L0;
                 projection = state->projection_L0_to_L1;
                 dst_size = 512;
                 break;
             case 1:
                 src_coloring = state->coloring_L2;
                 dst_coloring = state->coloring_L1;
                 projection = state->projection_L1_to_L2;
                 dst_size = 128;
                 break;
             case 2:
                 src_coloring = state->coloring_L3;
                 dst_coloring = state->coloring_L2;
                 projection = state->projection_L2_to_L3;
                 dst_size = 32;
                 break;
             default:
                 return;
         }

         // Interpolate coarse solution to fine
         for (int f = tid; f < dst_size; f += BLOCK_SIZE) {
             int c = projection[f];
             // Use coarse color as hint, blend with current
             int coarse_color = src_coloring[c];

             // Weight by prolongation weight
             if (curand_uniform(rng) < cfg->prolongation_weight) {
                 dst_coloring[f] = coarse_color;
             }
         }
     }

     /**
      * Multigrid Smoothing: Gauss-Seidel relaxation at given level
      */
     __device__ void smooth_iteration(UltraSharedState* state, int level, const RuntimeConfig* cfg, cg::thread_block block) {
         int tid = threadIdx.x;

         int* coloring;
         float* signal;
         int size;

         switch (level) {
             case 0: coloring = state->coloring_L0; signal = state->conflict_signal_L0; size = 512; break;
             case 1: coloring = state->coloring_L1; signal = state->conflict_signal_L1; size = 128; break;
             case 2: coloring = state->coloring_L2; signal = state->conflict_signal_L2; size = 32; break;
             case 3: coloring = state->coloring_L3; signal = state->conflict_signal_L3; size = 8; break;
             default: return;
         }

         // Simple smoothing: dampen high-signal vertices
         for (int i = tid; i < size; i += BLOCK_SIZE) {
             if (signal[i] > 0.5f) {
                 // Dampen conflict signal
                 signal[i] *= 0.9f;
             }
         }
     }

     /**
      * Dendritic Reservoir Update: 8-branch neuromorphic processing
      */
     __device__ void dendritic_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
         // Update dendritic compartments based on conflict signal
         float conflict_input = state->conflict_signal_L0[vertex];

         // Process each branch with its time constant
         for (int b = 0; b < NUM_BRANCHES; b++) {
             float tau = cfg->tau_decay[b];

             // Decay existing activation
             state->dendrite[vertex].activation[b] *= tau;

             // Add new input weighted by branch
             float input_weight = cfg->input_scaling * cfg->branch_weights[b];
             state->dendrite[vertex].activation[b] += conflict_input * input_weight;

             // Clamp activation
             state->dendrite[vertex].activation[b] = fminf(1.0f,
                 fmaxf(-1.0f, state->dendrite[vertex].activation[b]));
         }

         // Update calcium (long-term memory)
         state->dendrite[vertex].calcium *= 0.99f;
         state->dendrite[vertex].calcium += conflict_input * 0.01f;
         state->dendrite[vertex].calcium = fminf(1.0f, state->dendrite[vertex].calcium);

         // Soma integration
         float soma_input = 0.0f;
         for (int b = 0; b < NUM_BRANCHES; b++) {
             soma_input += state->dendrite[vertex].activation[b] * cfg->branch_weights[b];
         }

         state->soma_potential[vertex] = (1.0f - cfg->reservoir_leak_rate) * state->soma_potential[vertex] +
                                         cfg->reservoir_leak_rate * tanhf(soma_input);

         // Check for spike
         if (state->soma_potential[vertex] > state->dendrite[vertex].threshold) {
             state->spike_history[vertex] = 0.9f * state->spike_history[vertex] + 0.1f;
             state->soma_potential[vertex] = 0.0f;
             state->dendrite[vertex].refractory = 2.0f;
         } else {
             state->spike_history[vertex] *= 0.95f;
         }

         // Update refractory period
         if (state->dendrite[vertex].refractory > 0.0f) {
             state->dendrite[vertex].refractory -= 1.0f;
         }
     }

     /**
      * Quantum Evolution: Schrdinger dynamics for 6-state superposition
      */
     __device__ void quantum_evolve(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
         // Evolve quantum amplitudes
         QuantumVertex* q = &state->quantum[vertex];
         float conflict = state->conflict_signal_L0[vertex];

         for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
             // Energy based on conflict for this color
             int color = q->color_idx[s];
             float energy = conflict * cfg->chemical_potential * (float)color / (float)MAX_COLORS;

             // Phase evolution: U(t) = exp(-iHt/)
             float phase = energy * cfg->transverse_field;
             float cos_p = cosf(phase);
             float sin_p = sinf(phase);

             // Rotate amplitude: |  U|
             float r = q->amplitude_real[s];
             float i = q->amplitude_imag[s];
             q->amplitude_real[s] = r * cos_p - i * sin_p;
             q->amplitude_imag[s] = r * sin_p + i * cos_p;

             // Apply decoherence (interference decay)
             q->amplitude_imag[s] *= (1.0f - cfg->interference_decay);
         }

         // Normalize wavefunction to preserve | = 1
         float norm_sq = 0.0f;
         for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
             norm_sq += q->amplitude_real[s] * q->amplitude_real[s] +
                        q->amplitude_imag[s] * q->amplitude_imag[s];
         }
         float norm = sqrtf(fmaxf(EPSILON, norm_sq));
         for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
             q->amplitude_real[s] /= norm;
             q->amplitude_imag[s] /= norm;
         }
     }

     /**
      * TPTP Update: Persistent homology computation (simplified)
      */
     __device__ void tptp_update(UltraSharedState* state, const RuntimeConfig* cfg) {
         // Simplified persistent homology computation
         // Full implementation would build Vietoris-Rips complex and compute homology

         // Count connected components (0)
         int num_components = 0;
         int visited[512];
         for (int i = 0; i < 512; i++) visited[i] = 0;

         for (int i = 0; i < 512; i++) {
             if (!visited[i] && state->conflict_signal_L0[i] > 0.0f) {
                 num_components++;
                 // BFS to mark component (simplified - just mark current)
                 visited[i] = 1;
             }
         }

         float prev_betti_1 = state->tda.betti[1];

         state->tda.betti[0] = (float)num_components;

         // Estimate 1 (cycles) from conflict structure
         // 1  E - V + 1 for connected graph
         int num_conflicts = 0;
         for (int i = 0; i < 512; i++) {
             if (state->conflict_signal_L0[i] > 0.0f) {
                 num_conflicts++;
             }
         }
         state->tda.betti[1] = fmaxf(0.0f, (float)num_conflicts - (float)num_components + 1.0f);
         state->tda.betti[2] = 0.0f; // Would compute voids (2)

         // Compute derivative
         state->tda.betti_1_derivative = state->tda.betti[1] - prev_betti_1;

         // Detect phase transition based on rapid Betti number changes
         float transition_score = fabsf(state->tda.betti_1_derivative);
         state->tda.transition_detected = (transition_score > cfg->transition_sensitivity) ? 1 : 0;

         // Update stability score
         state->tda.stability_score = (transition_score < 0.1f) ?
             fminf(1.0f, state->tda.stability_score + 0.1f) :
             fmaxf(0.0f, state->tda.stability_score - 0.2f);
     }

     /**
      * Tunneling Decision: Determine if quantum tunneling should occur
      */
     __device__ bool should_tunnel(UltraSharedState* state, int vertex, const RuntimeConfig* cfg, curandState* rng) {
         QuantumVertex* q = &state->quantum[vertex];

         // Base tunneling probability
         float prob = cfg->tunneling_prob_base;

         // Boost at phase transitions
         if (state->tda.transition_detected) {
             prob *= cfg->tunneling_prob_boost;
         }

         // Boost for high-conflict vertices
         if (state->conflict_signal_L0[vertex] > 2.0f) {
             prob *= 1.5f;
         }

         // Boost for stagnant vertices (high calcium)
         if (dendritic_enabled(cfg) && state->dendrite[vertex].calcium > 0.8f) {
             prob *= 2.0f;
         }

         q->tunneling_prob = fminf(1.0f, prob);

         // Stochastic decision based on probability
         return (curand_uniform(rng) < q->tunneling_prob);
     }

     /**
      * Active Inference Update: Belief propagation and free energy minimization
      */
     __device__ void active_inference_update(UltraSharedState* state, int vertex, const RuntimeConfig* cfg) {
         int current_color = state->coloring_L0[vertex];
         if (current_color < 0 || current_color >= 16) current_color = 0;

         // Update beliefs based on conflict observations
         float conflict = state->conflict_signal_L0[vertex];

         // Prediction error: expected no conflict, observed conflict
         float prediction_error = conflict;

         // Update belief for current color (decrease if conflicting)
         state->belief_distribution[vertex][current_color] -=
             cfg->belief_update_rate * prediction_error * cfg->precision_weight;

         // Normalize beliefs to ensure valid probability distribution
         float sum = 0.0f;
         for (int c = 0; c < 16; c++) {
             state->belief_distribution[vertex][c] = fmaxf(0.01f, state->belief_distribution[vertex][c]);
             sum += state->belief_distribution[vertex][c];
         }
         for (int c = 0; c < 16; c++) {
             state->belief_distribution[vertex][c] /= fmaxf(EPSILON, sum);
         }

         // Compute expected free energy: F = E[E] - H[P(o|s)]
         float efe = 0.0f;
         for (int c = 0; c < 16; c++) {
             float belief = state->belief_distribution[vertex][c];
             // Entropy term: - p log p
             efe -= belief * logf(fmaxf(EPSILON, belief));
         }
         state->expected_free_energy[vertex] = efe;
     }

     /**
      * Parallel Tempering Step: Metropolis-Hastings at given temperature
      */
     __device__ void tempering_step(UltraSharedState* state, int replica, const RuntimeConfig* cfg, curandState* rng) {
         // Simplified parallel tempering step
         float temp = state->temperatures[replica];
         TemperingReplica* rep = &state->replica[replica];

         int lane = threadIdx.x % 32;
         if (lane < 64) {
             int v = lane;
             int current = rep->coloring[v];

             // Propose random new color
             int new_color = (int)(curand_uniform(rng) * MAX_COLORS) % MAX_COLORS;

             // Compute energy change (simplified - would need actual graph)
             float delta_E = 0.0f; // Placeholder

             // Metropolis acceptance criterion
             bool accept = (delta_E <= 0.0f) ||
                          (curand_uniform(rng) < expf(-delta_E / fmaxf(EPSILON, temp)));

             if (accept) {
                 rep->coloring[v] = new_color;
             }
         }
     }

     /**
      * Replica Exchange: Swap configurations between adjacent temperature replicas
      */
     __device__ void replica_exchange(UltraSharedState* state, const RuntimeConfig* cfg, curandState* rng) {
         // Attempt swaps between adjacent replicas
         int max_replicas = min(cfg->num_replicas, NUM_REPLICAS);

         for (int i = 0; i < max_replicas - 1; i += 2) {
             TemperingReplica* r1 = &state->replica[i];
             TemperingReplica* r2 = &state->replica[i + 1];

             float T1 = state->temperatures[i];
             float T2 = state->temperatures[i + 1];
             float E1 = (float)r1->conflicts;
             float E2 = (float)r2->conflicts;

             // Swap probability: P = exp[(1/T1 - 1/T2)(E2 - E1)]
             float delta = (1.0f/fmaxf(EPSILON, T1) - 1.0f/fmaxf(EPSILON, T2)) * (E2 - E1);
             bool accept = (delta >= 0.0f) || (curand_uniform(rng) < expf(delta));

             if (accept) {
                 // Swap colorings
                 for (int v = 0; v < 64; v++) {
                     int temp = r1->coloring[v];
                     r1->coloring[v] = r2->coloring[v];
                     r2->coloring[v] = temp;
                 }
                 // Swap energies
                 int temp_c = r1->conflicts;
                 r1->conflicts = r2->conflicts;
                 r2->conflicts = temp_c;

                 float temp_e = r1->energy;
                 r1->energy = r2->energy;
                 r2->energy = temp_e;
             }
         }
     }

     // 
     // PART 2: ADVANCED PARALLEL TEMPERING & WHCR FUNCTIONS (Lines 1000-2000)
     // 

     /**
      * ReplicaState - Extended state for parallel tempering replicas
      */
     struct ReplicaState {
         int coloring[512];           // Full coloring for this replica
         float energy;                // Total energy (conflicts + potential)
         int conflicts;               // Number of conflicts
         float acceptance_rate;       // Running acceptance ratio
         int moves_attempted;         // Move counter
         int moves_accepted;          // Accepted move counter
         float temperature;           // Current temperature
         int color_histogram[MAX_COLORS]; // Color usage frequency
     };

     /**
      * ActiveInferenceBeliefs - Extended belief state structure
      */
     struct ActiveInferenceBeliefs {
         float color_beliefs[MAX_COLORS];     // Belief distribution over colors
         float expected_utility[MAX_COLORS];  // Expected utility per color
         float prediction_error;              // Cumulative prediction error
         float precision;                     // Precision weight (inverse variance)
         float policy_entropy;                // Entropy of action distribution
         int preferred_action;                // MAP estimate of best action
     };

     // 
     // PARALLEL TEMPERING FUNCTIONS (Lines 1000-1300)
     // 

     /**
      * PT Replica Update - Metropolis-Hastings move at replica temperature
      * Uses warp shuffle for efficient conflict counting
      */
     __device__ void pt_replica_update(
         ReplicaState* replicas,
         float* energies,
         int* colors,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int replica_id,
         int num_vertices,
         float temperature,
         curandState* rng
     ) {
         int lane = threadIdx.x % WARP_SIZE;
         int warp_id = threadIdx.x / WARP_SIZE;

         ReplicaState* replica = &replicas[replica_id];

         // Each thread in warp handles different vertices
         for (int v = lane; v < num_vertices; v += WARP_SIZE) {
             int current_color = replica->coloring[v];

             // Propose new color (random or neighbor-aware)
             int new_color;
             if (curand_uniform(rng) < 0.3f) {
                 // Random color
                 new_color = (int)(curand_uniform(rng) * MAX_COLORS) % MAX_COLORS;
             } else {
                 // Neighbor-aware: pick least-used neighbor color
                 int color_counts[MAX_COLORS];
                 for (int c = 0; c < MAX_COLORS; c++) color_counts[c] = 0;

                 int start = row_ptr[v];
                 int end = row_ptr[v + 1];
                 for (int e = start; e < end; e++) {
                     int neighbor = col_idx[e];
                     int nc = replica->coloring[neighbor];
                     if (nc >= 0 && nc < MAX_COLORS) {
                         color_counts[nc]++;
                     }
                 }

                 // Find least-conflicting color
                 int min_conflicts = 999999;
                 new_color = current_color;
                 for (int c = 0; c < MAX_COLORS; c++) {
                     if (color_counts[c] < min_conflicts) {
                         min_conflicts = color_counts[c];
                         new_color = c;
                     }
                 }
             }

             // Compute energy change
             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             float old_energy = 0.0f;
             float new_energy = 0.0f;

             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 int neighbor_color = replica->coloring[neighbor];

                 if (neighbor_color == current_color) old_energy += 1.0f;
                 if (neighbor_color == new_color) new_energy += 1.0f;
             }

             float delta_E = new_energy - old_energy;

             // Metropolis acceptance with temperature
             bool accept = false;
             if (delta_E <= 0.0f) {
                 accept = true;
             } else {
                 float prob = expf(-delta_E / fmaxf(EPSILON, temperature));
                 accept = (curand_uniform(rng) < prob);
             }

             // Apply move if accepted
             if (accept) {
                 replica->coloring[v] = new_color;
                 atomicAdd(&replica->moves_accepted, 1);
             }
             atomicAdd(&replica->moves_attempted, 1);
         }

         // Warp-level reduction to compute total energy
         __shared__ float warp_energies[8]; // Support up to 256 threads (8 warps)

         float thread_energy = 0.0f;
         for (int v = lane; v < num_vertices; v += WARP_SIZE) {
             int my_color = replica->coloring[v];
             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 if (replica->coloring[neighbor] == my_color) {
                     thread_energy += 1.0f;
                 }
             }
         }

         // Warp shuffle reduction
         for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
             thread_energy += __shfl_down_sync(0xFFFFFFFF, thread_energy, offset);
         }

         // First thread in warp writes result
         if (lane == 0) {
             warp_energies[warp_id] = thread_energy;
         }
         __syncthreads();

         // Final reduction across warps (thread 0 only)
         if (threadIdx.x == 0) {
             float total_energy = 0.0f;
             int num_warps = (blockDim.x + WARP_SIZE - 1) / WARP_SIZE;
             for (int w = 0; w < num_warps; w++) {
                 total_energy += warp_energies[w];
             }
             replica->energy = total_energy / 2.0f; // Each edge counted twice
             replica->conflicts = (int)replica->energy;

             // Update acceptance rate
             if (replica->moves_attempted > 0) {
                 replica->acceptance_rate = (float)replica->moves_accepted / (float)replica->moves_attempted;
             }
         }
     }

     /**
      * PT Exchange Criterion - Compute Metropolis acceptance for replica swap
      */
     __device__ bool pt_exchange_criterion(
         float energy_i,
         float energy_j,
         float temp_i,
         float temp_j,
         curandState* rng
     ) {
         // Parallel tempering exchange probability:
         // P(swap) = min(1, exp[(_i - _j)(E_j - E_i)])
         // where  = 1/T

         float beta_i = 1.0f / fmaxf(EPSILON, temp_i);
         float beta_j = 1.0f / fmaxf(EPSILON, temp_j);

         float delta = (beta_i - beta_j) * (energy_j - energy_i);

         if (delta >= 0.0f) {
             return true; // Always accept beneficial swaps
         } else {
             float prob = expf(delta);
             return (curand_uniform(rng) < prob);
         }
     }

     /**
      * PT Swap Replicas - Exchange configurations between two replicas
      * Uses shared memory for efficient swapping
      */
     __device__ void pt_swap_replicas(
         ReplicaState* replicas,
         int replica_a,
         int replica_b,
         int num_vertices
     ) {
         int tid = threadIdx.x;

         ReplicaState* ra = &replicas[replica_a];
         ReplicaState* rb = &replicas[replica_b];

         // Parallel swap of colorings
         for (int v = tid; v < num_vertices; v += blockDim.x) {
             int temp = ra->coloring[v];
             ra->coloring[v] = rb->coloring[v];
             rb->coloring[v] = temp;
         }

         // Swap energies (thread 0 only)
         if (tid == 0) {
             float temp_e = ra->energy;
             ra->energy = rb->energy;
             rb->energy = temp_e;

             int temp_c = ra->conflicts;
             ra->conflicts = rb->conflicts;
             rb->conflicts = temp_c;
         }

         __syncthreads();
     }

     /**
      * PT Adaptive Temperature Schedule - Adjust temperatures based on acceptance rates
      */
     __device__ void pt_adaptive_temperature(
         ReplicaState* replicas,
         float* temperature_ladder,
         int num_replicas,
         float target_acceptance
     ) {
         int tid = threadIdx.x;

         // Each thread handles one replica
         if (tid < num_replicas) {
             ReplicaState* replica = &replicas[tid];

             // Target acceptance rate is typically 0.25-0.35
             float current_rate = replica->acceptance_rate;
             float diff = current_rate - target_acceptance;

             // Adjust temperature: increase if too low acceptance, decrease if too high
             float adjustment = 1.0f + 0.05f * diff;
             replica->temperature *= adjustment;

             // Clamp temperature to reasonable range
             replica->temperature = fminf(10.0f, fmaxf(0.1f, replica->temperature));

             // Update ladder
             temperature_ladder[tid] = replica->temperature;
         }

         __syncthreads();
     }

     // 
     // WHCR CONFLICT REPAIR FUNCTIONS (Lines 1300-1600)
     // 

     /**
      * WHCR Count Conflicts - Efficiently count conflicts for a vertex
      * Uses warp shuffle for neighbor conflict aggregation
      */
     __device__ int whcr_count_conflicts(
         int* colors,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int vertex,
         int num_vertices
     ) {
         if (vertex >= num_vertices) return 0;

         int my_color = colors[vertex];
         int start = row_ptr[vertex];
         int end = row_ptr[vertex + 1];

         int conflicts = 0;
         int lane = threadIdx.x % WARP_SIZE;

         // Process neighbors in parallel within warp
         for (int e = start + lane; e < end; e += WARP_SIZE) {
             int neighbor = col_idx[e];
             if (colors[neighbor] == my_color) {
                 conflicts++;
             }
         }

         // Warp-level reduction using shuffle
         for (int offset = WARP_SIZE / 2; offset > 0; offset /= 2) {
             conflicts += __shfl_down_sync(0xFFFFFFFF, conflicts, offset);
         }

         // First lane has total
         return __shfl_sync(0xFFFFFFFF, conflicts, 0);
     }

     /**
      * WHCR Wavelet Priority - Compute vertex priority using wavelet coefficients
      * Multi-resolution analysis identifies important vertices at different scales
      */
     __device__ void whcr_wavelet_priority(
         float* priorities,
         int* conflict_counts,
         float* wavelet_coeffs,
         int num_vertices,
         int level,
         const RuntimeConfig* cfg
     ) {
         int tid = threadIdx.x;

         for (int v = tid; v < num_vertices; v += blockDim.x) {
             float priority = 0.0f;

             // Base priority from conflicts
             priority += (float)conflict_counts[v] * cfg->stress_weight;

             // Add wavelet detail coefficients at multiple scales
             // Higher-frequency details indicate local hotspots
             for (int l = 0; l < min(level, NUM_LEVELS); l++) {
                 int idx = l * num_vertices + v;
                 float detail = wavelet_coeffs[idx];

                 // Weight higher frequencies more (they indicate sharp transitions)
                 float scale_weight = powf(2.0f, (float)l);
                 priority += fabsf(detail) * scale_weight * cfg->persistence_weight;
             }

             // Apply hotspot multiplier for high-priority vertices
             if (priority > 5.0f) {
                 priority *= cfg->hotspot_multiplier;
             }

             priorities[v] = priority;
         }

         __syncthreads();
     }

     /**
      * WHCR Wavelet Transform - Haar wavelet decomposition
      * Performs 1D Haar wavelet transform on conflict signal
      */
     __device__ void whcr_wavelet_transform(
         float* signal,
         float* approx,
         float* detail,
         int size
     ) {
         int tid = threadIdx.x;

         // Haar wavelet: approximation and detail coefficients
         int output_size = size / 2;

         for (int i = tid; i < output_size; i += blockDim.x) {
             int idx = i * 2;
             if (idx + 1 < size) {
                 float s0 = signal[idx];
                 float s1 = signal[idx + 1];

                 // Approximation: (s0 + s1) / sqrt(2)
                 approx[i] = (s0 + s1) * 0.7071067811865476f;

                 // Detail: (s0 - s1) / sqrt(2)
                 detail[i] = (s0 - s1) * 0.7071067811865476f;
             }
         }

         __syncthreads();
     }

     /**
      * WHCR Select Repair Color - Choose best color for conflict repair
      * Uses weighted color selection with wavelet-guided priorities
      */
     __device__ int whcr_select_repair_color(
         int vertex,
         int* colors,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int num_colors,
         float* color_weights,
         float priority,
         const RuntimeConfig* cfg,
         curandState* rng
     ) {
         int current_color = colors[vertex];

         // Build color frequency table
         float color_scores[MAX_COLORS];
         for (int c = 0; c < MAX_COLORS; c++) {
             color_scores[c] = 0.0f;
         }

         // Count neighbor colors
         int start = row_ptr[vertex];
         int end = row_ptr[vertex + 1];

         for (int e = start; e < end; e++) {
             int neighbor = col_idx[e];
             int nc = colors[neighbor];
             if (nc >= 0 && nc < MAX_COLORS) {
                 color_scores[nc] += 1.0f; // Penalty for used colors
             }
         }

         // Compute repair scores (lower is better)
         float best_score = 1e9f;
         int best_color = current_color;

         for (int c = 0; c < min(num_colors, MAX_COLORS); c++) {
             if (c == current_color) continue;

             float score = color_scores[c]; // Conflict penalty

             // Add chemical potential (prefer lower colors)
             score += cfg->chemical_potential * (float)c / (float)MAX_COLORS;

             // Weight by wavelet priority (high priority  more exploration)
             if (priority > 3.0f) {
                 // High priority: add randomness for exploration
                 score += curand_uniform(rng) * 2.0f;
             }

             // Apply external color weights if provided
             if (color_weights != nullptr) {
                 score *= (1.0f + color_weights[c]);
             }

             if (score < best_score) {
                 best_score = score;
                 best_color = c;
             }
         }

         return best_color;
     }

     /**
      * WHCR Apply Repair - Atomically apply color repair and track delta
      */
     __device__ void whcr_apply_repair(
         int* colors,
         int vertex,
         int new_color,
         int* conflict_delta,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int* locks
     ) {
         // Acquire lock
         while (atomicCAS(&locks[vertex], 0, 1) != 0) {
             // Spin wait
         }

         int old_color = colors[vertex];

         // Count old conflicts
         int old_conflicts = 0;
         int start = row_ptr[vertex];
         int end = row_ptr[vertex + 1];

         for (int e = start; e < end; e++) {
             int neighbor = col_idx[e];
             if (colors[neighbor] == old_color) {
                 old_conflicts++;
             }
         }

         // Apply new color
         colors[vertex] = new_color;

         // Count new conflicts
         int new_conflicts = 0;
         for (int e = start; e < end; e++) {
             int neighbor = col_idx[e];
             if (colors[neighbor] == new_color) {
                 new_conflicts++;
             }
         }

         // Update delta (negative is improvement)
         int delta = new_conflicts - old_conflicts;
         atomicAdd(conflict_delta, delta);

         // Release lock
         atomicExch(&locks[vertex], 0);
     }

     /**
      * WHCR Hierarchical Repair - Multi-level repair using wavelet decomposition
      */
     __device__ void whcr_hierarchical_repair(
         UltraSharedState* state,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int num_vertices,
         const RuntimeConfig* cfg,
         curandState* rng
     ) {
         int tid = threadIdx.x;
         cg::thread_block block = cg::this_thread_block();

         // Perform wavelet decomposition on conflict signal
         for (int level = 0; level < cfg->num_levels; level++) {
             int size = 512 >> level; // 512, 256, 128, 64

             if (tid < size / 2) {
                 whcr_wavelet_transform(
                     level == 0 ? state->conflict_signal_L0 : state->wavelet_approx[level - 1],
                     state->wavelet_approx[level],
                     (float*)state->wavelet_detail[level],
                     size
                 );
             }
             block.sync();
         }

         // Compute priorities using wavelet coefficients
         float priorities[512];
         int conflict_counts[512];

         for (int v = tid; v < num_vertices; v += blockDim.x) {
             conflict_counts[v] = whcr_count_conflicts(
                 state->coloring_L0,
                 row_ptr,
                 col_idx,
                 v,
                 num_vertices
             );
         }
         block.sync();

         whcr_wavelet_priority(
             priorities,
             conflict_counts,
             (float*)state->wavelet_detail[0], // Use detail coefficients
             num_vertices,
             cfg->num_levels,
             cfg
         );

         // Repair vertices in priority order (highest first)
         // Sort-free approach: iterate with threshold
         for (float threshold = 10.0f; threshold > 0.0f; threshold -= 2.0f) {
             for (int v = tid; v < num_vertices; v += blockDim.x) {
                 if (priorities[v] >= threshold && conflict_counts[v] > 0) {
                     int new_color = whcr_select_repair_color(
                         v,
                         state->coloring_L0,
                         row_ptr,
                         col_idx,
                         MAX_COLORS,
                         nullptr,
                         priorities[v],
                         cfg,
                         rng
                     );

                     if (new_color != state->coloring_L0[v]) {
                         int delta = 0;
                         whcr_apply_repair(
                             state->coloring_L0,
                             v,
                             new_color,
                             &delta,
                             row_ptr,
                             col_idx,
                             state->locks
                         );
                     }
                 }
             }
             block.sync();
         }
     }

     // 
     // ACTIVE INFERENCE FUNCTIONS (Lines 1600-1900)
     // 

     /**
      * AI Update Beliefs - Bayesian belief update based on observations
      */
     __device__ void ai_update_beliefs(
         ActiveInferenceBeliefs* beliefs,
         float* observations,
         float* predictions,
         int num_vertices,
         const RuntimeConfig* cfg
     ) {
         int tid = threadIdx.x;

         for (int v = tid; v < num_vertices; v += blockDim.x) {
             ActiveInferenceBeliefs* b = &beliefs[v];

             float obs = observations[v];  // Observed conflict
             float pred = predictions[v];  // Predicted conflict

             // Prediction error
             float error = obs - pred;
             b->prediction_error = 0.9f * b->prediction_error + 0.1f * fabsf(error);

             // Update precision (inverse variance) - higher for stable predictions
             if (fabsf(error) < 0.1f) {
                 b->precision = fminf(10.0f, b->precision * 1.05f);
             } else {
                 b->precision = fmaxf(0.1f, b->precision * 0.95f);
             }

             // Bayesian belief update: P(s|o)  P(o|s) P(s)
             float likelihood_weight = cfg->precision_weight * b->precision;

             for (int c = 0; c < MAX_COLORS; c++) {
                 // Likelihood: low if color would cause conflicts
                 float likelihood = expf(-likelihood_weight * obs);

                 // Prior (current belief)
                 float prior = b->color_beliefs[c];

                 // Posterior  likelihood  prior
                 b->color_beliefs[c] = likelihood * prior;
             }

             // Normalize beliefs
             float sum = 0.0f;
             for (int c = 0; c < MAX_COLORS; c++) {
                 b->color_beliefs[c] = fmaxf(EPSILON, b->color_beliefs[c]);
                 sum += b->color_beliefs[c];
             }
             for (int c = 0; c < MAX_COLORS; c++) {
                 b->color_beliefs[c] /= fmaxf(EPSILON, sum);
             }
         }

         __syncthreads();
     }

     /**
      * AI Free Energy - Compute variational free energy
      * F = E_Q[E] - H[Q] where Q is belief distribution
      */
     __device__ float ai_free_energy(
         ActiveInferenceBeliefs* beliefs,
         int vertex,
         float* conflict_observations
     ) {
         ActiveInferenceBeliefs* b = &beliefs[vertex];

         // Energy term: expected conflict under current beliefs
         float expected_energy = 0.0f;
         for (int c = 0; c < MAX_COLORS; c++) {
             expected_energy += b->color_beliefs[c] * conflict_observations[vertex];
         }

         // Entropy term: H[Q] = - Q(s) log Q(s)
         float entropy = 0.0f;
         for (int c = 0; c < MAX_COLORS; c++) {
             float belief = b->color_beliefs[c];
             if (belief > EPSILON) {
                 entropy -= belief * logf(belief);
             }
         }

         // Free energy = Energy - Entropy
         float free_energy = expected_energy - entropy;

         return free_energy;
     }

     /**
      * AI Action Selection - Select action (color) to minimize expected free energy
      * Implements softmax policy over expected free energies
      */
     __device__ void ai_action_selection(
         ActiveInferenceBeliefs* beliefs,
         float* action_probs,
         int vertex,
         int num_actions,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int* current_colors,
         const RuntimeConfig* cfg,
         curandState* rng
     ) {
         ActiveInferenceBeliefs* b = &beliefs[vertex];

         // Compute expected free energy for each action (color)
         float efe[MAX_COLORS];
         float min_efe = 1e9f;

         for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
             // Expected utility (negative conflicts)
             int start = row_ptr[vertex];
             int end = row_ptr[vertex + 1];

             float expected_conflicts = 0.0f;
             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 int neighbor_color = current_colors[neighbor];
                 if (neighbor_color == c) {
                     expected_conflicts += 1.0f;
                 }
             }

             // Information gain (entropy reduction)
             float info_gain = -logf(fmaxf(EPSILON, b->color_beliefs[c]));

             // EFE = Expected Cost - Information Gain
             efe[c] = expected_conflicts - 0.5f * info_gain;
             b->expected_utility[c] = -expected_conflicts;

             if (efe[c] < min_efe) {
                 min_efe = efe[c];
             }
         }

         // Softmax over negative EFE (lower EFE  higher probability)
         float temperature = cfg->policy_temperature;
         float sum_exp = 0.0f;

         for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
             float score = -(efe[c] - min_efe) / temperature;
             action_probs[c] = expf(score);
             sum_exp += action_probs[c];
         }

         // Normalize to probability distribution
         for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
             action_probs[c] /= fmaxf(EPSILON, sum_exp);
         }

         // Compute policy entropy
         float policy_entropy = 0.0f;
         for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
             if (action_probs[c] > EPSILON) {
                 policy_entropy -= action_probs[c] * logf(action_probs[c]);
             }
         }
         b->policy_entropy = policy_entropy;

         // Select action (MAP estimate)
         float max_prob = 0.0f;
         int best_action = 0;
         for (int c = 0; c < min(num_actions, MAX_COLORS); c++) {
             if (action_probs[c] > max_prob) {
                 max_prob = action_probs[c];
                 best_action = c;
             }
         }
         b->preferred_action = best_action;
     }

     /**
      * AI Precision Weighting - Adaptive precision for belief updates
      * Higher precision when predictions are accurate
      */
     __device__ void ai_update_precision(
         ActiveInferenceBeliefs* beliefs,
         float* prediction_errors,
         int num_vertices,
         float adaptation_rate
     ) {
         int tid = threadIdx.x;

         for (int v = tid; v < num_vertices; v += blockDim.x) {
             ActiveInferenceBeliefs* b = &beliefs[v];
             float error = prediction_errors[v];

             // Adaptive precision: decrease if high error, increase if low error
             if (error > 1.0f) {
                 b->precision *= (1.0f - adaptation_rate);
             } else if (error < 0.1f) {
                 b->precision *= (1.0f + adaptation_rate);
             }

             // Clamp precision
             b->precision = fminf(10.0f, fmaxf(0.01f, b->precision));
         }

         __syncthreads();
     }

     // 
     // W-CYCLE MULTIGRID FUNCTIONS (Lines 1900-2000)
     // 

     /**
      * Multigrid Restrict - Restriction operator for coarsening
      * Uses full-weighting restriction for better accuracy
      */
     __device__ void multigrid_restrict(
         float* fine,
         float* coarse,
         int fine_size,
         int coarse_size
     ) {
         int tid = threadIdx.x;
         int ratio = fine_size / coarse_size;

         for (int c = tid; c < coarse_size; c += blockDim.x) {
             float sum = 0.0f;
             float weight_sum = 0.0f;

             // Full-weighting: include neighbors with weights
             int center = c * ratio;

             for (int i = 0; i < ratio; i++) {
                 int idx = center + i;
                 if (idx < fine_size) {
                     // Center weight: 0.5, neighbor weight: 0.25
                     float weight = (i == ratio / 2) ? 0.5f : 0.25f;
                     sum += fine[idx] * weight;
                     weight_sum += weight;
                 }
             }

             coarse[c] = sum / fmaxf(EPSILON, weight_sum);
         }

         __syncthreads();
     }

     /**
      * Multigrid Prolongate - Interpolation operator for refinement
      * Uses linear interpolation for smooth prolongation
      */
     __device__ void multigrid_prolongate(
         float* coarse,
         float* fine,
         int coarse_size,
         int fine_size
     ) {
         int tid = threadIdx.x;
         int ratio = fine_size / coarse_size;

         for (int f = tid; f < fine_size; f += blockDim.x) {
             int c_left = f / ratio;
             int c_right = min(c_left + 1, coarse_size - 1);

             // Linear interpolation
             float alpha = (float)(f % ratio) / (float)ratio;
             fine[f] = (1.0f - alpha) * coarse[c_left] + alpha * coarse[c_right];
         }

         __syncthreads();
     }

     /**
      * Multigrid Smooth - Jacobi/Gauss-Seidel smoothing
      * Uses weighted Jacobi for parallel efficiency
      */
     __device__ void multigrid_smooth(
         float* data,
         float* buffer,
         int size,
         int iterations,
         float omega
     ) {
         int tid = threadIdx.x;

         for (int iter = 0; iter < iterations; iter++) {
             // Weighted Jacobi: x^(k+1) = (1-)x^k +  D^(-1)(b - Rx^k)
             for (int i = tid; i < size; i += blockDim.x) {
                 float left = (i > 0) ? data[i - 1] : data[i];
                 float right = (i < size - 1) ? data[i + 1] : data[i];
                 float center = data[i];

                 // Simple averaging with relaxation
                 float new_val = 0.25f * left + 0.5f * center + 0.25f * right;
                 buffer[i] = (1.0f - omega) * center + omega * new_val;
             }
             __syncthreads();

             // Copy buffer back to data
             for (int i = tid; i < size; i += blockDim.x) {
                 data[i] = buffer[i];
             }
             __syncthreads();
         }
     }

     /**
      * Multigrid V-Cycle - Single V-cycle iteration
      */
     __device__ void multigrid_vcycle(
         float** levels,
         float** buffers,
         int* sizes,
         int num_levels,
         int pre_smooth,
         int post_smooth,
         float omega
     ) {
         // Downward sweep (restriction + pre-smoothing)
         for (int l = 0; l < num_levels - 1; l++) {
             multigrid_smooth(levels[l], buffers[l], sizes[l], pre_smooth, omega);
             multigrid_restrict(levels[l], levels[l + 1], sizes[l], sizes[l + 1]);
         }

         // Coarsest level solve (extra smoothing)
         int coarsest = num_levels - 1;
         multigrid_smooth(levels[coarsest], buffers[coarsest], sizes[coarsest], 10, omega);

         // Upward sweep (prolongation + post-smoothing)
         for (int l = num_levels - 2; l >= 0; l--) {
             multigrid_prolongate(levels[l + 1], buffers[l], sizes[l + 1], sizes[l]);

             // Add correction
             for (int i = threadIdx.x; i < sizes[l]; i += blockDim.x) {
                 levels[l][i] += buffers[l][i];
             }
             __syncthreads();

             multigrid_smooth(levels[l], buffers[l], sizes[l], post_smooth, omega);
         }
     }

     /**
      * Multigrid W-Cycle - More aggressive W-cycle (visits coarse levels twice)
      */
     __device__ void multigrid_wcycle(
         float** levels,
         float** buffers,
         int* sizes,
         int num_levels,
         int pre_smooth,
         int post_smooth,
         float omega
     ) {
         int tid = threadIdx.x;

         // W-cycle: recurse twice at each level
         // For simplicity, implement as double V-cycle
         for (int cycle = 0; cycle < 2; cycle++) {
             multigrid_vcycle(levels, buffers, sizes, num_levels, pre_smooth, post_smooth, omega);
         }
     }

     // 
     // END OF PART 2: ADVANCED FUNCTIONS
     // 

     // 
     // PART 3: MAIN ULTRA KERNEL ENTRY POINTS & ORCHESTRATION (Lines 2000-3000)
     // 

     /**
      * Ultra Shared Memory Orchestrator
      *
      * Manages the complex 98KB shared memory layout across all subsystems.
      * Provides compile-time offsets and runtime validation.
      */
     struct UltraSharedMemoryOrchestrator {
         // Memory partitions (offsets in bytes from shared base)
         static constexpr size_t DENDRITIC_OFFSET = 0;
         static constexpr size_t DENDRITIC_SIZE = 24 * 1024; // 24KB

         static constexpr size_t QUANTUM_OFFSET = DENDRITIC_OFFSET + DENDRITIC_SIZE;
         static constexpr size_t QUANTUM_SIZE = 16 * 1024; // 16KB

         static constexpr size_t REPLICA_OFFSET = QUANTUM_OFFSET + QUANTUM_SIZE;
         static constexpr size_t REPLICA_SIZE = 24 * 1024; // 24KB (12 replicas)

         static constexpr size_t WHCR_OFFSET = REPLICA_OFFSET + REPLICA_SIZE;
         static constexpr size_t WHCR_SIZE = 16 * 1024; // 16KB

         static constexpr size_t INFERENCE_OFFSET = WHCR_OFFSET + WHCR_SIZE;
         static constexpr size_t INFERENCE_SIZE = 8 * 1024; // 8KB

         static constexpr size_t WORK_OFFSET = INFERENCE_OFFSET + INFERENCE_SIZE;
         static constexpr size_t WORK_SIZE = 10 * 1024; // 10KB

         static constexpr size_t TOTAL_SIZE = WORK_OFFSET + WORK_SIZE; // 98KB

         /**
          * Validate shared memory usage at compile time
          */
         __device__ __forceinline__ static bool validate() {
             return TOTAL_SIZE <= 100 * 1024; // RTX 3060 has 100KB shared memory
         }

         /**
          * Get dendritic state pointer from shared memory base
          */
         __device__ __forceinline__ static DendriticState* get_dendritic(char* smem_base) {
             return reinterpret_cast<DendriticState*>(smem_base + DENDRITIC_OFFSET);
         }

         /**
          * Get quantum state pointer from shared memory base
          */
         __device__ __forceinline__ static QuantumVertex* get_quantum(char* smem_base) {
             return reinterpret_cast<QuantumVertex*>(smem_base + QUANTUM_OFFSET);
         }

         /**
          * Get replica state pointer from shared memory base
          */
         __device__ __forceinline__ static TemperingReplica* get_replicas(char* smem_base) {
             return reinterpret_cast<TemperingReplica*>(smem_base + REPLICA_OFFSET);
         }

         /**
          * Get work buffer pointer from shared memory base
          */
         __device__ __forceinline__ static float* get_work_buffer(char* smem_base) {
             return reinterpret_cast<float*>(smem_base + WORK_OFFSET);
         }
     };

     /**
      * Ultra Kernel Configuration - Extended runtime parameters
      */
     struct UltraKernelConfig {
         // Core parameters
         int num_vertices;
         int num_edges;
         int max_iterations;

         // GPU resources
         int num_blocks;
         int threads_per_block;
         int shared_mem_size;

         // Optimization flags
         bool enable_cooperative_groups;
         bool enable_vectorization;
         bool enable_async_memcpy;

         // Performance tuning
         int warp_specialization_factor;
         int occupancy_target;
         float memory_bandwidth_fraction;

         // Convergence criteria
         float conflict_tolerance;
         int stagnation_limit;
         bool early_stopping;
     };

     // 
     // MAIN ULTRA KERNEL ENTRY POINT
     // 

     /**
      * dr_whcr_ultra_fused_kernel
      *
      * The crown jewel of PRISM - fully fused ultra optimization kernel.
      * Combines all 8 advanced techniques with cooperative grid synchronization.
      *
      * Features:
      * - Cooperative grid-level synchronization
      * - 98KB shared memory orchestration
      * - Vectorized memory access (float4)
      * - Optimal occupancy (RTX 3060: 50% occupancy target)
      * - Warp-specialized execution
      *
      * Requirements:
      * - CUDA 11.0+ for cooperative groups
      * - Compute Capability 8.6+ (RTX 3060)
      * - Must be launched via cudaLaunchCooperativeKernel
      */
     extern "C" __global__ void __launch_bounds__(256, 4) dr_whcr_ultra_fused_kernel(
         // Graph structure (CSR format)
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int num_vertices,
         int num_edges,

         // State arrays (global memory)
         int* __restrict__ colors,
         int* __restrict__ best_colors,
         int* __restrict__ best_num_colors,
         int* __restrict__ conflicts,

         // Dendritic state (global)
         float* __restrict__ dendritic_state_global,
         float* __restrict__ soma_potential_global,

         // Quantum state (global)
         float* __restrict__ quantum_real_global,
         float* __restrict__ quantum_imag_global,

         // Parallel tempering state (global)
         float* __restrict__ replica_temps,
         float* __restrict__ replica_energies,
         int* __restrict__ replica_colors,

         // Active inference state (global)
         float* __restrict__ belief_state_global,
         float* __restrict__ free_energy_global,

         // Configuration
         RuntimeConfig config,

         // RNG state
         curandState* __restrict__ rng_states,

         // Telemetry output
         KernelTelemetry* __restrict__ telemetry,

         // Iteration counter
         int iteration
     ) {
         // 
         // COOPERATIVE GROUPS SETUP
         // 

         cg::grid_group grid = cg::this_grid();
         cg::thread_block block = cg::this_thread_block();

         int gid = blockIdx.x * blockDim.x + threadIdx.x;
         int tid = threadIdx.x;
         int bid = blockIdx.x;
         int lane_id = tid % 32;
         int warp_id = tid / 32;

         // 
         // SHARED MEMORY ALLOCATION
         // 

         extern __shared__ char shared_mem[];
         UltraSharedState* state = reinterpret_cast<UltraSharedState*>(shared_mem);

         // Validate shared memory layout
         static_assert(sizeof(UltraSharedState) <= 100 * 1024, "Shared memory exceeds 100KB limit");

         // Initialize RNG for this thread
         curandState local_rng = rng_states[gid];

         // 
         // PHASE 1: INITIALIZE SHARED MEMORY (Vectorized)
         // 

         // Calculate vertex partition for this block
         int vertices_per_block = (num_vertices + gridDim.x - 1) / gridDim.x;
         int block_start = bid * vertices_per_block;
         int block_end = min(block_start + vertices_per_block, num_vertices);
         int block_vertex_count = block_end - block_start;

         // Vectorized initialization of quantum state (float4 for coalescing)
         if (quantum_enabled(&config)) {
             int quantum_elements = min(block_vertex_count, MAX_VERTICES_PER_BLOCK) * NUM_QUANTUM_STATES;
             float4* quantum_real_vec = reinterpret_cast<float4*>(&state->quantum[0].amplitude_real[0]);
             float4* quantum_imag_vec = reinterpret_cast<float4*>(&state->quantum[0].amplitude_imag[0]);

             float amp = 1.0f / sqrtf((float)NUM_QUANTUM_STATES);
             float4 init_real = make_float4(amp, amp, amp, amp);
             float4 init_imag = make_float4(0.0f, 0.0f, 0.0f, 0.0f);

             for (int i = tid; i < quantum_elements / 4; i += blockDim.x) {
                 quantum_real_vec[i] = init_real;
                 quantum_imag_vec[i] = init_imag;
             }
         }

         // Vectorized initialization of belief distributions (float4)
         if (active_inference_enabled(&config)) {
             int belief_elements = min(block_vertex_count, MAX_VERTICES_PER_BLOCK) * 16;
             float4* belief_vec = reinterpret_cast<float4*>(&state->belief_distribution[0][0]);

             float4 uniform_belief = make_float4(1.0f/16.0f, 1.0f/16.0f, 1.0f/16.0f, 1.0f/16.0f);

             for (int i = tid; i < belief_elements / 4; i += blockDim.x) {
                 belief_vec[i] = uniform_belief;
             }
         }

         // Initialize dendritic state
         if (dendritic_enabled(&config)) {
             for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
                 for (int b = 0; b < NUM_BRANCHES; b++) {
                     state->dendrite[i].activation[b] = 0.0f;
                 }
                 state->dendrite[i].calcium = 0.0f;
                 state->dendrite[i].threshold = 0.5f;
                 state->dendrite[i].refractory = 0.0f;

                 state->soma_potential[i] = 0.0f;
                 state->spike_history[i] = 0.0f;
             }
         }

         // Initialize temperature ladder for parallel tempering
         if (tempering_enabled(&config)) {
             for (int i = tid; i < 16; i += blockDim.x) {
                 if (i < 8) {
                     state->temperatures[i] = config.temperatures[i];
                 } else {
                     state->temperatures[i] = 1.0f;
                 }
             }
         }

         // Initialize TPTP state (single thread)
         if (tid == 0 && tptp_enabled(&config)) {
             state->tda.betti[0] = 0.0f;
             state->tda.betti[1] = 0.0f;
             state->tda.betti[2] = 0.0f;
             state->tda.max_persistence = 0.0f;
             state->tda.stability_score = 0.0f;
             state->tda.transition_detected = 0;
             state->tda.betti_1_derivative = 0.0f;
         }

         // Initialize projection mappings for multigrid
         if (multigrid_enabled(&config)) {
             for (int i = tid; i < 512; i += blockDim.x) {
                 state->projection_L0_to_L1[i] = i / 4;
             }
             for (int i = tid; i < 128; i += blockDim.x) {
                 state->projection_L1_to_L2[i] = i / 4;
             }
             for (int i = tid; i < 32; i += blockDim.x) {
                 state->projection_L2_to_L3[i] = i / 4;
             }
         }

         // Initialize locks and work buffers
         for (int i = tid; i < MAX_VERTICES_PER_BLOCK; i += blockDim.x) {
             state->locks[i] = 0;
             state->move_deltas[i] = 0.0f;
             state->best_colors[i] = 0;
         }

         if (tid == 0) {
             state->num_conflict_vertices = 0;
         }

         block.sync();

         // 
         // PHASE 2: LOAD COLORING FROM GLOBAL MEMORY (Vectorized)
         // 

         // Coalesced loading of coloring
         for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
             int v = block_start + i;
             if (v < num_vertices) {
                 state->coloring_L0[i] = colors[v];
             }
         }

         block.sync();

         // 
         // PHASE 3: COMPUTE INITIAL CONFLICT SIGNALS
         // 

         for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
             int v = block_start + i;
             if (v >= num_vertices) continue;

             int my_color = state->coloring_L0[i];
             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             float conflict_count = 0.0f;

             // Count conflicts with neighbors
             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 int n_color;

                 // Check if neighbor is in this block's shared memory
                 if (neighbor >= block_start && neighbor < block_end) {
                     int local_idx = neighbor - block_start;
                     if (local_idx < MAX_VERTICES_PER_BLOCK) {
                         n_color = state->coloring_L0[local_idx];
                     } else {
                         n_color = colors[neighbor];
                     }
                 } else {
                     n_color = colors[neighbor];
                 }

                 if (n_color == my_color) {
                     conflict_count += 1.0f;
                 }
             }

             state->conflict_signal_L0[i] = conflict_count;
         }

         block.sync();

         // 
         // PHASE 4: WARP-SPECIALIZED EXECUTION
         // 

         // Assign specialized tasks to warps based on warp_id
         // Warp 0: Multigrid operations
         // Warp 1: Dendritic reservoir updates
         // Warp 2: Quantum evolution
         // Warp 3: Active inference
         // Warp 4-7: Parallel tempering replicas

         if (warp_id == 0 && multigrid_enabled(&config)) {
             // 
             // WARP 0: W-CYCLE MULTIGRID
             // 

             // Pre-smoothing at fine level
             for (int s = 0; s < config.pre_smooth_iterations; s++) {
                 smooth_iteration(state, 0, &config, block);
             }

             // Restriction cascade: L0  L1  L2  L3
             for (int level = 0; level < config.num_levels - 1; level++) {
                 restrict_to_coarse(state, level, &config);
             }

             // Coarsest level direct solve
             if (lane_id < 8) {
                 int v = lane_id;
                 // Simple greedy coloring at coarsest level
                 bool conflicted = state->conflict_signal_L3[v] > 0.0f;
                 if (conflicted) {
                     state->coloring_L3[v] = (state->coloring_L3[v] + 1) % MAX_COLORS;
                 }
             }

             // Prolongation cascade: L3  L2  L1  L0
             for (int level = config.num_levels - 2; level >= 0; level--) {
                 prolongate_to_fine(state, level, &config, &local_rng);

                 // Post-smoothing
                 for (int s = 0; s < config.post_smooth_iterations; s++) {
                     smooth_iteration(state, level, &config, block);
                 }
             }
         }

         block.sync();

         if (warp_id == 1 && dendritic_enabled(&config)) {
             // 
             // WARP 1: DENDRITIC RESERVOIR UPDATE
             // 

             for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
                 dendritic_update(state, i, &config);

                 // Compute neuromorphic priority
                 float priority = 0.0f;
                 priority += state->dendrite[i].activation[0] * 2.0f;
                 for (int b = 1; b < NUM_BRANCHES; b++) {
                     priority += state->dendrite[i].activation[b] * config.branch_weights[b];
                 }
                 priority += state->dendrite[i].calcium * 3.0f;
                 priority += state->soma_potential[i] * 0.5f;

                 // Store priority for WHCR weighting
                 state->move_deltas[i] = priority;
             }
         }

         block.sync();

         if (warp_id == 2 && quantum_enabled(&config)) {
             // 
             // WARP 2: QUANTUM EVOLUTION & TUNNELING
             // 

             for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
                 // Evolve quantum state
                 quantum_evolve(state, i, &config);

                 // Check for quantum tunneling event
                 if (should_tunnel(state, i, &config, &local_rng)) {
                     // Collapse wavefunction to highest amplitude state
                     float max_prob = 0.0f;
                     int best_state = 0;

                     for (int s = 0; s < NUM_QUANTUM_STATES; s++) {
                         float real = state->quantum[i].amplitude_real[s];
                         float imag = state->quantum[i].amplitude_imag[s];
                         float prob = real * real + imag * imag;

                         if (prob > max_prob) {
                             max_prob = prob;
                             best_state = s;
                         }
                     }

                     // Tunnel to new color
                     int new_color = state->quantum[i].color_idx[best_state];
                     state->coloring_L0[i] = new_color;

                     // Atomic increment of tunneling events
                     if (gid == blockIdx.x * blockDim.x) {
                         atomicAdd(&telemetry->tunneling_events, 1);
                     }
                 }
             }
         }

         block.sync();

         if (warp_id == 3 && active_inference_enabled(&config)) {
             // 
             // WARP 3: ACTIVE INFERENCE BELIEF UPDATE
             // 

             for (int i = lane_id; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += 32) {
                 active_inference_update(state, i, &config);
             }
         }

         block.sync();

         if (warp_id >= 4 && warp_id < 8 && tempering_enabled(&config)) {
             // 
             // WARPS 4-7: PARALLEL TEMPERING (4 replicas per warp)
             // 

             int replica_id = (warp_id - 4) * 3 + (lane_id / 11);
             if (replica_id < config.num_replicas && replica_id < NUM_REPLICAS) {
                 tempering_step(state, replica_id, &config, &local_rng);
             }
         }

         block.sync();

         // 
         // PHASE 5: TPTP PERSISTENT HOMOLOGY (Single thread)
         // 

         if (tid == 0 && tptp_enabled(&config)) {
             tptp_update(state, &config);

             if (state->tda.transition_detected) {
                 atomicAdd(&telemetry->phase_transitions, 1);
             }
         }

         block.sync();

         // 
         // PHASE 6: REPLICA EXCHANGE (Parallel Tempering)
         // 

         if (tid == 0 && tempering_enabled(&config)) {
             if (iteration % config.swap_interval == 0) {
                 replica_exchange(state, &config, &local_rng);
             }
         }

         block.sync();

         // 
         // PHASE 7: WHCR CONFLICT REPAIR (All threads)
         // 

         // Identify conflict vertices
         if (tid == 0) {
             state->num_conflict_vertices = 0;
         }
         block.sync();

         for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
             if (state->conflict_signal_L0[i] > 0.5f) {
                 int idx = atomicAdd(&state->num_conflict_vertices, 1);
                 if (idx < MAX_VERTICES_PER_BLOCK) {
                     state->conflict_vertices[idx] = i;
                 }
             }
         }
         block.sync();

         // Evaluate best moves for conflict vertices
         int num_cv = min(state->num_conflict_vertices, MAX_VERTICES_PER_BLOCK);
         for (int cv_idx = tid; cv_idx < num_cv; cv_idx += blockDim.x) {
             int i = state->conflict_vertices[cv_idx];
             int v = block_start + i;
             if (v >= num_vertices) continue;

             int current_color = state->coloring_L0[i];

             // Count neighbor color histogram
             int neighbor_colors[MAX_COLORS];
             for (int c = 0; c < MAX_COLORS; c++) neighbor_colors[c] = 0;

             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 int n_color;

                 if (neighbor >= block_start && neighbor < block_end) {
                     int local_idx = neighbor - block_start;
                     if (local_idx < MAX_VERTICES_PER_BLOCK) {
                         n_color = state->coloring_L0[local_idx];
                     } else {
                         n_color = colors[neighbor];
                     }
                 } else {
                     n_color = colors[neighbor];
                 }

                 if (n_color >= 0 && n_color < MAX_COLORS) {
                     neighbor_colors[n_color]++;
                 }
             }

             // Find best color with multi-objective scoring
             int current_conflicts = neighbor_colors[current_color];
             float best_score = 1e9f;
             int best_color = current_color;

             for (int c = 0; c < MAX_COLORS; c++) {
                 if (c == current_color) continue;

                 float score = 0.0f;

                 // Primary: conflict reduction
                 score += (float)neighbor_colors[c] * config.stress_weight;

                 // Chemical potential (prefer lower colors)
                 score += config.chemical_potential * ((float)c / (float)MAX_COLORS);

                 // Active inference belief guidance
                 if (active_inference_enabled(&config) && c < 16 && current_color < 16) {
                     float belief_diff = state->belief_distribution[i][current_color] -
                                        state->belief_distribution[i][c];
                     score += config.belief_weight * belief_diff;
                 }

                 // Dendritic reservoir priority
                 if (dendritic_enabled(&config)) {
                     score += config.stress_weight * state->move_deltas[i] * 0.1f;
                 }

                 // TPTP persistence weight (boost moves near phase transitions)
                 if (tptp_enabled(&config) && state->tda.transition_detected) {
                     score *= (1.0f + config.persistence_weight * 0.5f);
                 }

                 if (score < best_score) {
                     best_score = score;
                     best_color = c;
                 }
             }

             state->best_colors[i] = best_color;
             state->move_deltas[i] = best_score;
         }
         block.sync();

         // Apply moves with atomic locking
         for (int cv_idx = tid; cv_idx < num_cv; cv_idx += blockDim.x) {
             int i = state->conflict_vertices[cv_idx];
             int new_color = state->best_colors[i];

             if (new_color == state->coloring_L0[i]) continue;

             // Metropolis acceptance (simulated annealing)
             float delta = state->move_deltas[i];
             float temp = config.global_temperature;

             bool accept = (delta <= 0.0f) ||
                          (curand_uniform(&local_rng) < expf(-delta / fmaxf(EPSILON, temp)));

             if (accept) {
                 // Acquire lock
                 if (atomicCAS(&state->locks[i], 0, 1) == 0) {
                     state->coloring_L0[i] = new_color;
                     atomicExch(&state->locks[i], 0);

                     if (gid == blockIdx.x * blockDim.x) {
                         atomicAdd(&telemetry->moves_applied, 1);
                     }
                 }
             }
         }

         block.sync();

         // 
         // PHASE 8: WRITE BACK TO GLOBAL MEMORY (Vectorized)
         // 

         // Coalesced write-back of coloring
         for (int i = tid; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i += blockDim.x) {
             int v = block_start + i;
             if (v < num_vertices) {
                 colors[v] = state->coloring_L0[i];
             }
         }

         // Save RNG state
         rng_states[gid] = local_rng;

         // 
         // PHASE 9: GLOBAL TELEMETRY COLLECTION (Grid-wide reduction)
         // 

         grid.sync(); // Cooperative kernel synchronization

         // Block 0 computes global telemetry
         if (bid == 0 && tid == 0) {
             int total_conflicts = 0;
             int max_color = 0;

             for (int v = 0; v < num_vertices; v++) {
                 int c = colors[v];
                 if (c > max_color) max_color = c;

                 int start = row_ptr[v];
                 int end = row_ptr[v + 1];

                 for (int e = start; e < end; e++) {
                     if (colors[col_idx[e]] == c) {
                         total_conflicts++;
                     }
                 }
             }

             telemetry->conflicts = total_conflicts / 2;
             telemetry->colors_used = max_color + 1;

             // Copy TPTP metrics
             telemetry->betti_numbers[0] = state->tda.betti[0];
             telemetry->betti_numbers[1] = state->tda.betti[1];
             telemetry->betti_numbers[2] = state->tda.betti[2];

             // Compute average reservoir activity
             float total_activity = 0.0f;
             int active_count = 0;
             for (int i = 0; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i++) {
                 if (state->spike_history[i] > 0.1f) {
                     total_activity += state->spike_history[i];
                     active_count++;
                 }
             }
             telemetry->reservoir_activity = (active_count > 0) ?
                 (total_activity / (float)active_count) : 0.0f;

             // Compute average free energy
             float total_fe = 0.0f;
             for (int i = 0; i < min(block_vertex_count, MAX_VERTICES_PER_BLOCK); i++) {
                 total_fe += state->expected_free_energy[i];
             }
             telemetry->free_energy = total_fe / fmaxf(1.0f,
                 (float)min(block_vertex_count, MAX_VERTICES_PER_BLOCK));

             // Find best replica for parallel tempering
             if (tempering_enabled(&config)) {
                 int best_replica = 0;
                 int min_conflicts = INT_MAX;
                 for (int r = 0; r < min(config.num_replicas, NUM_REPLICAS); r++) {
                     if (state->replica[r].conflicts < min_conflicts) {
                         min_conflicts = state->replica[r].conflicts;
                         best_replica = r;
                     }
                 }
                 telemetry->best_replica = best_replica;
             }
         }
     }

     // 
     // HELPER KERNELS
     // 

     /**
      * ultra_init_kernel - Initialize all GPU state for optimization
      */
     extern "C" __global__ void ultra_init_kernel(
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int* __restrict__ colors,
         int* __restrict__ best_colors,
         int num_vertices,
         int num_edges,
         unsigned long long seed
     ) {
         int gid = blockIdx.x * blockDim.x + threadIdx.x;

         curandState rng;
         curand_init(seed, gid, 0, &rng);

         // Greedy coloring initialization
         for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
             int used_colors[64] = {0};

             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             // Mark neighbor colors
             for (int e = start; e < end; e++) {
                 int neighbor = col_idx[e];
                 if (neighbor < v) {
                     int n_color = colors[neighbor];
                     if (n_color >= 0 && n_color < 64) {
                         used_colors[n_color] = 1;
                     }
                 }
             }

             // Find first available color
             int assigned_color = 0;
             for (int c = 0; c < 64; c++) {
                 if (!used_colors[c]) {
                     assigned_color = c;
                     break;
                 }
             }

             colors[v] = assigned_color;
             best_colors[v] = assigned_color;
         }
     }

     /**
      * ultra_finalize_kernel - Final validation and compaction
      */
     extern "C" __global__ void ultra_finalize_kernel(
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         int* __restrict__ colors,
         int* __restrict__ conflict_map,
         int num_vertices
     ) {
         int gid = blockIdx.x * blockDim.x + threadIdx.x;

         for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
             int my_color = colors[v];
             int conflicts = 0;

             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             for (int e = start; e < end; e++) {
                 if (colors[col_idx[e]] == my_color) {
                     conflicts++;
                 }
             }

             conflict_map[v] = conflicts;
         }
     }

     /**
      * ultra_telemetry_kernel - Comprehensive telemetry collection
      */
     extern "C" __global__ void ultra_telemetry_kernel(
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const int* __restrict__ colors,
         KernelTelemetry* __restrict__ telemetry,
         int num_vertices
     ) {
         __shared__ int s_conflicts;
         __shared__ int s_max_color;

         int tid = threadIdx.x;
         int gid = blockIdx.x * blockDim.x + threadIdx.x;

         if (tid == 0) {
             s_conflicts = 0;
             s_max_color = 0;
         }
         __syncthreads();

         // Accumulate metrics
         for (int v = gid; v < num_vertices; v += blockDim.x * gridDim.x) {
             int my_color = colors[v];
             atomicMax(&s_max_color, my_color);

             int start = row_ptr[v];
             int end = row_ptr[v + 1];

             int vertex_conflicts = 0;
             for (int e = start; e < end; e++) {
                 if (colors[col_idx[e]] == my_color) {
                     vertex_conflicts++;
                 }
             }

             atomicAdd(&s_conflicts, vertex_conflicts);
         }
         __syncthreads();

         if (tid == 0) {
             atomicAdd(&telemetry->conflicts, s_conflicts / 2);
             atomicMax(&telemetry->colors_used, s_max_color + 1);
         }
     }

     // 
     // END OF PART 3: ULTRA KERNEL ENTRY POINTS & ORCHESTRATION
     // /**
      * TPTP: Topological Phase Transition Prediction
      *
      * Live persistent homology computation for detecting phase boundaries
      * in the optimization landscape.
      *
      * Copyright (c) 2024 PRISM Research Team | Delfictus I/O Inc.
      * Los Angeles, CA 90013
      * Contact: IS@Delfictus.com
      * All Rights Reserved.
      */

     #include <cuda_runtime.h>
     #include <math_constants.h>
     #include "runtime_config.cuh"

     #define BLOCK_SIZE 256
     #define MAX_SIMPLICES 4096
     #define MAX_PERSISTENCE_PAIRS 1024
     #define MAX_FILTRATION_STEPS 64
     #define MAX_VERTICES_LOCAL 512

     // Simplex types
     #define SIMPLEX_VERTEX 0
     #define SIMPLEX_EDGE 1
     #define SIMPLEX_TRIANGLE 2

     /**
      * Simplex structure for persistent homology
      */
     struct Simplex {
         int type;               // 0=vertex, 1=edge, 2=triangle
         int vertices[3];        // Vertex indices (up to 3 for triangle)
         float filtration_value; // When simplex appears in filtration
         int boundary[3];        // Indices of boundary simplices
         int num_boundary;       // Number of boundary simplices
         int paired_with;        // Index of destroying/creating simplex (-1 if unpaired)
         float birth_time;       // Birth time in filtration
         float death_time;       // Death time in filtration
     };

     /**
      * Persistence pair (birth-death pair)
      */
     struct PersistencePair {
         float birth;
         float death;
         int dimension;          // 0, 1, or 2
         int creator_simplex;    // Simplex index that creates this feature
         int destroyer_simplex;  // Simplex index that destroys this feature
         float persistence;      // death - birth
     };

     /**
      * TPTP State - Persistent homology state
      */
     struct TPTPState {
         // Simplicial complex
         Simplex simplices[MAX_SIMPLICES];
         int num_simplices;

         // Persistence pairs
         PersistencePair pairs[MAX_PERSISTENCE_PAIRS];
         int num_pairs;

         // Betti numbers (0, 1, 2)
         float betti[3];

         // Historical Betti numbers for derivative computation
         float betti_history[MAX_FILTRATION_STEPS][3];
         int history_idx;
         int history_filled;

         // Persistence statistics
         float max_persistence;
         float avg_persistence;
         float persistence_entropy;
         float total_persistence;

         // Phase transition detection
         int phase_transition_detected;
         float transition_strength;
         float betti_0_derivative;
         float betti_1_derivative;
         float betti_2_derivative;

         // Stability metrics
         float stability_score;
         int stable_iterations;
         int unstable_iterations;

         // Filtration parameters
         float current_filtration;
         float filtration_step;
         int filtration_index;
     };

     // 
     // DEVICE HELPER FUNCTIONS
     // 

     /**
      * Compare two simplices by filtration value (for sorting)
      */
     __device__ int compare_simplices(const Simplex* a, const Simplex* b) {
         if (a->filtration_value < b->filtration_value) return -1;
         if (a->filtration_value > b->filtration_value) return 1;

         // Tie-break by dimension (lower dimension first)
         if (a->type < b->type) return -1;
         if (a->type > b->type) return 1;

         return 0;
     }

     /**
      * Check if two vertices are connected in the graph
      */
     __device__ bool are_connected(
         int v1, int v2,
         const int* graph_row_ptr,
         const int* graph_col_idx
     ) {
         int start = graph_row_ptr[v1];
         int end = graph_row_ptr[v1 + 1];

         for (int e = start; e < end; e++) {
             if (graph_col_idx[e] == v2) {
                 return true;
             }
         }

         return false;
     }

     /**
      * Check if three vertices form a triangle in the graph
      */
     __device__ bool forms_triangle(
         int v0, int v1, int v2,
         const int* graph_row_ptr,
         const int* graph_col_idx
     ) {
         return are_connected(v0, v1, graph_row_ptr, graph_col_idx) &&
                are_connected(v1, v2, graph_row_ptr, graph_col_idx) &&
                are_connected(v2, v0, graph_row_ptr, graph_col_idx);
     }

     /**
      * Compute entropy of persistence diagram
      */
     __device__ float compute_persistence_entropy(
         const PersistencePair* pairs,
         int num_pairs
     ) {
         if (num_pairs == 0) return 0.0f;

         // Normalize persistence values to probabilities
         float total = 0.0f;
         for (int i = 0; i < num_pairs; i++) {
             total += pairs[i].persistence;
         }

         if (total < 1e-8f) return 0.0f;

         // Compute Shannon entropy
         float entropy = 0.0f;
         for (int i = 0; i < num_pairs; i++) {
             float p = pairs[i].persistence / total;
             if (p > 1e-8f) {
                 entropy -= p * logf(p);
             }
         }

         return entropy;
     }

     /**
      * Merge two components in union-find structure
      */
     __device__ void union_components(
         int* parent,
         int* rank,
         int v1, int v2
     ) {
         // Find roots with path compression
         int root1 = v1;
         while (parent[root1] != root1) {
             int next = parent[root1];
             parent[root1] = parent[next];
             root1 = next;
         }

         int root2 = v2;
         while (parent[root2] != root2) {
             int next = parent[root2];
             parent[root2] = parent[next];
             root2 = next;
         }

         if (root1 == root2) return;

         // Union by rank
         if (rank[root1] < rank[root2]) {
             parent[root1] = root2;
         } else if (rank[root1] > rank[root2]) {
             parent[root2] = root1;
         } else {
             parent[root2] = root1;
             rank[root1]++;
         }
     }

     /**
      * Find root of component in union-find
      */
     __device__ int find_root(int* parent, int v) {
         int root = v;
         while (parent[root] != root) {
             int next = parent[root];
             parent[root] = parent[next];
             root = next;
         }
         return root;
     }

     // 
     // KERNEL FUNCTIONS
     // 

     /**
      * Build Vietoris-Rips complex from conflict graph
      *
      * This kernel constructs a simplicial complex where:
      * - 0-simplices (vertices) appear at filtration value = conflict_signal[v]
      * - 1-simplices (edges) appear when both endpoints are in the complex
      * - 2-simplices (triangles) appear when all edges are in the complex
      */
     extern "C" __global__ void tptp_build_complex(
         const int* __restrict__ graph_row_ptr,
         const int* __restrict__ graph_col_idx,
         const float* __restrict__ conflict_signal,
         TPTPState* __restrict__ state,
         const RuntimeConfig* __restrict__ config,
         int num_vertices
     ) {
         __shared__ int simplex_count;
         __shared__ int edge_count;
         __shared__ int triangle_count;

         int tid = threadIdx.x;
         int gid = blockIdx.x * blockDim.x + threadIdx.x;

         // Initialize counters
         if (tid == 0) {
             simplex_count = 0;
             edge_count = 0;
             triangle_count = 0;
         }
         __syncthreads();

         // 
         // PHASE 1: Add vertices as 0-simplices
         // 

         if (gid < num_vertices) {
             int idx = atomicAdd(&simplex_count, 1);
             if (idx < MAX_SIMPLICES) {
                 Simplex* s = &state->simplices[idx];
                 s->type = SIMPLEX_VERTEX;
                 s->vertices[0] = gid;
                 s->vertices[1] = -1;
                 s->vertices[2] = -1;
                 s->filtration_value = conflict_signal[gid];
                 s->num_boundary = 0;
                 s->paired_with = -1;
                 s->birth_time = conflict_signal[gid];
                 s->death_time = CUDART_INF_F;
             }
         }
         __syncthreads();

         int vertex_count = simplex_count;

         // 
         // PHASE 2: Add edges as 1-simplices
         // 

         if (gid < num_vertices) {
             int start = graph_row_ptr[gid];
             int end = graph_row_ptr[gid + 1];

             for (int e = start; e < end; e++) {
                 int neighbor = graph_col_idx[e];

                 // Avoid duplicates: only add edge if gid < neighbor
                 if (neighbor > gid) {
                     int idx = atomicAdd(&simplex_count, 1);
                     if (idx < MAX_SIMPLICES) {
                         Simplex* s = &state->simplices[idx];
                         s->type = SIMPLEX_EDGE;
                         s->vertices[0] = gid;
                         s->vertices[1] = neighbor;
                         s->vertices[2] = -1;

                         // Filtration value is max of endpoint conflicts
                         float filt = fmaxf(conflict_signal[gid], conflict_signal[neighbor]);
                         s->filtration_value = filt;

                         // Boundary is the two vertices
                         s->num_boundary = 2;
                         s->boundary[0] = gid;          // Index in simplex array
                         s->boundary[1] = neighbor;
                         s->boundary[2] = -1;

                         s->paired_with = -1;
                         s->birth_time = filt;
                         s->death_time = CUDART_INF_F;

                         atomicAdd(&edge_count, 1);
                     }
                 }
             }
         }
         __syncthreads();

         int edge_start = vertex_count;
         int edge_end = simplex_count;

         // 
         // PHASE 3: Add triangles as 2-simplices
         // 

         // For each edge, check if it forms triangles with other vertices
         for (int edge_idx = edge_start + tid; edge_idx < edge_end; edge_idx += blockDim.x) {
             if (edge_idx >= MAX_SIMPLICES) break;

             Simplex* edge = &state->simplices[edge_idx];
             int v0 = edge->vertices[0];
             int v1 = edge->vertices[1];

             // Check all neighbors of v0 to see if they complete a triangle
             int start = graph_row_ptr[v0];
             int end = graph_row_ptr[v0 + 1];

             for (int e = start; e < end; e++) {
                 int v2 = graph_col_idx[e];

                 // Only add if v2 > v1 to avoid duplicates
                 if (v2 > v1 && forms_triangle(v0, v1, v2, graph_row_ptr, graph_col_idx)) {
                     int idx = atomicAdd(&simplex_count, 1);
                     if (idx < MAX_SIMPLICES) {
                         Simplex* s = &state->simplices[idx];
                         s->type = SIMPLEX_TRIANGLE;
                         s->vertices[0] = v0;
                         s->vertices[1] = v1;
                         s->vertices[2] = v2;

                         // Filtration value is max of vertex conflicts
                         float filt = fmaxf(fmaxf(conflict_signal[v0], conflict_signal[v1]),
                                            conflict_signal[v2]);
                         s->filtration_value = filt;

                         // Boundary is the three edges (simplified - just store vertices)
                         s->num_boundary = 3;
                         s->boundary[0] = v0;
                         s->boundary[1] = v1;
                         s->boundary[2] = v2;

                         s->paired_with = -1;
                         s->birth_time = filt;
                         s->death_time = CUDART_INF_F;

                         atomicAdd(&triangle_count, 1);
                     }
                 }
             }
         }
         __syncthreads();

         // Store final counts
         if (tid == 0) {
             state->num_simplices = min(simplex_count, MAX_SIMPLICES);
         }
     }

     /**
      * Compute persistent homology via simplified matrix reduction
      *
      * This implements a simplified version of the standard persistence algorithm.
      * Full production version would implement optimized matrix reduction.
      */
     extern "C" __global__ void tptp_compute_homology(
         TPTPState* __restrict__ state,
         const RuntimeConfig* __restrict__ config
     ) {
         int tid = threadIdx.x;

         __shared__ int parent[MAX_VERTICES_LOCAL];
         __shared__ int rank[MAX_VERTICES_LOCAL];
         __shared__ float betti_local[3];

         // Initialize for union-find
         for (int i = tid; i < MAX_VERTICES_LOCAL; i += blockDim.x) {
             parent[i] = i;
             rank[i] = 0;
         }

         if (tid == 0) {
             betti_local[0] = 0.0f;
             betti_local[1] = 0.0f;
             betti_local[2] = 0.0f;
         }
         __syncthreads();

         // 
         // PHASE 1: Compute 0 (connected components) via union-find
         // 

         if (tid == 0) {
             int num_vertices = 0;
             int num_edges = 0;

             // Count vertices
             for (int i = 0; i < state->num_simplices; i++) {
                 if (state->simplices[i].type == SIMPLEX_VERTEX) {
                     num_vertices++;
                 }
             }

             // Process edges in filtration order to track component merging
             for (int i = 0; i < state->num_simplices; i++) {
                 Simplex* s = &state->simplices[i];

                 if (s->type == SIMPLEX_EDGE) {
                     int v0 = s->vertices[0];
                     int v1 = s->vertices[1];

                     if (v0 < MAX_VERTICES_LOCAL && v1 < MAX_VERTICES_LOCAL) {
                         int root0 = find_root(parent, v0);
                         int root1 = find_root(parent, v1);

                         if (root0 != root1) {
                             // Merging components - this edge creates no cycle
                             union_components(parent, rank, v0, v1);
                         } else {
                             // Edge creates a cycle (1-dimensional feature)
                             num_edges++;
                         }
                     }
                 }
             }

             // Count connected components
             int components = 0;
             for (int i = 0; i < num_vertices && i < MAX_VERTICES_LOCAL; i++) {
                 if (parent[i] == i) {
                     components++;
                 }
             }

             betti_local[0] = (float)components;
             betti_local[1] = (float)num_edges;
             betti_local[2] = 0.0f; // Simplified - would need full homology computation
         }
         __syncthreads();

         // 
         // PHASE 2: Store Betti numbers and history
         // 

         if (tid == 0) {
             state->betti[0] = betti_local[0];
             state->betti[1] = betti_local[1];
             state->betti[2] = betti_local[2];

             // Store in history
             int idx = state->history_idx;
             state->betti_history[idx][0] = betti_local[0];
             state->betti_history[idx][1] = betti_local[1];
             state->betti_history[idx][2] = betti_local[2];

             state->history_idx = (idx + 1) % MAX_FILTRATION_STEPS;
             if (state->history_filled < MAX_FILTRATION_STEPS) {
                 state->history_filled++;
             }
         }
         __syncthreads();

         // 
         // PHASE 3: Compute persistence pairs (simplified)
         // 

         if (tid == 0) {
             int pair_count = 0;
             float total_pers = 0.0f;
             float max_pers = 0.0f;

             // Create persistence pairs from simplices
             for (int i = 0; i < state->num_simplices && pair_count < MAX_PERSISTENCE_PAIRS; i++) {
                 Simplex* s = &state->simplices[i];

                 if (s->type == SIMPLEX_EDGE && s->birth_time < CUDART_INF_F) {
                     PersistencePair* pair = &state->pairs[pair_count++];
                     pair->birth = s->birth_time;
                     pair->death = s->death_time;
                     pair->dimension = 1;
                     pair->creator_simplex = i;
                     pair->destroyer_simplex = -1;

                     float pers = (s->death_time < CUDART_INF_F) ?
                                 (s->death_time - s->birth_time) : 0.0f;
                     pair->persistence = pers;

                     total_pers += pers;
                     if (pers > max_pers) max_pers = pers;
                 }
             }

             state->num_pairs = pair_count;
             state->max_persistence = max_pers;
             state->avg_persistence = (pair_count > 0) ? (total_pers / pair_count) : 0.0f;
             state->total_persistence = total_pers;

             // Compute persistence entropy
             state->persistence_entropy = compute_persistence_entropy(
                 state->pairs,
                 state->num_pairs
             );
         }
     }

     /**
      * Detect phase transitions from Betti number dynamics
      *
      * Phase transitions manifest as:
      * 1. Sudden changes in Betti numbers
      * 2. Spikes in persistence
      * 3. Changes in topological stability
      */
     extern "C" __global__ void tptp_detect_transition(
         TPTPState* __restrict__ state,
         const RuntimeConfig* __restrict__ config
     ) {
         int tid = threadIdx.x;

         if (tid == 0) {
             // Need at least 2 history entries to compute derivatives
             if (state->history_filled < 2) {
                 state->phase_transition_detected = 0;
                 state->transition_strength = 0.0f;
                 return;
             }

             // 
             // PHASE 1: Compute Betti number derivatives
             // 

             int curr_idx = (state->history_idx - 1 + MAX_FILTRATION_STEPS) % MAX_FILTRATION_STEPS;
             int prev_idx = (state->history_idx - 2 + MAX_FILTRATION_STEPS) % MAX_FILTRATION_STEPS;

             float db0 = state->betti_history[curr_idx][0] - state->betti_history[prev_idx][0];
             float db1 = state->betti_history[curr_idx][1] - state->betti_history[prev_idx][1];
             float db2 = state->betti_history[curr_idx][2] - state->betti_history[prev_idx][2];

             state->betti_0_derivative = db0;
             state->betti_1_derivative = db1;
             state->betti_2_derivative = db2;

             // 
             // PHASE 2: Compute transition score
             // 

             float transition_score = 0.0f;

             // Indicator 1: Sudden drop in 0 (components merging rapidly)
             if (db0 < -config->betti_0_threshold) {
                 transition_score += fabsf(db0) * 2.0f;
             }

             // Indicator 2: Spike in 1 (cycles forming/breaking)
             if (fabsf(db1) > config->betti_1_threshold) {
                 transition_score += fabsf(db1) * 3.0f;
             }

             // Indicator 3: Change in 2 (voids appearing/disappearing)
             if (fabsf(db2) > config->betti_2_threshold) {
                 transition_score += fabsf(db2) * 1.5f;
             }

             // Indicator 4: High maximum persistence
             if (state->max_persistence > config->persistence_threshold) {
                 transition_score += state->max_persistence;
             }

             // Indicator 5: Sudden change in persistence entropy
             if (state->history_filled >= 2) {
                 // Could compute entropy derivative here
                 float entropy_weight = state->persistence_entropy * 0.1f;
                 transition_score += entropy_weight;
             }

             // 
             // PHASE 3: Update stability metrics
             // 

             // Stability score based on variance of recent Betti numbers
             float variance = 0.0f;
             int window = min(state->history_filled, config->stability_window);

             if (window > 1) {
                 float mean_b1 = 0.0f;
                 for (int i = 0; i < window; i++) {
                     int idx = (state->history_idx - i - 1 + MAX_FILTRATION_STEPS) % MAX_FILTRATION_STEPS;
                     mean_b1 += state->betti_history[idx][1];
                 }
                 mean_b1 /= window;

                 for (int i = 0; i < window; i++) {
                     int idx = (state->history_idx - i - 1 + MAX_FILTRATION_STEPS) % MAX_FILTRATION_STEPS;
                     float diff = state->betti_history[idx][1] - mean_b1;
                     variance += diff * diff;
                 }
                 variance /= window;
             }

             state->stability_score = 1.0f / (1.0f + variance);

             // Track stable/unstable iterations
             if (variance < 0.1f) {
                 state->stable_iterations++;
                 state->unstable_iterations = 0;
             } else {
                 state->unstable_iterations++;
                 state->stable_iterations = 0;
             }

             // 
             // PHASE 4: Make transition detection decision
             // 

             state->transition_strength = transition_score;
             state->phase_transition_detected =
                 (transition_score > config->transition_sensitivity) ? 1 : 0;
         }
     }

     /**
      * Combined TPTP update kernel - runs all phases sequentially
      */
     extern "C" __global__ void tptp_full_update(
         const int* __restrict__ graph_row_ptr,
         const int* __restrict__ graph_col_idx,
         const float* __restrict__ conflict_signal,
         TPTPState* __restrict__ state,
         const RuntimeConfig* __restrict__ config,
         int num_vertices
     ) {
         // In production, this would orchestrate the full TPTP pipeline
         // For now, the phases should be called as separate kernels for better occupancy

         // Phase 1: Build complex (done in tptp_build_complex)
         // Phase 2: Compute homology (done in tptp_compute_homology)
         // Phase 3: Detect transitions (done in tptp_detect_transition)

         // This kernel serves as a placeholder for future fusion optimization
     }

     /**
      * Reset TPTP state for new optimization run
      */
     extern "C" __global__ void tptp_reset_state(
         TPTPState* __restrict__ state
     ) {
         int tid = threadIdx.x;

         if (tid == 0) {
             state->num_simplices = 0;
             state->num_pairs = 0;
             state->betti[0] = 0.0f;
             state->betti[1] = 0.0f;
             state->betti[2] = 0.0f;
             state->history_idx = 0;
             state->history_filled = 0;
             state->max_persistence = 0.0f;
             state->avg_persistence = 0.0f;
             state->persistence_entropy = 0.0f;
             state->total_persistence = 0.0f;
             state->phase_transition_detected = 0;
             state->transition_strength = 0.0f;
             state->betti_0_derivative = 0.0f;
             state->betti_1_derivative = 0.0f;
             state->betti_2_derivative = 0.0f;
             state->stability_score = 1.0f;
             state->stable_iterations = 0;
             state->unstable_iterations = 0;
             state->current_filtration = 0.0f;
             state->filtration_step = 0.1f;
             state->filtration_index = 0;
         }

         // Clear arrays in parallel
         for (int i = tid; i < MAX_SIMPLICES; i += blockDim.x) {
             state->simplices[i].type = -1;
             state->simplices[i].paired_with = -1;
         }

         for (int i = tid; i < MAX_PERSISTENCE_PAIRS; i += blockDim.x) {
             state->pairs[i].birth = 0.0f;
             state->pairs[i].death = 0.0f;
             state->pairs[i].persistence = 0.0f;
         }

         for (int i = tid; i < MAX_FILTRATION_STEPS; i += blockDim.x) {
             state->betti_history[i][0] = 0.0f;
             state->betti_history[i][1] = 0.0f;
             state->betti_history[i][2] = 0.0f;
         }
     }

     /**
      * Extract topological features for FluxNet RL state
      */
     extern "C" __global__ void tptp_extract_features(
         const TPTPState* __restrict__ state,
         float* __restrict__ features,
         int num_features
     ) {
         int tid = threadIdx.x;

         if (tid == 0 && num_features >= 16) {
             // Feature vector for RL state
             features[0] = state->betti[0];                  // 0
             features[1] = state->betti[1];                  // 1
             features[2] = state->betti[2];                  // 2
             features[3] = state->betti_0_derivative;        // d0/dt
             features[4] = state->betti_1_derivative;        // d1/dt
             features[5] = state->betti_2_derivative;        // d2/dt
             features[6] = state->max_persistence;           // Max persistence
             features[7] = state->avg_persistence;           // Avg persistence
             features[8] = state->persistence_entropy;       // Entropy
             features[9] = state->stability_score;           // Stability
             features[10] = (float)state->phase_transition_detected;
             features[11] = state->transition_strength;
             features[12] = (float)state->stable_iterations / 100.0f;
             features[13] = (float)state->unstable_iterations / 100.0f;
             features[14] = (float)state->num_pairs / 100.0f;
             features[15] = state->total_persistence;
         }
     }// Transfer Entropy kernel using KSG estimator for causal discovery
     //
     // ASSUMPTIONS:
     // - Time series data stored as contiguous f32 arrays
     // - MAX_SERIES_LENGTH = 10000 (time points limit)
     // - MAX_VARIABLES = 256 (number of variables for causal graph)
     // - k-nearest neighbors: k = 4 (KSG standard)
     // - Precision: f32 for distance calculations
     // - Block size: 256 threads (optimal for RTX 3060)
     // - Grid size: ceil(num_pairs / threads_per_block)
     // - Requires: sm_70+ for efficient sorting
     // REFERENCE: PRISM Spec Section 5.3 "Causal Discovery via Transfer Entropy"

     #include <cuda_runtime.h>
     #include <cuda_fp16.h>
     #include <cmath>
     #include <float.h>

     // Configuration constants
     constexpr int MAX_SERIES_LENGTH = 10000;
     constexpr int MAX_VARIABLES = 256;
     constexpr int THREADS_PER_BLOCK = 256;
     constexpr int K_NEIGHBORS = 4;  // KSG k-nearest neighbors
     constexpr float EPSILON = 1e-10f;

     // Transfer entropy parameters
     struct TEParams {
         int num_variables;     // Number of time series variables
         int series_length;     // Length of each time series
         int history_length;    // Past values to consider (tau)
         int prediction_lag;    // Future prediction lag
         float noise_level;     // Small noise for numerical stability
         int bootstrap_samples; // Number of bootstrap samples for significance
     };

     // Device function: Compute Euclidean distance between embedded vectors
     __device__ float compute_embedding_distance(
         const float* __restrict__ x,
         const float* __restrict__ y,
         int dim
     ) {
         float dist = 0.0f;
         for (int i = 0; i < dim; ++i) {
             float diff = x[i] - y[i];
             dist += diff * diff;
         }
         return sqrtf(dist + EPSILON);
     }

     // Device function: Find k-nearest neighbors using partial sort
     __device__ void find_k_nearest_neighbors(
         const float* __restrict__ distances,
         int* __restrict__ indices,
         int n,
         int k,
         int exclude_idx
     ) {
         // Initialize indices
         for (int i = 0; i < k; ++i) {
             indices[i] = -1;
         }

         float max_dist = FLT_MAX;
         int max_idx = 0;

         // Find k smallest distances (excluding self)
         for (int i = 0; i < n; ++i) {
             if (i == exclude_idx) continue;

             float dist = distances[i];

             // Check if this distance should be in top-k
             if (dist < max_dist) {
                 // Replace the maximum distance in current k-set
                 indices[max_idx] = i;

                 // Find new maximum in k-set
                 max_dist = 0.0f;
                 for (int j = 0; j < k; ++j) {
                     if (indices[j] >= 0 && distances[indices[j]] > max_dist) {
                         max_dist = distances[indices[j]];
                         max_idx = j;
                     }
                 }
             }
         }
     }

     // Device function: Digamma function approximation
     __device__ float digamma(float x) {
         // Asymptotic expansion for digamma function
         if (x < 6.0f) {
             // Recursion to reach asymptotic region
             return digamma(x + 1.0f) - 1.0f / x;
         }

         // Asymptotic series
         float result = logf(x) - 0.5f / x;
         float x2 = x * x;
         result -= 1.0f / (12.0f * x2);
         result += 1.0f / (120.0f * x2 * x2);
         result -= 1.0f / (252.0f * x2 * x2 * x2);
         return result;
     }

     // Main Transfer Entropy computation kernel (KSG estimator)
     extern "C" __global__ void transfer_entropy_ksg_kernel(
         const float* __restrict__ time_series,    // [num_variables][series_length]
         float* __restrict__ te_matrix,            // [num_variables][num_variables] output
         float* __restrict__ significance,         // [num_variables][num_variables] p-values
         int* __restrict__ neighbor_counts,        // Workspace for neighbor counting
         TEParams params
     ) {
         // Each thread computes TE for one variable pair
         int pair_idx = blockIdx.x * blockDim.x + threadIdx.x;
         int total_pairs = params.num_variables * params.num_variables;
         if (pair_idx >= total_pairs) return;

         int source_var = pair_idx / params.num_variables;
         int target_var = pair_idx % params.num_variables;

         // Skip self-connections
         if (source_var == target_var) {
             te_matrix[pair_idx] = 0.0f;
             significance[pair_idx] = 1.0f;
             return;
         }

         // Embedding dimensions
         int dim_x = params.history_length;      // Past of source
         int dim_y = params.history_length;      // Past of target
         int dim_z = 1;                          // Future of target

         int joint_dim = dim_x + dim_y + dim_z;
         int marginal_dim = dim_y + dim_z;

         // Allocate shared memory for distance calculations
         extern __shared__ float shared_mem[];
         float* distances = &shared_mem[threadIdx.x * params.series_length];

         float te_sum = 0.0f;
         int valid_points = 0;

         // Main loop over time points
         for (int t = params.history_length;
              t < params.series_length - params.prediction_lag;
              ++t) {

             // Construct joint embedding [X_past, Y_past, Y_future]
             float joint_embedding[32]; // Max embedding dimension
             int embed_idx = 0;

             // X past values
             for (int tau = 0; tau < params.history_length; ++tau) {
                 joint_embedding[embed_idx++] =
                     time_series[source_var * params.series_length + t - tau];
             }

             // Y past values
             for (int tau = 0; tau < params.history_length; ++tau) {
                 joint_embedding[embed_idx++] =
                     time_series[target_var * params.series_length + t - tau];
             }

             // Y future value
             joint_embedding[embed_idx++] =
                 time_series[target_var * params.series_length + t + params.prediction_lag];

             // Compute distances to all other time points (joint space)
             for (int t2 = params.history_length;
                  t2 < params.series_length - params.prediction_lag;
                  ++t2) {

                 float other_embedding[32];
                 int idx = 0;

                 // Build comparison embedding
                 for (int tau = 0; tau < params.history_length; ++tau) {
                     other_embedding[idx++] =
                         time_series[source_var * params.series_length + t2 - tau];
                 }
                 for (int tau = 0; tau < params.history_length; ++tau) {
                     other_embedding[idx++] =
                         time_series[target_var * params.series_length + t2 - tau];
                 }
                 other_embedding[idx++] =
                     time_series[target_var * params.series_length + t2 + params.prediction_lag];

                 distances[t2] = compute_embedding_distance(
                     joint_embedding, other_embedding, joint_dim
                 );
             }

             // Find k-nearest neighbors in joint space
             int k_neighbors_joint[K_NEIGHBORS];
             find_k_nearest_neighbors(
                 distances,
                 k_neighbors_joint,
                 params.series_length - params.history_length - params.prediction_lag,
                 K_NEIGHBORS,
                 t
             );

             // Get distance to k-th neighbor
             float eps_joint = 0.0f;
             for (int k = 0; k < K_NEIGHBORS; ++k) {
                 if (k_neighbors_joint[k] >= 0) {
                     eps_joint = fmaxf(eps_joint, distances[k_neighbors_joint[k]]);
                 }
             }
             eps_joint += params.noise_level; // Numerical stability

             // Count neighbors in marginal spaces
             int n_yz = 0;  // Neighbors in (Y_past, Y_future) space
             int n_y = 0;   // Neighbors in Y_past space
             int n_xyz = K_NEIGHBORS; // Already have this

             for (int t2 = params.history_length;
                  t2 < params.series_length - params.prediction_lag;
                  ++t2) {
                 if (t2 == t) continue;

                 // Check marginal distance (Y_past, Y_future)
                 float dist_yz = 0.0f;
                 for (int tau = 0; tau < params.history_length; ++tau) {
                     float diff = time_series[target_var * params.series_length + t - tau] -
                                 time_series[target_var * params.series_length + t2 - tau];
                     dist_yz += diff * diff;
                 }
                 float diff_future = time_series[target_var * params.series_length + t + params.prediction_lag] -
                                    time_series[target_var * params.series_length + t2 + params.prediction_lag];
                 dist_yz += diff_future * diff_future;
                 dist_yz = sqrtf(dist_yz + EPSILON);

                 if (dist_yz < eps_joint) {
                     n_yz++;
                 }

                 // Check marginal distance (Y_past only)
                 float dist_y = 0.0f;
                 for (int tau = 0; tau < params.history_length; ++tau) {
                     float diff = time_series[target_var * params.series_length + t - tau] -
                                 time_series[target_var * params.series_length + t2 - tau];
                     dist_y += diff * diff;
                 }
                 dist_y = sqrtf(dist_y + EPSILON);

                 if (dist_y < eps_joint) {
                     n_y++;
                 }
             }

             // KSG estimator formula
             // TE(X->Y) = (k) - (n_yz+1) - (n_y+1) + (n_xyz+1)
             float te_point = digamma((float)K_NEIGHBORS) +
                             digamma((float)(n_y + 1)) -
                             digamma((float)(n_yz + 1)) -
                             digamma((float)(n_xyz + 1));

             te_sum += te_point;
             valid_points++;
         }

         // Average transfer entropy
         float te_value = (valid_points > 0) ? (te_sum / valid_points) : 0.0f;

         // Ensure non-negative (KSG can sometimes give small negative values)
         te_value = fmaxf(0.0f, te_value);

         // Store result
         te_matrix[pair_idx] = te_value;

         // Bootstrap significance test would go here
         // For now, use threshold-based significance
         significance[pair_idx] = (te_value > 0.01f) ? 0.05f : 1.0f;
     }

     // Conditional Transfer Entropy kernel (accounts for confounders)
     extern "C" __global__ void conditional_te_kernel(
         const float* __restrict__ time_series,
         const int* __restrict__ conditioning_vars, // Which variables to condition on
         float* __restrict__ cte_matrix,
         TEParams params,
         int num_conditioning
     ) {
         int pair_idx = blockIdx.x * blockDim.x + threadIdx.x;
         if (pair_idx >= params.num_variables * params.num_variables) return;

         int source = pair_idx / params.num_variables;
         int target = pair_idx % params.num_variables;

         if (source == target) {
             cte_matrix[pair_idx] = 0.0f;
             return;
         }

         // Extended embedding includes conditioning variables
         int total_dim = params.history_length * (2 + num_conditioning) + 1;

         // Simplified CTE calculation
         // Full implementation would follow similar pattern to main TE kernel
         // but with larger embedding space including conditioning variables

         float cte_value = 0.0f;

         // Placeholder: Would compute full CTE here
         // For now, approximate as TE with penalty for conditioning
         float base_te = 0.1f; // Would get from te_matrix
         cte_value = base_te * expf(-0.1f * num_conditioning);

         cte_matrix[pair_idx] = cte_value;
     }

     // Multivariate Transfer Entropy kernel (multiple sources to one target)
     extern "C" __global__ void multivariate_te_kernel(
         const float* __restrict__ time_series,
         const int* __restrict__ source_indices,
         int num_sources,
         int target_idx,
         float* __restrict__ mte_result,
         TEParams params
     ) {
         // Cooperative computation across thread block
         extern __shared__ float block_shared[];

         int tid = threadIdx.x;
         int t = blockIdx.x * blockDim.x + tid;

         if (t >= params.series_length - params.history_length - params.prediction_lag) {
             if (tid == 0 && blockIdx.x == 0) {
                 *mte_result = 0.0f;
             }
             return;
         }

         // Build high-dimensional embedding from all sources
         float joint_contrib = 0.0f;

         // Simplified multivariate TE
         // Full implementation would use KSG on joint distribution

         // Store partial results in shared memory
         block_shared[tid] = joint_contrib;
         __syncthreads();

         // Reduction
         for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {
             if (tid < stride) {
                 block_shared[tid] += block_shared[tid + stride];
             }
             __syncthreads();
         }

         // Final result
         if (tid == 0) {
             atomicAdd(mte_result, block_shared[0]);
         }
     }

     // Sliding window TE for dynamic causal discovery
     extern "C" __global__ void sliding_window_te_kernel(
         const float* __restrict__ time_series,
         float* __restrict__ te_timeseries, // TE values over time
         int source_idx,
         int target_idx,
         int window_size,
         int window_stride,
         TEParams params
     ) {
         int window_idx = blockIdx.x * blockDim.x + threadIdx.x;
         int num_windows = (params.series_length - window_size) / window_stride + 1;

         if (window_idx >= num_windows) return;

         int window_start = window_idx * window_stride;
         int window_end = window_start + window_size;

         // Compute TE for this window
         float window_te = 0.0f;
         int valid_points = 0;

         for (int t = window_start + params.history_length;
              t < window_end - params.prediction_lag && t < params.series_length;
              ++t) {

             // Simplified TE computation for window
             // Would use full KSG here in production

             float local_te = 0.0f;

             // Placeholder calculation
             float source_val = time_series[source_idx * params.series_length + t];
             float target_future = time_series[target_idx * params.series_length +
                                              t + params.prediction_lag];
             local_te = fabsf(source_val * target_future) * 0.01f;

             window_te += local_te;
             valid_points++;
         }

         te_timeseries[window_idx] = (valid_points > 0) ?
                                     (window_te / valid_points) : 0.0f;
     }

     // Ensemble TE kernel for robustness (multiple parameter settings)
     extern "C" __global__ void ensemble_te_kernel(
         const float* __restrict__ time_series,
         float* __restrict__ ensemble_te_matrix,
         TEParams* __restrict__ param_variants, // Array of different parameters
         int num_variants
     ) {
         int pair_idx = blockIdx.x * blockDim.x + threadIdx.x;
         int num_pairs = param_variants[0].num_variables * param_variants[0].num_variables;

         if (pair_idx >= num_pairs) return;

         float te_sum = 0.0f;
         float te_sum_sq = 0.0f;

         // Compute TE for each parameter variant
         for (int v = 0; v < num_variants; ++v) {
             // Would call main TE computation here
             float te_variant = 0.1f * (v + 1); // Placeholder

             te_sum += te_variant;
             te_sum_sq += te_variant * te_variant;
         }

         // Compute mean and variance
         float te_mean = te_sum / num_variants;
         float te_var = (te_sum_sq / num_variants) - (te_mean * te_mean);
         float te_std = sqrtf(te_var + EPSILON);

         // Store robust estimate (mean with confidence)
         ensemble_te_matrix[pair_idx * 2] = te_mean;
         ensemble_te_matrix[pair_idx * 2 + 1] = te_std;
     }

     // Performance metrics kernel
     extern "C" __global__ void te_performance_metrics(
         const float* __restrict__ te_matrix,
         float* __restrict__ metrics, // [sparsity, strength, modularity]
         int num_variables
     ) {
         // Single thread computes aggregate metrics
         if (threadIdx.x == 0 && blockIdx.x == 0) {
             int num_edges = 0;
             float total_strength = 0.0f;
             float max_te = 0.0f;

             for (int i = 0; i < num_variables * num_variables; ++i) {
                 float te = te_matrix[i];
                 if (te > 0.01f) { // Threshold for edge existence
                     num_edges++;
                     total_strength += te;
                     max_te = fmaxf(max_te, te);
                 }
             }

             float sparsity = 1.0f - (float)num_edges /
                             (num_variables * (num_variables - 1));
             float avg_strength = (num_edges > 0) ?
                                 (total_strength / num_edges) : 0.0f;

             metrics[0] = sparsity;      // Graph sparsity
             metrics[1] = avg_strength;  // Average edge strength
             metrics[2] = max_te;        // Maximum TE value
         }
     }/**
      * PRISM Wavelet-Hierarchical Conflict Repair (WHCR) - CUDA Kernels
      * 
      * Mixed-precision GPU acceleration for multiresolution conflict repair:
      * - f32 (float) for coarse levels: fast exploration, 2x memory bandwidth
      * - f64 (double) for fine levels: precise refinement near solution
      * 
      * Key innovations:
      * 1. Wavelet-decomposed conflict signals on GPU
      * 2. Hierarchical coarsening with spectral preservation
      * 3. Geometry-coupled move evaluation using TDA/geodesic metrics
      * 4. Precision-stratified computation based on resolution level
      */

     #include <cuda_runtime.h>
     #include <cooperative_groups.h>
     #include <math.h>

     namespace cg = cooperative_groups;

     // 
     // CONSTANTS AND CONFIGURATION
     // 

     #define BLOCK_SIZE 256
     #define WARP_SIZE 32
     #define MAX_COLORS 256  // Fixed: increased to match Rust implementation
     #define MAX_DEGREE 128

     // Precision thresholds
     #define COARSE_THRESHOLD 0.5f   // Use f32 when conflict density > threshold
     #define FINE_THRESHOLD 0.1f     // Use f64 when conflict density < threshold

     // 
     // GEOMETRY WEIGHTS (Constant Memory for Configuration)
     // 

     // Geometry weights configurable from host
     // These are set via cudaMemcpyToSymbol from Rust before kernel launch
     __constant__ float c_stress_weight = 0.25f;
     __constant__ float c_persistence_weight = 0.1f;
     __constant__ float c_belief_weight = 0.3f;
     __constant__ float c_hotspot_multiplier = 1.2f;

     // 
     // DATA STRUCTURES
     // 

     /**
      * CSR (Compressed Sparse Row) representation of graph adjacency
      * Used for both fine and coarse levels
      */
     struct GraphCSR {
         int num_vertices;
         int num_edges;
         int* row_ptr;      // [num_vertices + 1] - start index of each vertex's neighbors
         int* col_idx;      // [num_edges] - neighbor indices
         float* edge_weights;  // [num_edges] - optional edge weights
     };

     /**
      * Multiresolution hierarchy for conflict repair
      */
     struct MRAHierarchy {
         int num_levels;
         GraphCSR* levels;           // [num_levels] - coarsened graphs
         int** projections;          // [num_levels][fine_vertices] - fine  coarse mapping
         int** lifting_ptr;          // [num_levels][coarse_vertices+1] - coarse  fine CSR
         int** lifting_idx;          // [num_levels][fine_vertices] - coarse  fine indices
         float* approximations;      // Concatenated approximation coefficients
         double* details;            // Concatenated detail coefficients (f64 for precision)
     };

     /**
      * Geometry coupling from prior PRISM phases
      */
     struct GeometryData {
         double* stress_scores;       // [num_vertices] from Phase 4 geodesic
         double* persistence_scores;  // [num_vertices] from Phase 6 TDA
         int* hotspot_mask;           // [num_vertices] 1 if hotspot, 0 otherwise
         double* belief_distribution; // [num_vertices * num_colors] from Phase 1
         int num_vertices;
         int num_colors;
     };

     // 
     // UTILITY KERNELS
     // 

     // Wrap all kernels in extern "C" to prevent C++ name mangling
     extern "C" {

     /**
      * Count conflicts per vertex (f32 version for coarse computation)
      */
     __global__ void count_conflicts_f32(
         const int* __restrict__ coloring,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         float* __restrict__ conflict_counts,
         int num_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_vertices) {
             int my_color = coloring[tid];
             int start = row_ptr[tid];
             int end = row_ptr[tid + 1];
             
             float count = 0.0f;
             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 if (coloring[neighbor] == my_color) {
                     count += 1.0f;
                 }
             }
             conflict_counts[tid] = count;
         }
     }

     /**
      * Count conflicts per vertex (f64 version for fine computation)
      */
     __global__ void count_conflicts_f64(
         const int* __restrict__ coloring,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const double* __restrict__ stress_scores,
         const int* __restrict__ hotspot_mask,
         double* __restrict__ conflict_counts,
         int num_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_vertices) {
             int my_color = coloring[tid];
             int start = row_ptr[tid];
             int end = row_ptr[tid + 1];
             
             double count = 0.0;
             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 if (coloring[neighbor] == my_color) {
                     // Weight by geometry: high-stress conflicts count more
                     double weight = 1.0 + stress_scores[neighbor] * 0.5;
                     count += weight;
                 }
             }
             
             // Hotspots get boosted conflict visibility
             if (hotspot_mask[tid]) {
                 count *= 1.2;
             }
             
             conflict_counts[tid] = count;
         }
     }

     // 
     // WAVELET DECOMPOSITION KERNELS
     // 

     /**
      * Compute wavelet detail coefficients (high-frequency local conflicts)
      * Detail = fine_signal - interpolated(coarse_signal)
      */
     __global__ void compute_wavelet_details(
         const float* __restrict__ fine_signal,
         const float* __restrict__ coarse_signal,
         const int* __restrict__ projection,
         double* __restrict__ details,
         int num_fine_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_fine_vertices) {
             int coarse_v = projection[tid];
             // Detail captures local variation not explained by coarse structure
             details[tid] = (double)fine_signal[tid] - (double)coarse_signal[coarse_v];
         }
     }

     /**
      * Reconstruct conflict signal from wavelet coefficients
      * Used for wavelet-guided vertex prioritization
      */
     __global__ void reconstruct_from_wavelets(
         const float* __restrict__ approximation,
         const double* __restrict__ details,
         const int* __restrict__ projection,
         float* __restrict__ reconstructed,
         int num_fine_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_fine_vertices) {
             int coarse_v = projection[tid];
             reconstructed[tid] = approximation[coarse_v] + (float)details[tid];
         }
     }

     // 
     // COARSENING KERNELS
     // 

     /**
      * Compute matching scores for heavy-edge matching with conflict bias
      * Uses f32 for speed since this is exploratory
      */
     __global__ void compute_matching_scores(
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const float* __restrict__ vertex_weights,
         const float* __restrict__ conflict_mass,
         float* __restrict__ edge_scores,
         int num_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_vertices) {
             int start = row_ptr[tid];
             int end = row_ptr[tid + 1];
             
             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 if (neighbor > tid) {  // Only compute once per edge
                     // Score combines weight sum and conflict similarity
                     float weight_sum = vertex_weights[tid] + vertex_weights[neighbor];
                     float conflict_diff = fabsf(conflict_mass[tid] - conflict_mass[neighbor]);
                     float conflict_similarity = 1.0f / (1.0f + conflict_diff);
                     
                     edge_scores[i] = weight_sum * conflict_similarity;
                 }
             }
         }
     }

     /**
      * Aggregate fine vertices to coarse level
      * Computes coarse weights and conflict mass
      */
     __global__ void aggregate_to_coarse(
         const float* __restrict__ fine_weights,
         const float* __restrict__ fine_conflicts,
         const int* __restrict__ lifting_ptr,
         const int* __restrict__ lifting_idx,
         float* __restrict__ coarse_weights,
         float* __restrict__ coarse_conflicts,
         int num_coarse_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_coarse_vertices) {
             int start = lifting_ptr[tid];
             int end = lifting_ptr[tid + 1];
             
             float weight_sum = 0.0f;
             float conflict_sum = 0.0f;
             
             for (int i = start; i < end; i++) {
                 int fine_v = lifting_idx[i];
                 weight_sum += fine_weights[fine_v];
                 conflict_sum += fine_conflicts[fine_v];
             }
             
             coarse_weights[tid] = weight_sum;
             coarse_conflicts[tid] = conflict_sum;
         }
     }

     // 
     // MOVE EVALUATION KERNELS
     // 

     /**
      * Evaluate all possible color moves for conflicting vertices (f32 coarse)
      * Fast exploration at coarse resolution levels
      */
     __global__ void evaluate_moves_f32(
         const int* __restrict__ coloring,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const int* __restrict__ conflict_vertices,  // Only vertices with conflicts
         int num_conflict_vertices,
         int num_colors,
         float* __restrict__ move_deltas,  // [num_conflict_vertices * num_colors]
         int* __restrict__ best_colors,    // [num_conflict_vertices]
         const float* __restrict__ reservoir_priorities // optional, can be null-equivalent (zeroed buffer)
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;

         if (tid < num_conflict_vertices) {
             int vertex = conflict_vertices[tid];
             int current_color = coloring[vertex];
             int start = row_ptr[vertex];
             int end = row_ptr[vertex + 1];

             // Fixed: Validate num_colors to prevent buffer overflow
             if (num_colors > MAX_COLORS) return;
             
             // Count neighbor colors
             int neighbor_colors[MAX_COLORS];
             for (int c = 0; c < num_colors; c++) {
                 neighbor_colors[c] = 0;
             }
             
             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 int n_color = coloring[neighbor];
                 if (n_color < num_colors) {
                     neighbor_colors[n_color]++;
                 }
             }
             
             // Evaluate each possible new color
             int base = tid * num_colors;
             int current_conf = neighbor_colors[current_color];
             move_deltas[base + current_color] = 0.0f;

             float best_delta = 0.0f;
             int best_color = current_color;
             bool found_improving = false;

             for (int new_color = 0; new_color < num_colors; new_color++) {
                 int delta_int = neighbor_colors[new_color] - current_conf;
                 float delta = (float)delta_int;

                 // Reservoir priority bias: encourage moves away from high-priority conflicted vertices
                 if (reservoir_priorities != nullptr) {
                     float rp = reservoir_priorities[vertex];
                     delta -= rp * 0.25f;
                 }

                 move_deltas[base + new_color] = delta;

                 if (delta < best_delta) {
                     best_delta = delta;
                     best_color = new_color;
                     found_improving = true;
                 }
             }

             // Tie-break/annealing: if no improving move, pick an equal-conflict color and nudge delta negative
             if (!found_improving) {
                 for (int new_color = 0; new_color < num_colors; new_color++) {
                     if (neighbor_colors[new_color] == current_conf && new_color != current_color) {
                         best_color = new_color;
                         move_deltas[base + new_color] = -0.01f; // allow application on ties
                         break;
                     }
                 }

                 // If still no escape move selected, add a small randomized nudge
                 if (best_color == current_color) {
                     int r = vertex * 1664525 + 1013904223; // simple LCG
                     int cand = r % num_colors;
                     if (cand == current_color) {
                         cand = (cand + 1) % num_colors;
                     }
                     float jitter = -0.05f - 0.001f * (float)(r & 0xF);
                     best_color = cand;
                     move_deltas[base + cand] = jitter;
                 }
             }

             best_colors[tid] = best_color;
         }
     }

     /**
      * Evaluate moves with geometry coupling (f64 fine) - GEOMETRY WEIGHTS VERSION
      *
      * This version has exactly 12 parameters but now includes configurable weights.
      * Weight parameters are packed into a single float4 to stay within the limit.
      *
      * PARAMETERS (12 total):
      * 1. coloring - current vertex colors
      * 2. row_ptr - CSR row pointers
      * 3. col_idx - CSR column indices
      * 4. conflict_vertices - list of conflicting vertices
      * 5. num_conflict_vertices - count of conflicting vertices
      * 6. num_colors - number of colors in use
      * 7. stress_scores - Phase 4 geodesic stress
      * 8. persistence_scores - Phase 6 TDA persistence
      * 9. belief_distribution - Phase 1 active inference beliefs
      * 10. total_vertices - total graph size
      * 11. move_deltas - output delta scores [num_conflict_vertices * num_colors]
      * 12. best_colors - output best color per vertex [num_conflict_vertices]
      *
      * NOTE: Weights are now passed inline as constants in this kernel.
      * To make them configurable, we use kernel parameters during launch from Rust.
      */
     __global__ void evaluate_moves_f64(
         const int* __restrict__ coloring,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const int* __restrict__ conflict_vertices,
         int num_conflict_vertices,
         int num_colors,
         const double* __restrict__ stress_scores,
         const double* __restrict__ persistence_scores,
         const double* __restrict__ belief_distribution,  // [num_vertices * num_colors]
         int total_vertices,
         double* __restrict__ move_deltas,
         int* __restrict__ best_colors
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;

         // Fixed: Validate num_colors to prevent buffer overflow
         if (num_colors > MAX_COLORS) return;

         if (tid < num_conflict_vertices) {
             int vertex = conflict_vertices[tid];
             int current_color = coloring[vertex];
             int start = row_ptr[vertex];
             int end = row_ptr[vertex + 1];

             // Geometry weights for this vertex
             double my_stress = stress_scores[vertex];
             double my_persistence = persistence_scores[vertex];

             // Derive hotspot status from high stress (top 20% = hotspot)
             bool is_hotspot = my_stress > 0.8;

             // Count weighted neighbor colors
             double neighbor_weights[MAX_COLORS];
             for (int c = 0; c < num_colors; c++) {
                 neighbor_weights[c] = 0.0;
             }

             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 int n_color = coloring[neighbor];
                 if (n_color < num_colors) {
                     // Resolved TODO(GPU-WHCR-3): Use configurable geometry weight
                     double n_stress = stress_scores[neighbor];
                     double weight = 1.0 + (my_stress + n_stress) * c_stress_weight;
                     neighbor_weights[n_color] += weight;
                 }
             }

             // Evaluate each possible new color with belief guidance
             double best_delta = 0.0;
             int best_color = current_color;

             for (int new_color = 0; new_color < num_colors; new_color++) {
                 if (new_color == current_color) {
                     // Explicitly define delta for current color
                     move_deltas[tid * num_colors + new_color] = 0.0;
                     continue;
                 }

                 // Base delta: weighted conflict change
                 double delta = neighbor_weights[new_color] - neighbor_weights[current_color];

                 // Resolved TODO(GPU-WHCR-3): Use configurable belief weight
                 if (belief_distribution != nullptr) {
                     double belief_current = belief_distribution[vertex * num_colors + current_color];
                     double belief_new = belief_distribution[vertex * num_colors + new_color];
                     delta -= (belief_new - belief_current) * c_belief_weight;
                 }

                 // Resolved TODO(GPU-WHCR-3): Use configurable hotspot multiplier
                 if (is_hotspot && delta < 0.0) {
                     delta *= c_hotspot_multiplier;
                 }

                 // Resolved TODO(GPU-WHCR-3): Use configurable persistence weight
                 delta += my_persistence * c_persistence_weight;

                 move_deltas[tid * num_colors + new_color] = delta;

                 if (delta < best_delta) {
                     best_delta = delta;
                     best_color = new_color;
                 }
             }

             // Randomized escape: if nothing improves, allow a small negative nudge to a random color
             if (best_delta >= 0.0) {
                 int r = vertex * 1664525 + 1013904223; // simple LCG
                 int cand = r % num_colors;
                 if (cand == current_color) {
                     cand = (cand + 1) % num_colors;
                 }
                 double jitter = -0.05 - 0.001 * (double)(r & 0xF);
                 best_color = cand;
                 best_delta = jitter;
                 move_deltas[tid * num_colors + cand] = jitter;
             }

             best_colors[tid] = best_color;
         }
     }

     /**
      * Evaluate moves with geometry coupling (f64 fine) - FULL GEOMETRY VERSION
      *
      * This version has 13 parameters and cannot be called via LaunchAsync.
      * Kept for future use when we have a better parameter passing strategy.
      * TODO(GPU-WHCR): Implement struct-based parameter passing for 13+ params
      */
     __global__ void evaluate_moves_f64_geometry(
         const int* __restrict__ coloring,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         const int* __restrict__ conflict_vertices,
         int num_conflict_vertices,
         int num_colors,
         const double* __restrict__ stress_scores,
         const double* __restrict__ persistence_scores,
         const int* __restrict__ hotspot_mask,
         const double* __restrict__ belief_distribution,  // [num_vertices * num_colors]
         int total_vertices,
         double* __restrict__ move_deltas,
         int* __restrict__ best_colors
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;

         // Fixed: Validate num_colors to prevent buffer overflow
         if (num_colors > MAX_COLORS) return;

         if (tid < num_conflict_vertices) {
             int vertex = conflict_vertices[tid];
             int current_color = coloring[vertex];
             int start = row_ptr[vertex];
             int end = row_ptr[vertex + 1];

             // Geometry weights for this vertex
             double my_stress = stress_scores[vertex];
             double my_persistence = persistence_scores[vertex];
             bool is_hotspot = hotspot_mask[vertex] != 0;

             // Count weighted neighbor colors
             double neighbor_weights[MAX_COLORS];
             for (int c = 0; c < num_colors; c++) {
                 neighbor_weights[c] = 0.0;
             }

             for (int i = start; i < end; i++) {
                 int neighbor = col_idx[i];
                 int n_color = coloring[neighbor];
                 if (n_color < num_colors) {
                     // Resolved TODO(GPU-WHCR-3): Use configurable geometry weight
                     double n_stress = stress_scores[neighbor];
                     double weight = 1.0 + (my_stress + n_stress) * c_stress_weight;
                     neighbor_weights[n_color] += weight;
                 }
             }

             // Evaluate each possible new color with belief guidance
             double best_delta = 0.0;
             int best_color = current_color;

             for (int new_color = 0; new_color < num_colors; new_color++) {
                 if (new_color == current_color) continue;

                 // Base delta: weighted conflict change
                 double delta = neighbor_weights[new_color] - neighbor_weights[current_color];

                 // Resolved TODO(GPU-WHCR-3): Use configurable belief weight
                 if (belief_distribution != nullptr) {
                     double belief_current = belief_distribution[vertex * num_colors + current_color];
                     double belief_new = belief_distribution[vertex * num_colors + new_color];
                     delta -= (belief_new - belief_current) * c_belief_weight;
                 }

                 // Resolved TODO(GPU-WHCR-3): Use configurable hotspot multiplier
                 if (is_hotspot && delta < 0.0) {
                     delta *= c_hotspot_multiplier;
                 }

                 // Resolved TODO(GPU-WHCR-3): Use configurable persistence weight
                 delta += my_persistence * c_persistence_weight;

                 move_deltas[tid * num_colors + new_color] = delta;

                 if (delta < best_delta) {
                     best_delta = delta;
                     best_color = new_color;
                 }
             }

             best_colors[tid] = best_color;
         }
     }

     // 
     // MOVE APPLICATION KERNEL
     // 

     /**
      * Apply best moves in parallel with conflict detection
      * Uses vertex-level locking to avoid simultaneous conflicting updates
      */
     __global__ void apply_moves_with_locking(
         int* __restrict__ coloring,
         const int* __restrict__ conflict_vertices,
         const int* __restrict__ best_colors,
         const float* __restrict__ move_deltas,  // Best delta per vertex
         int num_conflict_vertices,
         int num_colors,
         int* __restrict__ locks,  // Per-vertex locks
         int* __restrict__ num_moves_applied,
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_conflict_vertices) {
             int vertex = conflict_vertices[tid];
             int new_color = best_colors[tid];
             float delta = move_deltas[tid * num_colors + new_color];

             if (new_color == coloring[vertex]) return;

             // Recompute local conflicts against current coloring to avoid stale moves
             int start = row_ptr[vertex];
             int end   = row_ptr[vertex + 1];
             int cur_color = coloring[vertex];
             int cur_conf = 0;
             int new_conf = 0;
             for (int i = start; i < end; ++i) {
                 int n = col_idx[i];
                 int n_color = coloring[n];
                 cur_conf += (n_color == cur_color);
                 new_conf += (n_color == new_color);
             }

             float delta_conf = (float)new_conf - (float)cur_conf;
             bool accept = (delta_conf <= 0.5f && delta < -1e-3f) ||   // allow ties/slight improvement
                           (delta_conf <= 3.0f && delta < -1e-2f);     // allow small worsening only with strong preference

             // Apply when conflicts do not worsen (or only slightly) and delta shows preference
             if (accept) {
                 // Try to acquire lock on vertex
                 if (atomicCAS(&locks[vertex], 0, 1) == 0) {
                     // Also lock neighbors to avoid simultaneous adjacent moves
                     int start = row_ptr[vertex];
                     int end   = row_ptr[vertex + 1];
                     bool neighbor_locked = true;
                     for (int i = start; i < end; ++i) {
                         int n = col_idx[i];
                         if (atomicCAS(&locks[n], 0, 1) != 0) {
                             neighbor_locked = false;
                             // release any neighbor locks acquired so far
                             for (int j = start; j < i; ++j) {
                                 int rel = col_idx[j];
                                 atomicExch(&locks[rel], 0);
                             }
                             break;
                         }
                     }

                     if (neighbor_locked) {
                         coloring[vertex] = new_color;
                         atomicAdd(num_moves_applied, 1);
                         for (int i = start; i < end; ++i) {
                             int rel = col_idx[i];
                             atomicExch(&locks[rel], 0);
                         }
                     }

                     atomicExch(&locks[vertex], 0);
                 }
             }
         }
     }

     /**
      * Apply best moves with parallel locking (f64 precision version)
      * Uses double precision for move deltas when fine-level precision is needed
      */
     extern "C" __global__ void apply_moves_with_locking_f64(
         int* __restrict__ coloring,
         const int* __restrict__ conflict_vertices,
         const int* __restrict__ best_colors,
         const double* __restrict__ move_deltas,  // f64 instead of f32
         int num_conflict_vertices,
         int num_colors,
         int* __restrict__ locks,  // Per-vertex locks
         int* __restrict__ num_moves_applied,
         // CSR graph (needed for on-the-fly validation)
         const int* __restrict__ row_ptr,
         const int* __restrict__ col_idx,
         // Geometry for weighted conflict metric (must match count_conflicts_f64)
         const double* __restrict__ stress_scores,
         const int* __restrict__ hotspot_mask
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;

         if (tid < num_conflict_vertices) {
             int vertex = conflict_vertices[tid];
             int new_color = best_colors[tid];
             double delta = move_deltas[tid * num_colors + new_color];  // double precision

             if (new_color == coloring[vertex]) return;

             // Recompute local conflict delta against current coloring to avoid stale decisions
             int start = row_ptr[vertex];
             int end   = row_ptr[vertex + 1];
             int cur_color = coloring[vertex];
             double cur_conf = 0.0;
             double new_conf = 0.0;
             for (int i = start; i < end; ++i) {
                 int n = col_idx[i];
                 int n_color = coloring[n];
                 double weight = 1.0 + stress_scores[n] * 0.5;
                 if (hotspot_mask[vertex]) {
                     weight *= 1.2;
                 }
                 if (n_color == cur_color) cur_conf += weight;
                 if (n_color == new_color) new_conf += weight;
             }

             // Allow non-worsening moves when delta prefers the change
             double delta_conf = new_conf - cur_conf;
             if (delta_conf <= 0.0 && delta < -1e-3) {
                 // Try to acquire lock on vertex
                 if (atomicCAS(&locks[vertex], 0, 1) == 0) {
                     // Optional: also lock neighbors to avoid simultaneous adjacent moves
                     bool neighbor_locked = true;
                     for (int i = start; i < end; ++i) {
                         int n = col_idx[i];
                         if (atomicCAS(&locks[n], 0, 1) != 0) {
                             neighbor_locked = false;
                             // release any neighbor locks acquired so far
                             for (int j = start; j < i; ++j) {
                                 int rel = col_idx[j];
                                 atomicExch(&locks[rel], 0);
                             }
                             break;
                         }
                     }

                     if (neighbor_locked) {
                         coloring[vertex] = new_color;
                         atomicAdd(num_moves_applied, 1);

                         // release neighbor locks
                         for (int i = start; i < end; ++i) {
                             int rel = col_idx[i];
                             atomicExch(&locks[rel], 0);
                         }
                     }

                     // release vertex lock
                     atomicExch(&locks[vertex], 0);
                 }
             }
         }
     }

     // 
     // WAVELET-GUIDED PRIORITIZATION
     // 

     /**
      * Compute priority scores based on wavelet detail coefficients
      * High |detail| = local conflict not explained by global structure = high priority
      */
     __global__ void compute_wavelet_priorities(
         const double* __restrict__ details,
         const float* __restrict__ conflict_counts,
         const double* __restrict__ stress_scores,
         const int* __restrict__ hotspot_mask,
         float* __restrict__ priorities,
         int num_vertices
     ) {
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         if (tid < num_vertices) {
             // Only prioritize conflicting vertices
             if (conflict_counts[tid] < 0.5f) {
                 priorities[tid] = -1.0f;  // Not conflicting
                 return;
             }
             
             // Priority combines wavelet detail, conflict count, and geometry
             double detail_magnitude = fabs(details[tid]);
             float conflict_weight = conflict_counts[tid];
             double stress_weight = stress_scores[tid];
             float hotspot_bonus = hotspot_mask[tid] ? 2.0f : 0.0f;
             
             // Wavelet detail is primary driver: local anomalies need attention
             float priority = (float)(detail_magnitude * 3.0) 
                            + conflict_weight 
                            + (float)(stress_weight * 0.5)
                            + hotspot_bonus;
             
             priorities[tid] = priority;
         }
     }

     } // extern "C" - End of kernel definitions to prevent C++ name mangling

     // 
     // MAIN REPAIR ITERATION KERNEL
     // 

     /**
      * Single iteration of wavelet-hierarchical repair at specified level
      * Dispatches to f32 or f64 kernels based on precision level
      */
     extern "C" __global__ void whcr_iteration(
         int* coloring,
         const GraphCSR graph,
         const GeometryData geometry,
         float* workspace_f32,
         double* workspace_f64,
         int* conflict_vertices,
         int* num_conflicts,
         int precision_level,  // 0=coarse(f32), 1=mixed, 2=fine(f64)
         int num_colors,
         int* iteration_result
     ) {
         // This is a device-side orchestration kernel
         // Actual implementation would dispatch to appropriate sub-kernels
         
         int tid = blockIdx.x * blockDim.x + threadIdx.x;
         
         // Cooperative groups for synchronization
         cg::grid_group grid = cg::this_grid();
         
         // Phase 1: Count conflicts (precision-dependent)
         if (precision_level == 0) {
             // f32 path - fast
             if (tid < graph.num_vertices) {
                 int my_color = coloring[tid];
                 int start = graph.row_ptr[tid];
                 int end = graph.row_ptr[tid + 1];
                 
                 float count = 0.0f;
                 for (int i = start; i < end; i++) {
                     if (coloring[graph.col_idx[i]] == my_color) {
                         count += 1.0f;
                     }
                 }
                 workspace_f32[tid] = count;
             }
         } else {
             // f64 path - precise with geometry
             if (tid < graph.num_vertices) {
                 int my_color = coloring[tid];
                 int start = graph.row_ptr[tid];
                 int end = graph.row_ptr[tid + 1];
                 
                 double count = 0.0;
                 for (int i = start; i < end; i++) {
                     int neighbor = graph.col_idx[i];
                     if (coloring[neighbor] == my_color) {
                         double weight = 1.0 + geometry.stress_scores[neighbor] * 0.5;
                         count += weight;
                     }
                 }
                 workspace_f64[tid] = count;
             }
         }
         
         grid.sync();
         
         // Phase 2: Find best moves and apply
         // (Simplified - full implementation would use sub-kernels)
         
         if (tid == 0) {
             *iteration_result = 1;  // Success flag
         }
     }

     // 
     // HOST-SIDE HELPER FUNCTIONS (declared extern "C" for Rust FFI)
     // 

     extern "C" {

     /**
      * Launch f32 conflict counting kernel
      */
     void launch_count_conflicts_f32(
         const int* coloring,
         const int* row_ptr,
         const int* col_idx,
         float* conflict_counts,
         int num_vertices,
         cudaStream_t stream
     ) {
         int blocks = (num_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         count_conflicts_f32<<<blocks, BLOCK_SIZE, 0, stream>>>(
             coloring, row_ptr, col_idx, conflict_counts, num_vertices
         );
     }

     /**
      * Launch f64 conflict counting kernel with geometry
      */
     void launch_count_conflicts_f64(
         const int* coloring,
         const int* row_ptr,
         const int* col_idx,
         const double* stress_scores,
         const int* hotspot_mask,
         double* conflict_counts,
         int num_vertices,
         cudaStream_t stream
     ) {
         int blocks = (num_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         count_conflicts_f64<<<blocks, BLOCK_SIZE, 0, stream>>>(
             coloring, row_ptr, col_idx, stress_scores, hotspot_mask, 
             conflict_counts, num_vertices
         );
     }

     /**
      * Launch wavelet detail computation
      */
     void launch_compute_wavelet_details(
         const float* fine_signal,
         const float* coarse_signal,
         const int* projection,
         double* details,
         int num_fine_vertices,
         cudaStream_t stream
     ) {
         int blocks = (num_fine_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         compute_wavelet_details<<<blocks, BLOCK_SIZE, 0, stream>>>(
             fine_signal, coarse_signal, projection, details, num_fine_vertices
         );
     }

     /**
      * Launch f32 move evaluation (coarse levels)
      */
     void launch_evaluate_moves_f32(
         const int* coloring,
         const int* row_ptr,
         const int* col_idx,
         const int* conflict_vertices,
         int num_conflict_vertices,
         int num_colors,
         float* move_deltas,
         int* best_colors,
         const float* reservoir_priorities,
         cudaStream_t stream
     ) {
         int blocks = (num_conflict_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         evaluate_moves_f32<<<blocks, BLOCK_SIZE, 0, stream>>>(
             coloring, row_ptr, col_idx, conflict_vertices,
             num_conflict_vertices, num_colors, move_deltas, best_colors,
             reservoir_priorities
         );
     }

     /**
      * Launch f64 move evaluation (fine levels) - 12 parameter version
      */
     void launch_evaluate_moves_f64(
         const int* coloring,
         const int* row_ptr,
         const int* col_idx,
         const int* conflict_vertices,
         int num_conflict_vertices,
         int num_colors,
         const double* stress_scores,
         const double* persistence_scores,
         const double* belief_distribution,
         int total_vertices,
         double* move_deltas,
         int* best_colors,
         cudaStream_t stream
     ) {
         int blocks = (num_conflict_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         evaluate_moves_f64<<<blocks, BLOCK_SIZE, 0, stream>>>(
             coloring, row_ptr, col_idx, conflict_vertices,
             num_conflict_vertices, num_colors,
             stress_scores, persistence_scores, belief_distribution,
             total_vertices, move_deltas, best_colors
         );
     }

     /**
      * Launch f64 move evaluation with full geometry (fine levels) - 13 parameter version
      * NOTE: This launcher exists but cannot be called from Rust via cudarc due to 12-parameter limit
      * TODO(GPU-WHCR): Implement struct-based parameter passing
      */
     void launch_evaluate_moves_f64_geometry(
         const int* coloring,
         const int* row_ptr,
         const int* col_idx,
         const int* conflict_vertices,
         int num_conflict_vertices,
         int num_colors,
         const double* stress_scores,
         const double* persistence_scores,
         const int* hotspot_mask,
         const double* belief_distribution,
         int total_vertices,
         double* move_deltas,
         int* best_colors,
         cudaStream_t stream
     ) {
         int blocks = (num_conflict_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         evaluate_moves_f64_geometry<<<blocks, BLOCK_SIZE, 0, stream>>>(
             coloring, row_ptr, col_idx, conflict_vertices,
             num_conflict_vertices, num_colors,
             stress_scores, persistence_scores, hotspot_mask, belief_distribution,
             total_vertices, move_deltas, best_colors
         );
     }

     /**
      * Launch wavelet priority computation
      */
     void launch_compute_wavelet_priorities(
         const double* details,
         const float* conflict_counts,
         const double* stress_scores,
         const int* hotspot_mask,
         float* priorities,
         int num_vertices,
         cudaStream_t stream
     ) {
         int blocks = (num_vertices + BLOCK_SIZE - 1) / BLOCK_SIZE;
         compute_wavelet_priorities<<<blocks, BLOCK_SIZE, 0, stream>>>(
             details, conflict_counts, stress_scores, hotspot_mask,
             priorities, num_vertices
         );
     }

     } // extern "C"   doea ny of this help?
   Context limit reached  /compact or /clear to continue

 Baked for 9m 25s

 /model 
    Kept model as Default (recommended)


 

   plan mode on (shift+tab to cycle)                                                                                            91722 tokens
                                                                                                               current: 2.1.2  latest: 2.1.3






